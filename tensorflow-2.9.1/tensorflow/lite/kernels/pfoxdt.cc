/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
#include "tensorflow/lite/kernels/internal/optimized/integer_ops/conv.h"

#include <stddef.h>
#include <iostream>
#include <cstdint>
#include <vector>

// Only use multi-threaded Eigen if ruy is disabled.
#if !defined(TFLITE_WITH_RUY)
#define TFLITE_WITH_MULTITHREADED_EIGEN
#endif

#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/common.h"
#include "tensorflow/lite/kernels/cpu_backend_context.h"
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
#include "tensorflow/lite/kernels/eigen_support.h"
#endif
#include "tensorflow/lite/kernels/internal/compatibility.h"
#include "tensorflow/lite/kernels/internal/types.h"
// b/131835803 forces us to include multithreaded_conv.h before optimized_ops.h
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
#include "tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h"
#endif
#include "tensorflow/lite/kernels/internal/optimized/optimized_ops.h"
#include "tensorflow/lite/kernels/internal/quantization_util.h"
#include "tensorflow/lite/kernels/internal/reference/conv.h"
#include "tensorflow/lite/kernels/internal/reference/integer_ops/conv.h"
#include "tensorflow/lite/kernels/internal/tensor.h"
#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
#include "tensorflow/lite/kernels/internal/tensor_utils.h"
#include "tensorflow/lite/kernels/kernel_util.h"
#include "tensorflow/lite/kernels/padding.h"
#include "tensorflow/lite/util.h"

namespace tflite {
namespace ops {
namespace custom {
namespace pfoxdt {

// This file has 4 implementation of Conv.
enum KernelType {
  kReference,
  kGenericOptimized,  // Neon-free
  // kMultithreadOptimized is a mixture of an Eigen-based kernel when threads
  // are available and kGenericOptimized when we must use only one thread.
  kMultithreadOptimized,
  // The kernel uses use CBLAS interface for matrix multiplication.
  // It's fast when an optimized CBLAS implementation is available (e.g. Apple
  // Accelerate Framework), and it's slow when falling back to naive
  // implementation.
  kCblasOptimized,
};

const int kTensorNotAllocated = -1;

static constexpr size_t kMaxIm2colBufferSizeMobile = 1024 * 1024 * 1024;  // 1GB

const float filter_raw[4096]={0.0647742971777916, -0.15375655889511108, -0.43805992603302, -0.18954697251319885, 0.14588770270347595, 0.03848937898874283, 0.17220516502857208, -0.027823414653539658, 0.07070516049861908, 0.05902574583888054, 0.06561131775379181, 0.06215495243668556, 0.0896960198879242, -0.1064651608467102, -0.01985475979745388, 0.10850555449724197, 0.2971532642841339, 0.07906198501586914, 0.1355956643819809, -0.04254261776804924, 0.13019178807735443, 0.09019231796264648, 0.0062970370054244995, -0.14523570239543915, 0.06300303339958191, 0.06423858553171158, -0.11624553799629211, -0.07637184113264084, -0.18396919965744019, -0.1701851189136505, 0.025826526805758476, 0.018109308555722237, -0.06806819885969162, 0.3021683096885681, -0.14621944725513458, -0.1597723662853241, -0.06100524961948395, -0.05079296976327896, -0.21591220796108246, -0.22725103795528412, 0.31798699498176575, -0.23257054388523102, -0.07881985604763031, 0.011507769115269184, 0.004606341943144798, 0.033714015036821365, 0.023425370454788208, 0.17212101817131042, 0.14143270254135132, -0.01566844806075096, -0.2672778069972992, 0.06832703948020935, 0.051603835076093674, -0.08348444104194641, -0.15422888100147247, -0.014939428307116032, -0.10045549273490906, -0.03024871274828911, 0.10559402406215668, 0.19016216695308685, 0.0488339364528656, 0.15269482135772705, -0.005252724513411522, -0.004163976293057203, 0.235742449760437, 0.11477851122617722, -0.0025021194014698267, -0.21037361025810242, 0.4481438994407654, -0.14153894782066345, 0.11624008417129517, -0.11258729547262192, 0.0704573392868042, -0.07632472366094589, 0.01173928938806057, -0.24378153681755066, -0.008803105913102627, 0.0218840129673481, 0.07538122683763504, -0.22142280638217926, 0.16307221353054047, 0.02537773922085762, -0.09202174097299576, 0.01843477599322796, -0.06783547252416611, 0.029724162071943283, -0.23206236958503723, 0.18435601890087128, -0.09192093461751938, -0.005426120478659868, 0.10958949476480484, -0.03272222355008125, 0.058829013258218765, 0.2114381194114685, -0.3691810071468353, 0.33474433422088623, 0.022322621196508408, 0.14566221833229065, -0.028388937935233116, 0.18078920245170593, -0.2033204883337021, -0.08820334076881409, -0.11457949876785278, 0.012839008122682571, -0.018284941092133522, -0.10558030009269714, 0.07535020262002945, -0.16634953022003174, -0.032185304909944534, 0.24833770096302032, -0.0326792374253273, -0.08080997318029404, -0.023031555116176605, -0.10866856575012207, 0.0005550213390961289, -0.028936216607689857, -0.1346808224916458, -0.07077556848526001, 0.05350733548402786, -0.08378592133522034, -0.12245401740074158, -0.3613729476928711, 0.026889722794294357, 0.015609978698194027, 0.032528653740882874, 0.26954084634780884, 0.01081676222383976, -0.2578613758087158, -0.020393943414092064, -0.2398206889629364, 0.25830012559890747, -0.30703338980674744, -0.2650417387485504, -0.024578416720032692, -0.1297014355659485, 0.47659599781036377, -0.022941498085856438, -0.16867706179618835, 0.07458477467298508, 0.12895646691322327, -0.13397474586963654, 0.019054224714636803, 0.025215135887265205, -0.02225700579583645, -0.10311365127563477, -0.08544883131980896, -0.17787933349609375, 0.08335161209106445, -0.045880284160375595, -0.08875956386327744, -0.2382676899433136, 0.00536636495962739, 0.048863980919122696, -0.029711449518799782, 0.1776753067970276, 0.22266951203346252, -0.1344144344329834, -0.020018940791487694, 0.01802327297627926, -0.058604005724191666, -0.08839680254459381, -0.21194146573543549, -0.04543726146221161, -0.18820123374462128, 0.028012601658701897, 0.0453968346118927, 0.1700599044561386, -0.10234462469816208, -0.1493721306324005, 0.06767485290765762, -0.29306772351264954, -0.07023584842681885, -0.259130597114563, -0.22391153872013092, 0.09035369008779526, -0.12097268551588058, -0.32168981432914734, -0.02508648857474327, -0.30766940116882324, -0.06016639247536659, 0.02246599644422531, -0.31378576159477234, -0.007844642736017704, -0.07948171347379684, -0.17133791744709015, -0.12959878146648407, -0.0737098678946495, 0.008068080060184002, -0.24462063610553741, 0.01246211864054203, -0.1126929447054863, 0.03741268813610077, -0.3589896857738495, -0.3288901746273041, -0.10181472450494766, -0.3143790364265442, -0.18320155143737793, 0.06218036636710167, -0.26340171694755554, -0.03641616553068161, -0.05840219929814339, -0.1631213277578354, 0.19359242916107178, 0.004780059214681387, -0.1631568819284439, 0.5803830623626709, -0.22104310989379883, -0.16019974648952484, -0.1220521479845047, -0.056952934712171555, 0.160771444439888, -0.19782455265522003, 0.08291196078062057, -0.2753515839576721, 0.2951599657535553, -0.12813745439052582, -0.08495255559682846, -0.2760220468044281, -0.15615171194076538, 0.08183862268924713, -0.020504850894212723, -0.045362476259469986, -0.09453181177377701, -0.09289852529764175, -0.29916608333587646, -0.13358497619628906, 0.39581766724586487, 0.026448536664247513, -0.07395417988300323, 0.20159487426280975, -0.09517847001552582, 0.03872961923480034, 0.06693172454833984, -0.12017104774713516, -0.02655564434826374, 0.21913425624370575, -0.185245543718338, -0.10004924982786179, -0.016719738021492958, 0.06771295517683029, -0.04420551657676697, 0.03577181324362755, 0.2621120512485504, -0.10225852578878403, -0.12457449734210968, 0.01518447045236826, 0.05981343612074852, 0.04438690468668938, 0.003153908997774124, 0.015695421025156975, -0.03569478541612625, -0.14955982565879822, 0.09197063744068146, -0.044369254261255264, -0.010325860232114792, -0.07411637902259827, 0.34915679693222046, 0.06945793330669403, 0.547940194606781, 0.13949590921401978, 0.025809641927480698, 0.04317162185907364, -0.03684724122285843, -0.04237820580601692, -0.24073858559131622, 0.09276336431503296, -0.11494075506925583, -0.01662381738424301, -0.24376152455806732, -0.030300399288535118, -0.1221565455198288, -0.06156026944518089, -0.21157152950763702, 0.2500995099544525, 0.18248945474624634, 0.0007172084879130125, -0.004973765928298235, 0.2161536067724228, 0.03514789417386055, 0.03153093159198761, 0.07402493804693222, 0.035974208265542984, 0.03984644636511803, 0.05541788786649704, 0.07906872034072876, 0.022938793525099754, 0.07247857004404068, -0.0593029148876667, 0.044950615614652634, 0.1683722585439682, -0.13870200514793396, 0.03829510137438774, -0.09299624711275101, -0.020075760781764984, -0.09941177815198898, -0.07657769322395325, 0.06272029131650925, 0.09552782773971558, 0.008855203166604042, -0.040838953107595444, 0.010538851842284203, 0.07228346914052963, 0.06811752915382385, 0.07352037727832794, 0.12799428403377533, -0.07641546428203583, -0.12120421975851059, 0.06052689254283905, 0.06577827036380768, -0.1799927055835724, 0.012583300471305847, 0.08034353703260422, -0.24509559571743011, 0.12368332594633102, 0.023729786276817322, 0.07417044788599014, 0.15802039206027985, -0.00013076062896288931, 0.18964357674121857, 0.040946148335933685, 0.09532088041305542, 0.3403753340244293, 0.08339124917984009, 0.09370395541191101, -0.15892575681209564, -0.24626247584819794, 0.21644167602062225, 0.2451283037662506, -0.03304089605808258, 0.25552207231521606, -0.1532764881849289, -0.1247560977935791, 0.06463147699832916, -0.25406429171562195, 0.21707013249397278, 0.12557470798492432, 0.13039883971214294, 0.017717141658067703, 0.11461928486824036, -0.046359144151210785, -0.16765369474887848, 0.023590337485074997, -0.2008127123117447, 0.057675547897815704, 0.10799888521432877, 0.24493639171123505, 0.05395614355802536, 0.04394456744194031, 0.1066751554608345, 0.04869446903467178, -0.01257779449224472, 0.208958238363266, 0.03310339152812958, -0.12917427718639374, -0.24104593694210052, 0.10575370490550995, 0.222238227725029, -0.005351260770112276, -0.08635716140270233, 0.15790456533432007, 0.13562345504760742, 0.29939547181129456, 0.06393936276435852, -0.034640099853277206, 0.19689889252185822, 0.2531728148460388, 0.20038342475891113, 0.12783700227737427, -0.1243290826678276, -0.10917569696903229, 0.002673229668289423, 0.061383381485939026, 0.04941616952419281, 0.12904304265975952, 0.14035506546497345, 0.07721670717000961, 0.18436889350414276, -0.19433282315731049, -0.07734207063913345, 0.10219158232212067, -0.06154526397585869, -0.023697149008512497, 0.11358986794948578, -0.09335890412330627, 0.031704340130090714, 0.04518340528011322, 0.14810264110565186, -0.41833972930908203, -0.21671053767204285, 0.005110287573188543, 0.07001864910125732, 0.04846172779798508, -0.04866892471909523, -0.11169317364692688, 0.092551089823246, 0.1220054104924202, 0.016593119129538536, 0.13958805799484253, -0.08392903953790665, -0.1397688090801239, 0.005904949735850096, -0.14917415380477905, 0.028722688555717468, -0.019453704357147217, -0.03755179047584534, -0.01898459531366825, -0.20954285562038422, 0.009469060227274895, -0.005739362444728613, 0.03719671070575714, 0.19379568099975586, -0.009852943941950798, -0.08307601511478424, 0.022550128400325775, -0.0023061141837388277, 0.14669519662857056, 0.026872726157307625, -0.08375941962003708, -0.3094785213470459, -0.05388003960251808, -0.03603580966591835, 0.18302686512470245, -0.30586397647857666, 0.026346374303102493, -0.03147139400243759, -0.33550044894218445, -0.18574167788028717, -0.20061036944389343, 0.07599489390850067, -0.03721907362341881, -0.033621106296777725, -0.04747150093317032, -0.13397574424743652, -0.12066008150577545, -0.12492876499891281, -0.08815629780292511, 0.05530184134840965, -0.2662630081176758, -0.008545959368348122, 0.2898803949356079, 0.10992148518562317, -0.17381764948368073, 0.040887244045734406, 0.020064793527126312, -0.09983775019645691, 0.10185007005929947, -0.15470346808433533, -0.01814548298716545, 0.3142196536064148, -0.19269675016403198, -0.035690680146217346, -0.25543609261512756, -0.10591040551662445, 0.046242836862802505, -0.17587728798389435, 0.07213546335697174, -0.05361654981970787, -0.12953810393810272, 0.2496238499879837, -0.08584225177764893, -0.2540383040904999, -0.006342533975839615, -0.1436200886964798, 0.2765897214412689, 0.006719105411320925, 0.011028902605175972, -0.003640419337898493, 0.12686984241008759, -0.0014768977416679263, 0.5098686218261719, 0.11979660391807556, -0.08016803860664368, -0.12127874791622162, -0.10988523811101913, -0.14217862486839294, -0.27665114402770996, -0.1997319757938385, 0.20334497094154358, 0.04521157965064049, 0.06873734295368195, -0.27381372451782227, -0.30633294582366943, -0.20397138595581055, -0.02748829871416092, -0.13982343673706055, -0.026089902967214584, -0.08664896339178085, -0.03950944542884827, -0.08057259023189545, -0.10003511607646942, -0.05707245320081711, -0.106595978140831, -0.18287770450115204, -0.19957861304283142, 0.03980061039328575, 0.05202900618314743, -0.19261203706264496, 0.1564714014530182, 0.11383204907178879, -0.11771464347839355, -0.04455117881298065, -0.023856522515416145, 0.21818429231643677, -0.18760532140731812, -0.1229775995016098, -0.1804622858762741, -0.1069314107298851, -0.14552542567253113, 0.4268604815006256, 0.030099399387836456, -0.09116470068693161, -0.23738203942775726, -0.281637042760849, 0.07283628731966019, -0.09186074882745743, 0.18149949610233307, -0.0135142020881176, 0.07537000626325607, 0.40614762902259827, 0.4739372730255127, -0.017613675445318222, 0.3254738748073578, 0.04631861299276352, -0.04057114198803902, 0.1597621887922287, 0.16799651086330414, -0.16312307119369507, -0.013235841877758503, 0.1377922147512436, 0.13472551107406616, 0.1465737670660019, -0.04344340041279793, 0.04053681343793869, -0.08972824364900589, 0.2088398039340973, 0.0852426141500473, 0.0010687361937016249, 0.09362632036209106, 0.020383398979902267, -0.20278027653694153, -0.2197307050228119, -0.08807933330535889, -0.1546078771352768, 0.25989601016044617, -0.13522247970104218, -0.10392706841230392, -0.17094965279102325, -0.10584601014852524, 0.050142090767621994, -0.16662341356277466, -0.1473383903503418, 0.23656310141086578, 0.09248340129852295, -0.07911654561758041, -0.005709631834179163, 0.11882518231868744, -0.03185764327645302, -0.09920227527618408, 0.024149533361196518, 0.06993190199136734, 0.38115909695625305, -0.017550472170114517, 0.048991572111845016, 0.16499543190002441, -0.13118664920330048, -0.07200415432453156, -0.00990965310484171, -0.13127130270004272, 0.2434529811143875, -0.24256199598312378, -0.0216053519397974, 0.06913486123085022, 0.08549746125936508, 0.008819514885544777, -0.18798884749412537, 0.042740508913993835, 0.06574356555938721, -0.16830399632453918, 0.10735954344272614, 0.12105008959770203, 0.13542591035366058, -0.23455475270748138, 0.23274050652980804, -0.21293890476226807, -0.04073190689086914, 0.07875139266252518, 0.1417008638381958, -0.01948721334338188, 0.27653881907463074, 0.5400841236114502, 0.04655758664011955, 0.0028625812847167253, 0.23440046608448029, -0.025451524183154106, 0.008838290348649025, 0.27091723680496216, -0.16472198069095612, 0.12830491364002228, -0.2742900848388672, 0.02190338633954525, 0.21200749278068542, -0.07244392484426498, -0.11383679509162903, 0.09758294373750687, -0.016461379826068878, -0.08525923639535904, 0.0899796262383461, -0.08555562794208527, 0.08146829903125763, 0.013679923489689827, 0.1993546038866043, -0.0008838929352350533, 0.10648155212402344, -0.035264208912849426, 0.10544108599424362, -0.0010774490656331182, -0.11964970082044601, 0.10586846619844437, 0.09363216161727905, -0.03886978700757027, -0.01781545579433441, 0.007595638278871775, -0.029825717210769653, -0.06671760231256485, -0.044806163758039474, -0.024103276431560516, 0.19761481881141663, 0.04267619550228119, -0.033977802842855453, -0.06419198215007782, -0.09781816601753235, -0.13296015560626984, 0.12559688091278076, -0.033558350056409836, -0.06782522052526474, 0.14513160288333893, -0.045995719730854034, -0.07934356480836868, 0.13191001117229462, -0.13808564841747284, -0.10136965662240982, 0.30556848645210266, -0.1770874708890915, -0.10944882780313492, -0.11130885779857635, -0.03294952213764191, -0.11298351734876633, 0.011231533251702785, -0.07356391102075577, -0.07342730462551117, -0.2674156725406647, -0.034611571580171585, -0.1002797782421112, -0.029340248554944992, -0.10570415109395981, -0.19118696451187134, -0.18683360517024994, -0.03162883594632149, -0.1841733753681183, -0.01145930029451847, -0.07979889959096909, 0.05124995857477188, -0.03867790848016739, 0.018713580444455147, -0.09798230975866318, -0.04805365577340126, 0.10645683109760284, 0.2158074826002121, -0.04156065359711647, 0.05038053169846535, -0.0965668261051178, -0.011125506833195686, -0.06269758939743042, -0.019653968513011932, 0.1381012350320816, -0.04463702440261841, 0.05824610963463783, -0.21224796772003174, 0.06254905462265015, -0.05296548455953598, -0.12219304591417313, 0.1442379504442215, 0.07931874692440033, 0.18547500669956207, 0.009084460325539112, 0.1050795242190361, -0.0032313938718289137, -0.12766551971435547, 0.081297867000103, -0.0899091586470604, 0.20953041315078735, -0.06226922571659088, -0.10887131094932556, 0.0008927946328185499, 0.17629975080490112, -0.20605884492397308, -0.05880904942750931, 0.042768239974975586, -0.10016443580389023, -0.06504906713962555, 0.12122254073619843, 0.06604829430580139, 0.004746046382933855, 0.02324516698718071, -0.24280281364917755, 0.11976280808448792, 0.034889962524175644, -0.1745682656764984, -0.3579939305782318, -0.12413319945335388, -0.015330640599131584, -0.07622332125902176, -0.05075691267848015, 0.060622841119766235, -0.19991262257099152, -0.03925703093409538, -0.03198110684752464, -0.025493714958429337, -0.21397773921489716, -0.2156282365322113, -0.03654487803578377, -0.05192817002534866, -0.02353796176612377, -0.10077383369207382, -0.21191532909870148, -0.1409045159816742, -0.11242478340864182, 0.02554350718855858, 0.17711055278778076, 0.05636599659919739, -0.0645587295293808, 0.0012532013934105635, 0.060629889369010925, 0.006194769870489836, 0.12336547672748566, -0.10204961150884628, -0.09226352721452713, -0.2952302098274231, 0.0798494964838028, -0.11092974990606308, 0.00568991806358099, 0.16466832160949707, 0.03519376367330551, -0.0664047971367836, 0.1764332354068756, 0.1308722198009491, 0.07340215891599655, 0.24294744431972504, 0.2460174411535263, -0.1786605566740036, 0.13787756860256195, 0.14870606362819672, -0.2577580213546753, -0.09542810916900635, -0.01697656325995922, -0.05074506253004074, -0.08685363084077835, 0.11454719305038452, -0.22079582512378693, 0.07920493930578232, 0.381583571434021, 0.21836698055267334, 0.04559425264596939, -0.13769997656345367, -0.24549560248851776, -0.15943898260593414, 0.010574373416602612, 0.3625278174877167, 0.10552696138620377, 0.25917673110961914, -0.31794509291648865, 0.51545250415802, -0.0938069224357605, -0.10010433942079544, -0.1568283587694168, 0.15699352324008942, -0.14412526786327362, -0.18226279318332672, -0.03928670659661293, -0.17805300652980804, 0.08211854845285416, -0.023268630728125572, -0.08772026002407074, -0.022965770214796066, -0.13189886510372162, -0.09860379248857498, -0.132491797208786, -0.008349012583494186, 0.08409804105758667, -0.054881516844034195, 0.3524135947227478, 0.02856251783668995, -0.10732532292604446, -0.05599292367696762, -0.21722732484340668, -0.21028321981430054, 0.01588623970746994, 0.05106953904032707, -0.019710199907422066, 0.053995609283447266, -0.10998876392841339, 0.06489122658967972, -0.0822196900844574, 0.02211354300379753, 0.027347898110747337, 0.22738824784755707, 0.09103724360466003, -0.21002539992332458, -0.374239444732666, -0.07622631639242172, 0.028878703713417053, -0.03411367908120155, -0.11228150874376297, 0.11123377084732056, -0.00040960634942166507, -0.168209046125412, -0.21538183093070984, -0.09673692286014557, 0.03296688199043274, -0.18493781983852386, -0.24377986788749695, -0.03584584966301918, -0.20365524291992188, -0.08110184967517853, -0.09473003447055817, -0.14628233015537262, 0.035233110189437866, -0.07530377060174942, -0.09637747704982758, -0.187840074300766, -0.13194626569747925, 0.012732066214084625, -0.18474619090557098, -0.03668012097477913, -0.23137234151363373, -0.14594697952270508, 0.10647423565387726, -0.15945574641227722, -0.11135172843933105, 0.08249715715646744, 0.0014035269850865006, -0.06364815682172775, -0.2386191487312317, -0.2947337031364441, -0.23829926550388336, 0.04734022170305252, -0.15847715735435486, -0.21416445076465607, -0.06671164929866791, -0.1674090325832367, 0.12877054512500763, -0.37922176718711853, 0.026661880314350128, -0.11473175883293152, -0.21487173438072205, -0.16045396029949188, -0.15889020264148712, 0.22897225618362427, 0.044499557465314865, -0.23490095138549805, -0.2165910005569458, -0.14083229005336761, -0.09632787108421326, -0.07368391752243042, -0.0519171841442585, -0.1685420274734497, 0.022756578400731087, -0.24604511260986328, 0.16129446029663086, -0.08002405613660812, -0.1320091187953949, 0.12271440774202347, -0.08032786101102829, -0.05430717393755913, -0.023259809240698814, -0.3186618983745575, -0.0785788744688034, -0.08374945819377899, -0.07464133203029633, 0.012167878448963165, -0.2565610110759735, -0.2169344574213028, 0.05366167053580284, -0.18845050036907196, -0.04222089424729347, 0.028086427599191666, 0.11031413078308105, 0.10690081864595413, 0.08346258848905563, -0.1673405021429062, -0.08472941815853119, -0.0417446494102478, 0.04563302919268608, -0.40996214747428894, -0.025852801278233528, -0.2498367428779602, -0.12557421624660492, -0.10193336755037308, 0.08186890184879303, -0.0995662659406662, 0.15330059826374054, -0.04801240935921669, -0.20194880664348602, -0.07779702544212341, -0.00040713412454351783, -0.07997824251651764, 0.1523686647415161, -0.14792291820049286, 0.009672896936535835, -0.22299528121948242, 0.2474299520254135, -0.08121424168348312, 0.023411842063069344, -0.04772630333900452, 0.23105551302433014, 0.0051245978102087975, -0.1892634779214859, 0.09555568546056747, -0.05734126642346382, 0.036611221730709076, 0.10814918577671051, -0.03778620809316635, 0.010335545055568218, 0.2466488927602768, 0.377991646528244, 0.10005205124616623, -0.24783477187156677, -0.16857822239398956, -0.05320459604263306, -0.020183037966489792, 0.08222763240337372, 0.23507709801197052, 0.11175485700368881, -0.22515538334846497, 0.03864249214529991, -0.10461587458848953, 0.036810267716646194, 0.06893516331911087, 0.06479127705097198, 0.036119718104600906, -0.039629463106393814, 0.04828116297721863, 0.13145266473293304, 0.11873872578144073, -0.21967144310474396, 0.20701654255390167, 0.14610324800014496, -0.05284487456083298, 0.05251626297831535, -0.045923862606287, -0.0566396601498127, -0.03023628145456314, 0.03365490958094597, 0.104778952896595, -0.011257450096309185, -0.008740628138184547, -0.12298398464918137, 0.03567476198077202, -0.21252135932445526, 0.0023970454931259155, 0.10669651627540588, 0.04482998326420784, 0.006751984357833862, 0.13987022638320923, 0.08699989318847656, -0.11094430834054947, -0.14630205929279327, 0.0452674999833107, 0.09467548131942749, -0.023995645344257355, -0.17237047851085663, -0.03672901168465614, -0.07792896032333374, 0.10564462840557098, -0.006152512971311808, -0.031402260065078735, 0.16342732310295105, -0.140875443816185, -0.15704037249088287, -0.11970552057027817, 0.10674506425857544, 0.22434014081954956, 0.12663018703460693, -0.0676817074418068, 0.15362763404846191, -0.25311771035194397, -0.11510413885116577, -0.1379132866859436, 0.04028445854783058, 0.03840525820851326, -0.2014947086572647, -0.10844951122999191, 0.041343167424201965, 0.002804408548399806, 0.38165369629859924, 0.052913788706064224, 0.08512276411056519, 0.11009617894887924, -0.20458540320396423, -0.06363768130540848, 0.060633640736341476, 0.19849148392677307, 0.0441080741584301, -0.20695899426937103, 0.1329679638147354, 0.1625562608242035, 0.04699090123176575, 0.13073843717575073, 0.17509347200393677, -0.2098722606897354, 0.11803549528121948, 0.004555527586489916, -0.09699710458517075, 0.05103990435600281, -0.07020693272352219, -0.039195310324430466, 0.0407739095389843, 0.04889223352074623, 0.19401861727237701, 0.11655859649181366, -0.05712970346212387, 0.2074047029018402, 0.03667159378528595, 0.03560204803943634, 0.09185931086540222, -0.23805184662342072, 0.26285651326179504, -0.1744571179151535, 0.3971931040287018, 0.09980997443199158, 0.15005096793174744, -0.1931898444890976, 0.019536158069968224, -0.12782230973243713, -0.033742327243089676, 0.3348870575428009, -0.09543301165103912, 0.20889559388160706, 0.10930567979812622, 0.11981189996004105, 0.011595277115702629, 0.13886910676956177, 0.06908541172742844, -0.015652330592274666, 0.05981434881687164, 0.09298189729452133, -0.04519376903772354, -0.035972557961940765, 0.16701042652130127, 0.058502282947301865, 0.40563544631004333, 0.07106824219226837, -0.13640597462654114, -0.09427063167095184, -0.18135806918144226, 0.11828956753015518, 0.2014072686433792, -0.1714925318956375, -0.00779963843524456, -0.17498904466629028, 0.05133168026804924, -0.14463286101818085, -0.18706081807613373, 0.10428071767091751, -0.025157906115055084, 0.18654794991016388, -0.02766568586230278, 0.17428907752037048, 0.12362056970596313, -0.09714709222316742, 0.06783648580312729, 0.21544137597084045, 0.3431684076786041, -0.16061951220035553, -0.09523998945951462, 0.10731611400842667, 0.2103576511144638, 0.11994071304798126, 0.0724429190158844, -0.022846955806016922, -0.18799863755702972, 0.3072085976600647, 0.14458338916301727, 0.1784280687570572, -0.05948600545525551, 0.20834730565547943, 0.08326539397239685, 0.03069140389561653, -0.228152334690094, -0.11470839381217957, 0.3847076892852783, 0.09657549858093262, 0.06791560351848602, -0.048155371099710464, 0.1757519692182541, -0.226752370595932, 0.024862539023160934, 0.08476019650697708, 0.07008364051580429, 0.13880318403244019, 0.01687486656010151, -0.16095447540283203, -0.2608054280281067, 0.019396133720874786, -0.0029865424148738384, 0.14488378167152405, -0.09662608802318573, 0.28073441982269287, 0.13324372470378876, 0.10461640357971191, 0.3268146812915802, 0.1273123323917389, 0.10205663740634918, -0.31504058837890625, -0.13309574127197266, 0.006852669641375542, 0.13196341693401337, 0.25867658853530884, 0.07283344119787216, -0.01856488548219204, 0.06296124309301376, -0.21192921698093414, 0.00676768459379673, 0.14357657730579376, -0.05213678255677223, 0.07367885857820511, 0.27282917499542236, -0.2669588625431061, 0.2845967710018158, -0.08233139663934708, 0.15550115704536438, 0.031733617186546326, -0.0006269855075515807, -0.11502380669116974, -0.05232802405953407, -0.14287890493869781, 0.34818118810653687, -0.008532938547432423, -0.11394931375980377, 0.22223466634750366, -0.19706924259662628, -0.05314265936613083, -0.1638951301574707, -0.1926579475402832, -0.15165704488754272, 0.15640664100646973, 0.13380111753940582, 0.026980478316545486, -0.21595221757888794, 0.3946945071220398, 0.010199529118835926, 0.08143142610788345, -0.061695195734500885, -0.19968105852603912, -0.1383352279663086, 0.12017793953418732, 0.20028744637966156, -0.1158493161201477, -0.11418174207210541, -0.28678566217422485, -0.20373789966106415, 0.0061942501924932, 0.11256180703639984, -0.08048895746469498, 0.1777796745300293, 0.1181747242808342, -0.20121389627456665, 0.07929857820272446, 0.052908431738615036, 0.028923112899065018, 0.08716384321451187, 0.026941651478409767, -0.028837861493229866, 0.19423578679561615, -0.1358785629272461, -0.24447716772556305, -0.09575164318084717, -0.2811458706855774, -0.1731741726398468, -0.2163284718990326, 0.007110111881047487, -0.08382859081029892, 0.1828824132680893, -0.0174277201294899, -0.23357178270816803, 0.14871203899383545, -0.14178037643432617, -0.03376460447907448, -0.09841632843017578, 0.18788301944732666, 0.08290159702301025, -0.10105623304843903, 0.013045812025666237, 0.13630792498588562, 0.017114320769906044, -0.015052451752126217, -0.24134230613708496, 0.3313850462436676, -0.19679893553256989, -0.2731435298919678, -0.14913228154182434, 0.15476253628730774, 0.1519676148891449, 0.031077053397893906, 0.31716039776802063, 0.05812589451670647, 0.16680243611335754, 0.04347672313451767, 0.09593769907951355, -0.04939422383904457, 0.23149238526821136, 0.012731591239571571, -0.0027833380736410618, 0.10990773141384125, -0.19779253005981445, -0.03759708255529404, -0.026758873835206032, -0.03822646662592888, -0.017896533012390137, 0.06777212023735046, -0.027854610234498978, -0.1716809868812561, -0.07787058502435684, 0.18177010118961334, -0.1598956286907196, -0.22870299220085144, -0.4298992455005646, -0.0035400541964918375, -0.10586871206760406, 0.014027920551598072, -0.1090395525097847, 0.0568910613656044, -0.2894187867641449, 0.04622155427932739, -0.2275247424840927, 0.2405732274055481, -0.1308203935623169, 0.047960445284843445, 0.09876298159360886, 0.11962521076202393, -0.07795285433530807, 0.0803566575050354, -0.18613891303539276, 0.011294372379779816, 0.07174526900053024, -0.21394984424114227, -0.13782812654972076, 0.10676808655261993, 0.274554580450058, 0.036061741411685944, -0.054950591176748276, -0.30012914538383484, 0.04763862118124962, -0.09634397178888321, -0.04081907495856285, -0.15578435361385345, -0.11194288730621338, -0.03981555625796318, -0.22408299148082733, -0.11898469924926758, -0.1685105264186859, -0.06375128775835037, -0.24022117257118225, 0.18524304032325745, -0.031043246388435364, 0.11016478389501572, 0.29260319471359253, -0.30613693594932556, 0.06689124554395676, -0.09016037732362747, -0.20510026812553406, -0.06449717283248901, -0.04827791452407837, -0.014668561518192291, -0.23151074349880219, 0.08049978315830231, -0.010814974084496498, 0.19737213850021362, 0.1349066197872162, -0.007332647684961557, -0.014729626476764679, -0.037798281759023666, -0.05349192023277283, -0.038473986089229584, 0.1284932792186737, -0.1228642463684082, 0.014974335208535194, -0.13253158330917358, -0.03311396762728691, 0.11038241535425186, -0.08606777340173721, 0.0749795213341713, 0.028294110670685768, 0.19439546763896942, 0.10269869118928909, -0.17243023216724396, -0.034776318818330765, -0.037219349294900894, 0.055440373718738556, -0.05592508986592293, 0.005879088770598173, -0.15421262383460999, 0.18283937871456146, 0.2492365539073944, -0.3468974828720093, 0.2797120213508606, -0.17529398202896118, -0.07658214122056961, -0.05315013602375984, 0.10229099541902542, 0.017884796485304832, -0.13996019959449768, -0.09841442108154297, 0.014493848197162151, -0.04254771023988724, -0.07091137021780014, 0.14200296998023987, -0.17341257631778717, -0.010881266556680202, -0.09869203716516495, 0.026029987260699272, 0.10476966947317123, -0.1545199304819107, -0.04310230910778046, 0.10321097075939178, -0.03171012923121452, 0.14876899123191833, -0.08159492909908295, -0.03875751793384552, 0.018178749829530716, 0.008011155761778355, -0.036616116762161255, 0.1293802708387375, 0.09924478083848953, 0.12706053256988525, 0.04163679480552673, 0.09851649403572083, 0.061899732798337936, -0.035383373498916626, 0.016124214977025986, 0.19739697873592377, -0.35088372230529785, 0.10409924387931824, 0.5317655205726624, -0.13579626381397247, -0.07127414643764496, -0.06595007330179214, 0.014199024997651577, 0.02948383055627346, -0.1538417637348175, -0.18695899844169617, 0.11272890120744705, 0.09245898574590683, -0.043724656105041504, -0.015014932490885258, -0.04162199795246124, -0.2677142024040222, -0.10650099813938141, 0.22480037808418274, -0.022076420485973358, -0.01402670331299305, -0.18895560503005981, -0.025942906737327576, 0.08562640100717545, 0.08400382846593857, -0.004495312925428152, 0.11657723784446716, 0.10636545717716217, -0.09326139092445374, 0.1246955618262291, -0.12215737253427505, 0.001600043848156929, -0.10681165009737015, 0.09546832740306854, -0.18919548392295837, 0.1528262495994568, -0.09633637964725494, -0.017885973677039146, 0.03822486475110054, 0.48603004217147827, -0.018689442425966263, 0.0659046471118927, 0.06575889140367508, 0.01495453529059887, -0.1304161697626114, 0.020920397713780403, -0.27420076727867126, -0.21836857497692108, 0.05231623351573944, -0.10737699270248413, -0.0639515370130539, 0.18395142257213593, 0.006162736564874649, 0.45711469650268555, 0.04696669429540634, 0.14240965247154236, 0.19798295199871063, -0.1584036648273468, -0.24650312960147858, 0.024711905047297478, 0.06963308900594711, -0.1080838069319725, -0.11459788680076599, 0.182297483086586, -0.005062439478933811, -0.004820443224161863, -0.24537929892539978, 0.003963866736739874, -0.014747876673936844, 0.3868563175201416, -0.013732410036027431, 0.05247851461172104, -0.035499997437000275, 0.011144855059683323, -0.016907844692468643, -0.0780383050441742, -0.07120479643344879, 0.14729717373847961, -0.0420297235250473, -0.2023034244775772, -0.379319429397583, -0.028903303667902946, 0.4170810282230377, 0.2452915608882904, 0.29081347584724426, -0.10701199620962143, -0.07325299084186554, -0.04146970808506012, 0.00853791180998087, 0.08086137473583221, 0.06714565306901932, -0.04885144531726837, 0.08426101505756378, 0.0066073895432055, -0.11939124763011932, -0.343338280916214, -0.06575074791908264, -0.26124006509780884, -0.011365118436515331, -0.19934794306755066, 0.33032870292663574, 0.03442271053791046, 0.16524918377399445, -0.10764867067337036, 0.008908214047551155, -0.07161494344472885, 0.02223428152501583, 0.22333000600337982, 0.003505000378936529, -0.010064012371003628, -0.12875226140022278, 0.07797795534133911, -0.16779345273971558, -0.17599347233772278, -0.29075226187705994, -0.04361385852098465, -0.08672810345888138, -0.07155226171016693, 0.022443514317274094, -0.02954334206879139, -0.2132023274898529, 0.06955260783433914, 0.07449055463075638, 0.07946084439754486, 0.02413776144385338, -0.23740853369235992, 0.2995862364768982, 0.10037440061569214, -0.025470759719610214, 0.13599777221679688, 0.29307931661605835, -0.017047302797436714, -0.12318942695856094, 0.24310287833213806, 0.25499269366264343, 0.28940609097480774, 0.04596932604908943, -0.14659054577350616, 0.18514606356620789, -0.10346098244190216, 0.16804024577140808, -0.1307300478219986, -0.39933884143829346, 0.01206453237682581, -0.008459357544779778, -0.02894539013504982, 0.038739804178476334, -0.35111021995544434, 0.19043001532554626, -0.02769538015127182, -0.2768253684043884, -0.06599606573581696, -0.09048900008201599, -0.16583053767681122, 0.06645984202623367, 0.30192580819129944, -0.1781436651945114, 0.11453759670257568, 0.058208394795656204, -0.3141299784183502, 0.12764599919319153, -0.07373149693012238, -0.030601471662521362, -0.07704094052314758, 0.17719332873821259, 0.2648853659629822, -0.03384007140994072, -0.13681288063526154, -0.19171324372291565, -0.021558549255132675, -0.0629672035574913, -0.07738427817821503, 0.10229498893022537, -0.21564552187919617, -0.22876736521720886, -0.1170186772942543, -0.028803300112485886, -0.08775798976421356, 0.14572486281394958, 0.26153790950775146, -0.009799770079553127, -0.14336487650871277, 0.03512781113386154, -0.2021375596523285, -0.00636669248342514, 0.06363148242235184, -0.17115646600723267, 0.09829138219356537, -0.22165973484516144, 0.15044564008712769, -0.14902280271053314, -0.13491979241371155, -0.11427657306194305, 0.09591109305620193, 0.2217516303062439, 0.07263220101594925, -0.05170862004160881, -0.13654661178588867, 0.13008294999599457, -0.2119632214307785, -0.1586805284023285, -0.08590346574783325, -0.06711293011903763, -0.04322243109345436, -0.24470743536949158, -0.15087604522705078, -0.1168941855430603, 0.08144133538007736, 0.14878922700881958, -0.025531841441988945, 0.0029382435604929924, -0.4887523651123047, -0.4407784342765808, 0.11095951497554779, 0.06113763898611069, -0.08870159089565277, -0.12033003568649292, 0.06428821384906769, -0.043650053441524506, -0.05095565691590309, -0.1785842329263687, -0.09959404915571213, -0.07724711298942566, -0.06254276633262634, 0.09765785932540894, 0.12019205093383789, -0.18217961490154266, -0.032896894961595535, -0.0528145395219326, 0.08996786177158356, -0.1379658728837967, 0.19194766879081726, -0.19955481588840485, -0.07274805754423141, -0.012663538567721844, -0.18950022757053375, -0.03417889028787613, -0.04529827833175659, 0.2537873089313507, -0.026125678792595863, 0.15290001034736633, -0.36062073707580566, 0.2661583423614502, -0.13961200416088104, 0.055690180510282516, 0.0177727323025465, -0.05661247670650482, -0.22417227923870087, 0.19058401882648468, -0.25421106815338135, -0.1626909077167511, -0.12717294692993164, -0.013119679875671864, 0.2960904836654663, -0.08806953579187393, -0.12404774129390717, 0.23741331696510315, -0.2461491823196411, -0.17211078107357025, -0.07476463168859482, -0.06567580997943878, -0.20032162964344025, -0.2262384295463562, -0.4370329976081848, -0.13611558079719543, 0.0032811113633215427, -0.07301206886768341, -0.13966156542301178, -0.2494676113128662, -0.2824593484401703, 0.21031337976455688, -0.08591927587985992, -0.09040763229131699, 0.06543954461812973, -0.12322722375392914, -0.1622716635465622, -0.1471734493970871, -0.08465830236673355, 0.4115036725997925, -0.17066042125225067, 0.34984707832336426, 0.08383020013570786, -0.20958508551120758, -0.39979350566864014, -0.0017301056068390608, 0.04112601652741432, -0.2947281301021576, 0.044857561588287354, 0.07068571448326111, -0.069804348051548, -0.181029811501503, -0.12594404816627502, -0.3606204390525818, -0.1946055293083191, -0.17856435477733612, 0.10791037976741791, 0.0230660792440176, -0.23052679002285004, -0.23410922288894653, 0.04827852174639702, -0.1630096584558487, 0.219631627202034, 0.10653994232416153, 0.04636356979608536, 0.025706011801958084, 0.1438504159450531, -0.08982286602258682, -0.26201337575912476, -0.2728247046470642, 0.010090641677379608, 0.019419614225625992, -0.1006038635969162, -0.05092015117406845, -0.12861520051956177, 0.010296352207660675, -0.043623603880405426, 0.05820620059967041, -0.05080948770046234, -0.02220994234085083, -0.1429426521062851, 0.06276881694793701, -0.20632854104042053, -0.3228924572467804, -0.014684160239994526, 0.040013350546360016, 0.14271101355552673, 0.24099323153495789, -0.019132420420646667, -0.21143518388271332, 0.03210611268877983, -0.2218005508184433, 0.014967044815421104, 0.2876752018928528, 0.03383707255125046, -0.22921009361743927, -0.043550048023462296, -0.020054103806614876, 0.1538088172674179, -0.1436236947774887, -0.17459942400455475, 0.1729709506034851, -0.03743608668446541, -0.17681175470352173, -0.013024567626416683, -0.09434086829423904, 0.002957196906208992, -0.1458985060453415, -0.09109784662723541, -0.07591938972473145, -0.03390922769904137, -0.10980337113142014, 0.033422619104385376, 0.26730892062187195, 0.07666553556919098, 0.03135490044951439, -0.2058931589126587, -0.04811030626296997, -0.0631660670042038, -0.031065797433257103, -0.19251926243305206, -0.012930468656122684, -0.04637012258172035, -0.11180123686790466, 0.007051803171634674, -0.028913408517837524, 0.1836632341146469, -0.011248479597270489, -0.19489659368991852, 0.00425210315734148, -0.017284205183386803, -0.08115774393081665, -0.11757829785346985, -0.06772030144929886, 3.981837835453916e-06, 0.3277122676372528, 0.2054969221353531, -0.03765708953142166, -0.13786786794662476, 0.009680076502263546, 0.42079958319664, 0.06721055507659912, -0.1324440985918045, -0.0871744304895401, 0.00795383658260107, -0.10339313000440598, -0.08135107904672623, -0.1466263234615326, 0.05779195949435234, -0.13638576865196228, 0.13016748428344727, 0.2748507559299469, -0.1520368754863739, -0.0073784044943749905, -0.10232928395271301, 0.1375507265329361, 0.408372700214386, -0.024204431101679802, -0.17901962995529175, 0.017622489482164383, -0.06241517886519432, 0.08716771751642227, 0.025689151138067245, 0.17638017237186432, 0.03356845676898956, 0.12202136218547821, 0.009452087804675102, -0.1941642165184021, 0.15665416419506073, -0.048217903822660446, -0.1887226551771164, -0.11710190027952194, 0.11645636707544327, -0.050379082560539246, 0.15400463342666626, -0.15614676475524902, 0.21987418830394745, 0.14309747517108917, -0.0425347164273262, 0.05426487699151039, -0.07115847617387772, 0.07215776294469833, 0.19920893013477325, -0.06563283503055573, -0.0941338986158371, -0.16990281641483307, -0.08724118769168854, -0.06522844731807709, -0.1901375651359558, -0.1601182520389557, -0.32097411155700684, -0.013833967968821526, -0.06577026844024658, -0.23885954916477203, 0.16292160749435425, 0.10385031998157501, -0.09773562103509903, -0.08516775816679001, 0.13404111564159393, 0.03481920808553696, 0.0783575102686882, -0.016174107789993286, 0.06992650032043457, -0.06910231709480286, -0.08228480815887451, 0.09716411679983139, 0.38179799914360046, 0.061593249440193176, -0.059887368232011795, 0.031242607161402702, 0.013575595803558826, 0.048727795481681824, -0.04723402112722397, -0.027177048847079277, 0.04368168115615845, -0.2116585075855255, -0.0039051854982972145, 0.11509932577610016, -0.060286469757556915, -0.07081487029790878, -0.09784439206123352, 0.08122866600751877, -0.033024534583091736, 0.013634679839015007, 0.031340960413217545, 0.016333451494574547, -0.05220749229192734, -0.14072160422801971, 0.301128625869751, -0.032302383333444595, -0.0848497599363327, -0.02225641906261444, 0.09720457345247269, -0.08020850270986557, 0.04512828215956688, 0.09693007916212082, -0.0281521026045084, -0.002155493712052703, -0.1059114933013916, -0.2485569715499878, -0.03297508880496025, -0.08898372948169708, 0.12574008107185364, -0.2640308141708374, 0.3325803577899933, 0.19670963287353516, 0.11414309591054916, 0.029877036809921265, -0.13532117009162903, 0.19137124717235565, 0.22456985712051392, -0.1612645387649536, -0.09113439917564392, 0.12428000569343567, -0.017644954845309258, 0.0034231527242809534, 0.07049325853586197, 0.21105420589447021, 0.053279031068086624, -0.004327715840190649, -0.06519988924264908, 0.011236608028411865, -0.10912743210792542, 0.1284307986497879, -0.02523978240787983, -0.05895382538437843, 0.07988736778497696, 0.05239428952336311, -0.08082475513219833, 0.20510229468345642, 0.04288851097226143, -0.10679001361131668, -0.03897102177143097, -0.1345692276954651, 0.03972078487277031, -0.12085451185703278, -0.061076413840055466, 0.013482187874615192, 0.20096749067306519, -0.08802953362464905, -0.004255021922290325, -0.022463358938694, 0.07125210762023926, 0.05027490481734276, -0.35543617606163025, 0.11628801375627518, -0.1662174016237259, -0.21644827723503113, -0.016654960811138153, 0.2620694637298584, -0.12684279680252075, 0.03068523295223713, 0.24785250425338745, 0.06083820015192032, 0.13905392587184906, -0.024155225604772568, 0.12519629299640656, 0.2077779471874237, 0.0010488091502338648, 0.014100540429353714, 0.004980597645044327, -0.0793733298778534, -0.1407337635755539, 0.11682742834091187, 0.009420186281204224, 0.008154759183526039, -0.07256314158439636, -0.05141313746571541, -0.27953165769577026, 0.04260258004069328, -0.016079585999250412, -0.12448849529027939, -0.020987646654248238, -0.0018175909062847495, 0.0681256502866745, 0.13115859031677246, -0.2712840735912323, -0.20543299615383148, -0.016236478462815285, 0.03613808751106262, -0.15177036821842194, 0.14590738713741302, 0.16203024983406067, -0.10538521409034729, -0.0070405323058366776, -0.0889069214463234, -0.04464332014322281, -0.08432371914386749, -0.06993035972118378, -0.01932222954928875, 0.35557010769844055, 0.00038117007352411747, -0.06426142901182175, 0.08963436633348465, 0.12020180374383926, -0.10994579643011093, 0.023263823240995407, 0.04831084609031677, -0.05045212805271149, -0.049380917102098465, 0.007390582002699375, 0.03593280538916588, -0.04638014733791351, 0.04675242677330971, -0.1174279972910881, 0.02959059178829193, 0.03153722733259201, 0.11119868606328964, -0.2657746374607086, 0.15815837681293488, -0.0035658779088407755, -0.0942004844546318, -0.08271462470293045, 0.021608920767903328, -0.006961979903280735, -0.04903894662857056, 0.01985539123415947, -0.05357229337096214, -0.11052180826663971, -0.13606572151184082, 0.006862967275083065, -0.04671342298388481, 0.039717696607112885, 0.1111239418387413, -0.019168328493833542, 0.00407870439812541, -0.002243903698399663, 0.18128018081188202, -0.034491416066884995, 0.007183261215686798, -0.0439889058470726, -0.1472482979297638, -0.084568090736866, -0.14144407212734222, -0.025272833183407784, -0.042836423963308334, -0.08772309124469757, -0.09435191005468369, 0.086748868227005, 0.23612524569034576, -0.009722355753183365, -0.11790770292282104, 0.2094801366329193, 0.012121954932808876, -0.06973625719547272, 0.27662962675094604, 0.013895811513066292, 0.08442129194736481, 0.028795314952731133, -0.1857175976037979, 0.3373657166957855, -0.15932415425777435, -0.08577850461006165, 0.027633357793092728, -0.04269957169890404, -0.022353649139404297, 0.0021626693196594715, -0.09504496306180954, 0.05614757910370827, -0.11918275058269501, -0.03869958221912384, 0.012209095992147923, 0.0002695671864785254, -0.18704316020011902, -0.03153865411877632, -0.13998787105083466, -0.14742359519004822, -0.0718851163983345, 0.040203601121902466, -0.1486233025789261, -0.07357826828956604, 0.2426411360502243, -0.021118246018886566, 0.016476312652230263, -0.23738490045070648, -0.23360760509967804, 0.11496583372354507, 0.012220874428749084, -0.04778033867478371, -0.004977620206773281, 0.018858522176742554, -0.17102548480033875, 0.001521890051662922, -0.1702517569065094, -0.02132454141974449, -0.12166859209537506, -0.1440417468547821, -0.11410621553659439, -0.0012980486499145627, -0.049152325838804245, -0.17307154834270477, -0.22007523477077484, -0.1118234172463417, -0.08565887063741684, -0.0157217588275671, -0.0012918580323457718, -0.0011203832691535354, -0.08417715132236481, 0.07469706982374191, -0.10270550847053528, -0.026506438851356506, -0.018504435196518898, 0.0889618992805481, 0.14202946424484253, 0.048466045409440994, 0.14590854942798615, 0.14333891868591309, 0.0050796810537576675, 0.21737483143806458, 0.08581121265888214, -0.15396960079669952, 0.09681016951799393, 0.06670941412448883, 0.15482622385025024, 0.06014356017112732, 0.02291972003877163, -0.10628165304660797, -0.09693378210067749, 0.03225725144147873, -0.11698533594608307, -0.04990614578127861, -0.0024143951013684273, -0.02888079173862934, -0.013469675555825233, 0.028722763061523438, -0.10667432099580765, -0.133246049284935, 0.023898378014564514, 0.19430793821811676, 0.1825699359178543, 0.05037621408700943, -0.03183514624834061, 0.013051295652985573, 0.09402640908956528, 0.0663069635629654, -0.023451000452041626, -0.12860289216041565, 0.05507688596844673, 0.013556085526943207, -0.033222027122974396, -0.2380891740322113, 0.15814349055290222, 0.18575216829776764, -0.033476464450359344, -0.2134111225605011, 0.0006512202671729028, -0.16769924759864807, 0.12583810091018677, -0.07350684702396393, 0.13215067982673645, 0.11817770451307297, 0.03446503356099129, -0.0557456836104393, -0.0381767675280571, 0.12819194793701172, -0.24058809876441956, -0.15235982835292816, -0.1747155338525772, 0.12071654945611954, -0.022931857034564018, 0.019297752529382706, -0.048619069159030914, 0.09718504548072815, 0.08348918706178665, 0.13283003866672516, -0.04503203555941582, -0.0768195167183876, -0.002352690789848566, 0.2535618543624878, 0.12845925986766815, -0.23930002748966217, 0.034672968089580536, 0.0559900626540184, -0.22540047764778137, -0.061088643968105316, 0.012799296528100967, -0.043439146131277084, 0.0033001042902469635, 0.06448078900575638, 0.24979929625988007, -0.0023384690284729004, -0.06357462704181671, 0.10866234451532364, -0.003821918275207281, 0.09391747415065765, 0.1388445347547531, 0.0015912195667624474, 0.02449384331703186, -0.030215300619602203, 0.18639782071113586, -0.06900186836719513, -0.10680236667394638, -0.014304447919130325, -0.07562879472970963, -0.061570197343826294, 0.06426168233156204, -0.18538257479667664, 0.2536323070526123, -0.05121491104364395, -0.20197029411792755, -0.10232062637805939, -0.005535878706723452, 0.1947186142206192, 0.07130511105060577, 0.0795750617980957, 0.04472238942980766, -0.04260816052556038, 0.1642591953277588, 0.031140493229031563, 0.0760309025645256, 0.10458206385374069, -0.05073867738246918, 3.741536784218624e-05, 0.06265324354171753, -0.04245559126138687, 0.08110060542821884, 0.10094362497329712, -0.1414773166179657, -0.025834256783127785, 0.1850048154592514, -0.11580401659011841, 0.018018681555986404, -0.1475428342819214, -0.030814891681075096, 0.004294862505048513, 0.07186176627874374, 0.3836615979671478, 0.23899158835411072, -0.05412508547306061, -0.08146809041500092, 0.11665355414152145, -0.03003840520977974, 0.2183881253004074, -0.012058778665959835, -0.06540019065141678, 0.018950698897242546, -0.0035194193478673697, -0.010925616137683392, -0.0795876532793045, 0.13021716475486755, -0.011219639331102371, 0.03226809203624725, 0.06938015669584274, -0.057459890842437744, 0.11325705051422119, -0.0057475389912724495, -0.042821671813726425, 0.149769589304924, -0.1876090168952942, -0.12948103249073029, -0.1952963024377823, 0.06411285698413849, 0.029370443895459175, -0.0013894162839278579, 0.06726393103599548, -0.031147897243499756, -0.10753889381885529, -0.28309863805770874, -0.08742844313383102, -0.1232578381896019, -0.05576818436384201, 0.03344650939106941, -0.05848556384444237, -0.10307803750038147, 0.23533682525157928, 0.22037802636623383, -0.0018207250395789742, -0.18915265798568726, -0.08106671273708344, -0.07491445541381836, 0.0816478356719017, -0.03822121024131775, -0.07682330161333084, -0.3441135585308075, 0.24562326073646545, 0.026563970372080803, 0.2710430324077606, -0.06962085515260696, -0.01931525021791458, -0.07413197308778763, -0.03641129657626152, 0.03558526188135147, -0.14729364216327667, 0.014829554595053196, -0.03622877597808838, -0.027083760127425194, 0.07785074412822723, -0.10918904840946198, -0.06430113315582275, -0.04488176107406616, 0.26875540614128113, 0.06615845113992691, 0.07410301268100739, -0.2296171933412552, 0.01042389590293169, 0.17666451632976532, -0.04955465719103813, 0.0690516009926796, 0.0772404596209526, -0.012639876455068588, 0.2793359160423279, 0.005463485140353441, -0.0950966626405716, -0.03793109580874443, -0.14783774316310883, -0.057460661977529526, 0.07714291661977768, 0.007240889593958855, -0.1351463496685028, 0.020207203924655914, 0.08389227092266083, 0.1643865704536438, 0.14631237089633942, 0.08649016916751862, 0.17509672045707703, 0.0393500030040741, 0.06473591178655624, 0.03359014540910721, 0.011859784834086895, 0.13708831369876862, -0.002065215725451708, 0.07056193798780441, -0.13933272659778595, 0.09351518005132675, 0.03289187327027321, -0.047194406390190125, -0.15516850352287292, 0.09803157299757004, 0.24684275686740875, 0.18111033737659454, 0.2971973419189453, -0.0965213030576706, -0.04128056764602661, -0.03892552852630615, -0.1043519601225853, 0.24311181902885437, -0.08529563993215561, 0.05652627348899841, 0.12322351336479187, 0.2616921365261078, 0.08769116550683975, 0.14875590801239014, -0.04038116708397865, 0.01768633909523487, 0.13410164415836334, -0.20284204185009003, 0.005703683942556381, 0.25887733697891235, 0.24610143899917603, 0.18395790457725525, 0.010832495056092739, 0.11537778377532959, -0.19947625696659088, -0.0627746656537056, -0.08774939179420471, -0.030194977298378944, 0.01149701327085495, 0.20931212604045868, 0.06401337683200836, -0.12110449373722076, -0.25637704133987427, -0.43561235070228577, 0.401400089263916, 0.04281720519065857, -0.015159024856984615, 0.07844174653291702, 0.09683074057102203, 0.37714752554893494, -0.023781388998031616, -0.02432303875684738, 0.19492307305335999, 0.013063633814454079, -0.18681469559669495, 0.2959088087081909, 0.09964866936206818, 0.007453827653080225, 0.030601389706134796, -0.038704875856637955, -0.08772192895412445, 0.2009788304567337, -0.04526038467884064, -0.0658915787935257, -0.20244495570659637, 0.02865227870643139, 0.037166982889175415, 0.0342244990170002, -0.07695966958999634, 0.28955957293510437, -0.09282999485731125, 0.07804393768310547, -0.06006285175681114, -0.09480379521846771, -0.3505414128303528, -0.24879135191440582, -0.08158338069915771, -0.2175426036119461, 0.10493393242359161, -0.41797399520874023, -0.07018472254276276, -0.023250441998243332, -0.16843485832214355, -0.12660104036331177, -0.012582385912537575, -0.08633287250995636, 0.058188263326883316, -0.2606879770755768, 0.02396436035633087, 0.04950655251741409, -0.0009111826657317579, -0.005180651787668467, -0.07372856140136719, 0.0663868710398674, 0.07301830500364304, -0.027370711788535118, 0.02389567159116268, -0.07163813710212708, -0.44445693492889404, 0.16947264969348907, -0.11564035713672638, -0.17724311351776123, -0.1506311148405075, -0.1630278080701828, 0.010244453325867653, -0.13141290843486786, -0.3045956492424011, -0.254354327917099, -0.161461740732193, 0.11831570416688919, -0.1567194014787674, -0.0622108094394207, 0.004652172792702913, 0.0031055377330631018, -0.04268505424261093, -0.3121281862258911, -0.06498311460018158, -0.15765392780303955, -0.2939513921737671, 0.13051661849021912, -0.07594975084066391, -0.016655951738357544, -0.15405447781085968, -0.17645643651485443, -0.08134955167770386, 0.09214163571596146, -0.023905474692583084, -0.06346207112073898, 0.064228355884552, 0.18017618358135223, -0.01860339753329754, -0.1013285294175148, -0.13516314327716827, -0.18815593421459198, -0.11587788909673691, -0.27399274706840515, 0.06695199757814407, 0.023622428998351097, -0.13826431334018707, 0.01935478486120701, 0.15002350509166718, 0.20695994794368744, 0.03368907421827316, -0.20131227374076843, -0.35195082426071167, -0.2061978429555893, -0.2810992896556854, -0.027660386636853218, -0.052273690700531006, 0.19482974708080292, -0.08405792713165283, 0.04694811999797821, 0.09389553964138031, 0.11373575031757355, -0.264463871717453, 0.054880332201719284, -0.2867427170276642, -0.23857708275318146, 0.11398234218358994, -0.10173299163579941, 0.13540691137313843, -0.1152229756116867, -0.0779583752155304, -0.25451090931892395, -0.2518540620803833, -0.17657211422920227, -0.033480893820524216, -0.16874319314956665, 0.08920621871948242, -0.2766405940055847, -0.0857691615819931, 0.18269775807857513, 0.09651078283786774, -0.003974719904363155, 0.021461231634020805, 0.016391096636652946, -0.13884401321411133, 0.0674038827419281, -0.0733918771147728, -0.3635440170764923, -0.17622458934783936, 0.044863201677799225, -0.4808504283428192, -0.18656672537326813, 0.0392594151198864, 0.09492436796426773, -0.03657732903957367, -0.030146392062306404, 0.020836850628256798, 0.10386274755001068, -0.26639223098754883, 0.0006608760450035334, -0.1825788915157318, -0.07042404264211655, 0.14095081388950348, 0.11657904833555222, 0.06715786457061768, -0.14398662745952606, -0.1610925942659378, -0.2031812071800232, 0.0057463510893285275, 0.22677989304065704, 0.12399651855230331, 0.25704309344291687, 0.024556368589401245, 0.24662043154239655, -0.011781610548496246, -0.08637744933366776, 0.009443528018891811, 0.002661903155967593, 0.06829457730054855, -0.029695460572838783, -0.059293217957019806, 0.09996695816516876, 0.09259752184152603, 0.3900090754032135, 0.07260717451572418, -0.009294486604630947, 0.1382351517677307, 0.10335712134838104, 0.007032105699181557, 0.06351922452449799, 0.14167506992816925, -0.13245180249214172, 0.026239769533276558, -0.08494836091995239, -0.19025234878063202, 0.11127694696187973, 0.12015252560377121, 0.05719583481550217, 0.3138345181941986, 0.04841304570436478, -0.19454997777938843, 0.24225109815597534, 0.17665624618530273, 0.1599874049425125, -0.012664904817938805, 0.14821329712867737, 0.04506542906165123, -0.0074622961692512035, 0.028145931661128998, 0.10224524140357971, 0.05017324909567833, -0.09294888377189636, -0.062127579003572464, 0.0575442835688591, -0.06672504544258118, 0.10894406586885452, 0.10676030814647675, -0.07478874176740646, 0.014452586881816387, 0.027396727353334427, 0.08710131794214249, 0.1163896843791008, 0.12379001826047897, 0.16492684185504913, 0.2873377501964569, 0.03145197406411171, 0.03840486332774162, 0.17868511378765106, 0.05359136313199997, -0.14243154227733612, -0.1677737981081009, -0.10743710398674011, 0.26707029342651367, -0.1304503232240677, -0.20761781930923462, 0.07714907079935074, 0.24646031856536865, 0.028531838208436966, 0.17278112471103668, 0.20161403715610504, -0.05632026493549347, -0.054408613592386246, 0.1202453151345253, 0.1915648877620697, -0.0410921536386013, -0.027976669371128082, 0.011952598579227924, 0.0018638587789610028, 0.025869935750961304, -0.06608910113573074, 0.10535632818937302, 0.029182204976677895, -0.13154838979244232, -0.018751516938209534, 0.02880273573100567, -0.0684252381324768, 0.14507298171520233, 0.018175659701228142, -0.1475474238395691, 0.02400975674390793, -0.06715386360883713, 0.0710759162902832, 0.06290972977876663, 0.2573402225971222, 0.10338154435157776, -0.10027272999286652, 0.01053396612405777, -0.33191078901290894, -0.024598505347967148, 0.02221544459462166, -0.008520740084350109, -0.03162442520260811, 0.25719085335731506, 0.03977358713746071, -0.21618767082691193, -0.23960070312023163, -0.055651918053627014, 0.11837589740753174, 0.09296906739473343, 0.12424712628126144, 0.10565979033708572, 0.15231159329414368, 0.14027829468250275, 0.0915934145450592, -0.08117983490228653, 0.06794814765453339, -0.036868616938591, 0.3986598551273346, 0.03820008784532547, 0.1062258630990982, -0.32894954085350037, -0.08431297540664673, -0.16709686815738678, 0.03301803395152092, 0.03136090189218521, 0.1444137692451477, 0.002310308627784252, -0.027345702052116394, 0.3800331652164459, -0.09793617576360703, -0.17044618725776672, -0.04158978536725044, 0.2560681700706482, -0.06563390791416168, -0.15550749003887177, -0.05750219151377678, -0.017311429604887962, 0.055977724492549896, 0.4038034677505493, 0.2033706158399582, -0.01042800210416317, 0.011346730403602123, -0.056562889367341995, -0.17311903834342957, -0.018959123641252518, -0.03149750828742981, -0.012882203795015812, -0.08261670172214508, -0.05735781788825989, -0.07424316555261612, 0.04306727275252342, 0.15493835508823395, -0.1732579618692398, 0.05391611158847809, -0.08199994266033173, 0.14615027606487274, -0.06252942979335785, 0.09895151108503342, 0.22494450211524963, 0.14839351177215576, 0.05566534399986267, 0.009864415973424911, 0.03338053822517395, -0.05657365173101425, -0.09389198571443558, 0.04266936704516411, -0.2298201471567154, -0.03645404055714607, 0.036607977002859116, -0.20060200989246368, 0.3053523600101471, 0.08174307644367218, 0.04329654201865196, 0.010747469961643219, 0.0218399278819561, -0.023193074390292168, 0.08670981228351593, 0.29624873399734497, -0.06942199170589447, -0.024489691480994225, -0.038502540439367294, -0.05794568359851837, 0.041053131222724915, 0.20589974522590637, 0.002927440917119384, -0.0395476296544075, 0.13421717286109924, 0.220699742436409, -0.06320904195308685, 0.07350872457027435, -0.19626960158348083, -0.15615983307361603, 0.010788863524794579, 0.1184191182255745, -0.18484696745872498, -0.07095814496278763, -0.01873772032558918, -0.0725342407822609, 0.17326635122299194, 0.0007569001172669232, 0.09472591429948807, -0.10770060122013092, 0.06748316437005997, -0.2638483941555023, 0.027781039476394653, -0.10024000704288483, 0.06038937717676163, 0.2793924808502197, 0.027208328247070312, -0.07926294952630997, 0.2860552668571472, -0.005300233606249094, -0.07891377806663513, -0.1921490579843521, -0.07496505975723267, 0.024488484486937523, 0.06759489327669144, -0.10080447047948837, 0.03590831905603409, -0.018435616046190262, 0.003583270590752363, 0.10832978785037994, -0.02014041692018509, -0.14657464623451233, -0.2258608192205429, 0.05852491036057472, -0.08569033443927765, -0.07021757960319519, -0.06832059472799301, 0.010157452896237373, -0.01837623305618763, -0.05679580569267273, -0.04601370543241501, -0.07657649368047714, -0.06854433566331863, -0.13553448021411896, 0.09045679122209549, -0.16037727892398834, 0.019591238349676132, -0.0006034698453731835, -0.030098572373390198, 0.10623544454574585, -0.02276616357266903, 0.08544562011957169, -0.053992729634046555, -0.04383772239089012, 0.08348190784454346, -0.007876087911427021, -0.01037659216672182, 0.02746693789958954, -0.1227375790476799, -0.02901899442076683, 0.04951317980885506, -0.11077557504177094, 0.02088049240410328, -0.2957136034965515, -0.17173168063163757, 0.17606066167354584, 0.15844038128852844, -0.21644282341003418, -0.08312614262104034, 0.11238415539264679, 0.04441395774483681, 0.012705658562481403, 0.10079511255025864, 0.19091007113456726, -0.00792952161282301, -0.2949411869049072, -0.08447211235761642, 0.09582869708538055, -0.04594608396291733, -0.011167087592184544, 0.011845587752759457, 0.0542726032435894, 0.059843048453330994, -0.00439116358757019, 0.04950135201215744, -0.15709277987480164, -0.033407874405384064, 0.03890778124332428, 0.012563560158014297, -0.09675797820091248, 0.07242055982351303, -0.0312780998647213, 0.19440238177776337, 0.115973562002182, -0.22110503911972046, -0.028913797810673714, -0.05271342024207115, -0.1552661955356598, -0.016355231404304504, 0.12234383076429367, 0.06520389020442963, 0.11871008574962616, 0.09319713711738586, -0.13406887650489807, 0.14964242279529572, 0.028131645172834396, 0.04459432512521744, -0.19654163718223572, 0.041782159358263016, 0.0901779904961586, -0.06539151072502136, 0.0029338919557631016, -0.023595917969942093, -0.16561423242092133, -0.12165950983762741, -0.04362707957625389, 0.11976153403520584, 0.10160868614912033, 0.24947404861450195, 0.03344212844967842, 0.02490478754043579, -0.0654132291674614, 0.11583605408668518, -0.12759710848331451, -0.09284795820713043, -0.4027351140975952, -0.09428612142801285, -0.007493111304938793, -0.06037864461541176, -0.12633481621742249, 0.1662674993276596, -0.04110818728804588, 0.2912745177745819, 0.10446077585220337, -0.10727015882730484, 0.10333356261253357, -0.11762839555740356, 0.055807121098041534, 0.03778422251343727, -0.13832709193229675, -0.06299034506082535, 0.08051198720932007, -0.0027829534374177456, -0.3035963475704193, 0.18569843471050262, -0.14438316226005554, -0.04638483747839928, 0.18472638726234436, -0.048654183745384216, 0.3028089106082916, -0.030248142778873444, -0.21149471402168274, 0.10961347073316574, 0.14689069986343384, 0.09107358008623123, -0.10415174067020416, 0.2619863450527191, -0.1525835394859314, 0.01900205947458744, 0.031162600964307785, 0.12464629858732224, 0.09393453598022461, -0.1360163539648056, -0.05854248255491257, 0.1489054411649704, -0.030808737501502037, 0.08343903720378876, 0.0993552953004837, -0.06079113855957985, 0.13700002431869507, -0.003945442382246256, 0.19871050119400024, -0.2992343306541443, -0.15845245122909546, -0.10850901901721954, -0.1788095384836197, 0.15588228404521942, 0.02933710440993309, 0.2698909640312195, -0.19675500690937042, 0.11238270998001099, -0.0798710510134697, 0.15378166735172272, -0.03552943840622902, 0.1086684837937355, -0.03661223128437996, -0.042292702943086624, 0.0597405731678009, 0.001730560790747404, 0.130972221493721, 0.04382869973778725, -0.10298430919647217, -0.010411933995783329, -0.11987205594778061, 0.08264456689357758, 0.09992875903844833, -0.04024824872612953, 0.04365706443786621, 0.20859354734420776, -0.045636922121047974, 0.25720667839050293, -0.0859886109828949, -0.09275046736001968, 0.012567190453410149, -0.1464511603116989, 0.004956109914928675, 0.05084298923611641, -0.02131452038884163, 0.1500597596168518, -0.1487591564655304, 0.35859522223472595, -0.006291909143328667, -0.061358071863651276, 0.06162337586283684, 0.07701121270656586, -0.03642962872982025, -0.026787232607603073, -0.0224098302423954, 0.008102304302155972, 0.2306133210659027, 0.18649879097938538, 0.12348095327615738, 0.1589861661195755, 0.07497472316026688, -0.019818399101495743, 0.050400421023368835, -0.016484491527080536, -0.19021488726139069, -0.12876027822494507, 0.12897688150405884, -0.03725852072238922, 0.0742892399430275, 0.024385258555412292, -0.087484210729599, -0.02963211014866829, 0.06280171871185303, 0.027855275198817253, 0.0802859291434288, -0.15618062019348145, -0.13190153241157532, -0.08004572987556458, -0.05956553295254707, -0.27895644307136536, -0.05538380891084671, -0.013083625584840775, -0.19466182589530945, -0.04148707538843155, -0.317547082901001, -0.024313483387231827, -0.29331353306770325, 0.04559917002916336, 0.10274670273065567, 0.15467201173305511, -0.05715550482273102, -0.0799570083618164, -0.10928477346897125, 0.13987299799919128, 0.17293919622898102, -0.11566704511642456, 0.146658793091774, 0.11248432099819183, 0.07035005837678909, 0.10042589157819748, -0.06880994886159897, -0.17018063366413116, 0.013023626059293747, -0.09537914395332336, -0.15062850713729858, -0.16103877127170563, 0.15814189612865448, 0.0885772556066513, -0.1137806624174118, 0.05559540539979935, -0.11504525691270828, 0.12867169082164764, -0.0914510115981102, -0.007082993630319834, 0.16239619255065918, 0.004750228952616453, -0.1632142961025238, 0.3210390508174896, -0.10185100138187408, 0.1725194752216339, -0.1633112132549286, 0.2436545342206955, -0.11349707841873169, 0.07353525608778, -0.024128809571266174, 0.11081840842962265, -0.0769837349653244, 0.0018443323206156492, -0.037754129618406296, -0.13160808384418488, 0.03504296392202377, -0.03507918864488602, -0.06732384115457535, 0.13369831442832947, 0.12363249808549881, -0.17619210481643677, 0.09469199180603027, 0.2229982316493988, -0.12131159007549286, -0.1842244267463684, -0.1397598832845688, 0.14733415842056274, 0.018601184710860252, 0.0854373425245285, -0.16479374468326569, 0.2151266187429428, 0.011065371334552765, -0.07203229516744614, -0.0703427791595459, -0.0006049397052265704, 0.06341909617185593, 0.37231335043907166, -0.2413708120584488, -0.10845418274402618, -0.05358051881194115, 0.03591011092066765, 0.3926679491996765, -0.059699855744838715, -0.38140320777893066, 0.07285034656524658, -0.3897364139556885, -0.28024786710739136, 0.12900537252426147, 0.029128847643733025, -0.18897242844104767, -0.26382583379745483, -0.18894514441490173, -0.13954000174999237, 0.15594278275966644, 0.0013906733365729451, -0.11830110102891922, 0.0503234900534153, -0.068175308406353, -0.14479392766952515, -0.046222761273384094, 0.05391135811805725, 0.030655017122626305, -0.07172791659832001, 0.07211774587631226, 0.02286064438521862, 0.3891931474208832, -0.017796486616134644, -0.25796347856521606, -0.06430866569280624, 0.11380121111869812, -0.0549253448843956, -0.1871461719274521, -0.08846256136894226, -0.09028729796409607, -0.16317401826381683, -0.12291523069143295, 0.168152317404747, 0.16260387003421783, -0.01437931228429079, -0.15474896132946014, 0.41423964500427246, -0.10388967394828796, -0.18542075157165527, 0.13235178589820862, 0.018475593999028206, 0.018813008442521095, -0.19660380482673645, -0.25893205404281616, -0.035489145666360855, -0.14444047212600708, -0.19328507781028748, -0.10900552570819855, -0.1424991488456726, 0.05535329878330231, -0.015064011327922344, -0.0857599601149559, 0.1109590008854866, -0.26996123790740967, 0.04754098877310753, 0.04832290858030319, 0.10936686396598816, -0.4122096002101898, 0.0795874074101448, -0.005316347349435091, -0.013598590157926083, -0.009149279445409775, -0.08244530111551285, -0.17393775284290314, -0.1899159550666809, -0.21767406165599823, -0.223441481590271, 0.09507733583450317, -0.2568499743938446, 0.16188201308250427, -0.01399137917906046, -0.04036696255207062, 0.025031182914972305, -0.20346783101558685, 0.17673009634017944, -0.20681826770305634, -0.09998603165149689, -0.10142356902360916, -0.04169078916311264, 0.09682377427816391, -0.09442412853240967, -0.2035973221063614, -0.13219395279884338, 0.07952344417572021, 0.11090301722288132, -0.21239641308784485, -0.12354183942079544, -0.11871910095214844, -0.06682507693767548, -0.02402850240468979, -0.09997519850730896, 0.027354074642062187, -0.026299243792891502, 0.05722174793481827, 0.02218642272055149, -0.2436911165714264, 0.05316885933279991, -0.07057365030050278, 0.03458863124251366, -0.04727380350232124, 0.025074908509850502, -0.18144752085208893, -0.06378878653049469, -0.13117893040180206, -0.116852305829525, 0.1410144865512848, -0.13796310126781464, -0.034392643719911575, -0.04703247919678688, -0.18931488692760468, 0.19075435400009155, -0.03471435606479645, -0.08537357300519943, -0.17594490945339203, 0.12103047966957092, 0.0018039525020867586, 0.13446742296218872, 0.053422052413225174, 0.018325746059417725, -0.25121554732322693, 0.0688343197107315, 0.015315080992877483, 0.023443080484867096, -0.03821730613708496, 0.626326858997345, -0.05055632442235947, -0.16267117857933044, -0.1057407408952713, -0.20148524641990662, -0.0967845469713211, 0.0026883184909820557, -0.0016631227917969227, 0.31232327222824097, -0.14741939306259155, -0.18185077607631683, -0.015441473573446274, -0.06949978321790695, -0.034505296498537064, -0.18798120319843292, -0.08732783049345016, 0.10790736973285675, 0.09490562975406647, -0.4719839096069336, 0.08644271641969681, 0.16410866379737854, 0.07552690804004669, -0.23175403475761414, -0.009413362480700016, 0.09116695076227188, -0.06300390511751175, -0.00021877273684367537, -0.11815063655376434, -0.031001238152384758, 0.2633879780769348, 0.1125752329826355, -0.286977082490921, 0.29184624552726746, 0.01966988854110241, 0.0588613785803318, -0.00025653588818386197, 0.06063909828662872, -0.19617396593093872, -0.1919010579586029, 0.04993297532200813, -0.0791010931134224, 0.11231178790330887, 0.30023202300071716, 0.03126857802271843, 0.02399849146604538, -0.1104387491941452, -0.09087024629116058, 0.0385444350540638, -0.0780101865530014, 0.21367783844470978, 0.02469099499285221, -0.16241499781608582, 0.04265683889389038, -0.3040400743484497, -0.38022974133491516, -0.19660843908786774, -0.15747801959514618, 0.0678122267127037, -0.3483720123767853, -0.05895838513970375, -0.22648964822292328, -0.11431065946817398, 0.003558318829163909, -0.02938978001475334, -0.4061383306980133, 0.078719861805439, 0.04985873028635979, 0.31115126609802246, -0.03658346086740494, 0.05510200932621956, -0.10079140961170197, 0.07835302501916885, -0.018002009019255638, 0.19702079892158508, -0.07796913385391235, -0.15787826478481293, 0.6566937565803528, -0.024391312152147293, -0.010067305527627468, 0.118409663438797, -0.07598629593849182, -0.035004157572984695, 0.05349723622202873, -0.3013764023780823, -0.06486992537975311, -0.004077250603586435, 0.008123883977532387, 0.1239633783698082, -0.08036864548921585, 0.5471209287643433, 0.07850825786590576, -0.14387281239032745, -0.06790746003389359, 0.09006843715906143, 0.008383183740079403, -0.04696089029312134, 0.03901943564414978, -0.19497273862361908, 0.19568388164043427, -0.024682318791747093, -0.12985031306743622, 0.3058511018753052, -0.07545313984155655, 0.04107589274644852, -0.03605111315846443, 0.03157166764140129, 0.10053516924381256, -0.07295415550470352, 0.1372130811214447, -0.0035899775102734566, -0.032014403492212296, 0.13472558557987213, -0.2322091907262802, 0.021229607984423637, -0.11493046581745148, 0.012567486613988876, -0.015430513769388199, -0.27064844965934753, 0.03315696492791176, -0.019700951874256134, -0.2341027855873108, -0.0889604389667511, -0.06682088971138, -0.1556052714586258, -0.019063588231801987, -0.15060874819755554, 0.08043969422578812, 0.011075863614678383, 0.0021108840592205524, 0.06502953171730042, -0.07971876859664917, -0.01007804274559021, -0.07714012265205383, -0.1230657696723938, 0.033941175788640976, 0.0006858846754767001, -0.20015551149845123, 0.13846151530742645, -0.10460228472948074, -0.009536977857351303, -0.11415467411279678, -0.1645863801240921, -0.02385658584535122, -0.09442765265703201, -0.050233393907547, 0.14342334866523743, -0.05348293110728264, 0.0010961334919556975, -0.04557351768016815, -0.12269255518913269, 0.08469762653112411, 0.18372458219528198, 0.1443987488746643, -0.09214618802070618, -0.10733013600111008, -0.002466800855472684, 0.07233507931232452, -0.08225417882204056, 0.0030890509951859713, -0.07628516107797623, -0.11954724043607712, -0.06098621338605881, 0.04312384873628616, 0.16301922500133514, -0.03807971626520157, -0.16330011188983917, -0.1316068023443222, 0.08736129105091095, -0.08119237422943115, -0.05318745598196983, 0.029710115864872932, -0.16235671937465668, -0.08489011228084564, 0.14561469852924347, -0.05697769671678543, -0.03435169905424118, 0.2503220736980438, 0.036643870174884796, 0.10968999564647675, -0.08740994334220886, 0.012582474388182163, -0.222515270113945, -0.11265718191862106, -0.13309349119663239, -0.04473165050148964, -0.025006256997585297, 0.07415993511676788, -0.0745619684457779, -0.08685604482889175, -0.1390785127878189, 0.044277090579271317, 0.18468809127807617, -0.17069339752197266, -0.14939099550247192, -0.11178985238075256, 0.1389184147119522, 0.12776951491832733, -0.2583659291267395, -0.1459580510854721, -0.0502108670771122, 0.1216878890991211, -0.06272854655981064, -0.23739047348499298, 0.06930498778820038, -0.09195458889007568, -0.2178962230682373, -0.06689190864562988, -0.03646770492196083, -0.08005303144454956, 0.16751424968242645, -0.04160322621464729, -0.16032081842422485, 0.09142378717660904, 0.23031841218471527, -0.26789337396621704, -0.13933603465557098, -0.1468096375465393, -0.1112462505698204, -0.1322963535785675, 0.06217044219374657, 0.015225701965391636, -0.14899523556232452, -0.03279117867350578, 0.04049205780029297, 0.12165626883506775, -0.2970851957798004, -0.2315933108329773, -0.18112361431121826, -0.032815176993608475, -0.0061478097923099995, -0.4726793169975281, -0.03706387057900429, -0.5618489384651184, -0.015055817551910877, -0.17363837361335754, -0.027855033054947853, -0.036742035299539566, 0.032917264848947525, -0.17315107583999634, 0.01126454584300518, 0.00016224823775701225, -0.01299394853413105, 0.03240114077925682, -0.0673065260052681, -0.3527046740055084, -0.012838592752814293, -0.058563318103551865, 0.07405117899179459, -0.1545065939426422, -0.016268262639641762, -0.09841887652873993, -0.12301181256771088, -0.06880952417850494, 0.27683618664741516, -0.24842020869255066, 0.05856291204690933, -0.20243173837661743, 0.008890188299119473, -0.1641264110803604, 0.11317069083452225, -0.02456219121813774, 0.30925875902175903, 0.09432893246412277, 0.06245646998286247, 0.09523817151784897, 0.026603419333696365, 0.15397712588310242, 0.2220899760723114, -0.1327156126499176, -0.003597450675442815, 0.4670502841472626, 0.05005539953708649, -0.15195955336093903, -0.21254009008407593, 0.15685951709747314, 0.1685059666633606, -0.28498658537864685, 0.0015274886973202229, -0.1446714997291565, 0.06984254717826843, -0.09753882884979248, 0.00310354121029377, 0.06772554665803909, -0.008738736622035503, -0.0814976766705513, -0.1932639330625534, 0.15129360556602478, -0.270649254322052, 0.27149975299835205, -0.08845777064561844, 0.15727850794792175, -0.02318732813000679, -0.09756874293088913, -0.1600673645734787, 0.03837062790989876, -0.2042221575975418, -0.034293435513973236, -0.25799721479415894, -0.16953210532665253, -0.013505871407687664, 0.14230595529079437, 0.06538785994052887, 0.08350805938243866, -0.020004024729132652, 0.019524307921528816, 0.38002049922943115, -0.014833559282124043, -0.030869266018271446, 0.18966224789619446, -0.0619061142206192, 0.12670302391052246, 0.05978502705693245, -0.071408212184906, -0.07454249262809753, 0.04066246747970581, 0.0015293422620743513, 0.0008461838006041944, 0.11704833805561066, -0.10284523665904999, -0.07379460334777832, 0.2765255868434906, -0.0794035866856575, -0.08076794445514679, -0.008805472403764725, -0.031358279287815094, -0.010988111607730389, 0.058776888996362686, -0.018141765147447586, -0.11567927151918411, -0.015566648915410042, 0.2115364521741867, -0.1492592841386795, 0.06012733280658722, -0.13498206436634064, 0.1784321665763855, 0.2551582157611847, 0.016111284494400024, -0.19133660197257996, 0.021271495148539543, -0.053086064755916595, 0.018627740442752838, 0.020600613206624985, 0.0734587088227272, -0.20425520837306976, -0.10232322663068771, 0.25013530254364014, 0.01306456420570612, 0.21559099853038788, -0.13328085839748383, -0.007864478975534439, -0.09670141339302063, -0.16279254853725433, 0.028608474880456924, 0.11352719366550446, -0.08902401477098465, 0.19541510939598083, -0.30917438864707947, -0.05685478076338768, 0.06542552262544632, -0.1946287900209427, 0.006932336837053299, -0.10531129688024521, -0.013904334045946598, 0.045489247888326645, 0.22744572162628174, -0.1708720326423645, -0.20962050557136536, 0.22020390629768372, 0.18209819495677948, 0.19731520116329193, -0.002472090069204569, -0.12290289252996445, 0.046886950731277466, -0.06028313934803009, -0.085086390376091, -0.06840558350086212, -0.03178584948182106, -0.013799603097140789, -0.052492182701826096, -0.08217012137174606, 0.020964954048395157, -0.0812215730547905, -0.0013853020500391722, -0.2723955512046814, -0.00044706749031320214, -0.03308999538421631, 0.05432809144258499, -0.36567482352256775, -0.022595884278416634, -0.08852320909500122, -0.2265794724225998, 0.11423328518867493, 0.007608568761497736, -0.11420203000307083, -0.1168878972530365, 0.17160984873771667, -0.18935368955135345, -0.05649328976869583, -0.08898031711578369, -0.27616217732429504, -0.024105878546833992, -0.2930081784725189, -0.12473218888044357, -0.09837378561496735, 0.02351689711213112, -0.16254886984825134, 0.023519722744822502, -0.17548397183418274, 0.10777729749679565, -0.07814212143421173, -0.044456738978624344, -0.06742296367883682, -0.2205735296010971, 0.09368912130594254, 0.05377533286809921, -0.3421296775341034, -0.13402284681797028, 0.2708221971988678, -0.013647276908159256, -0.15445619821548462, -0.03529611974954605, 0.024464070796966553, -0.11715514212846756, -0.046755485236644745, 0.05392973870038986, -0.03910676762461662, -0.1458158791065216, -0.2185312956571579, -0.1917141228914261, -0.11363451927900314, 0.012435433454811573, -0.04454940930008888, -0.04697069898247719, -0.07120966911315918, -0.1533622145652771, 0.18875031173229218, -0.033651676028966904, 0.11980327218770981, -0.12038478255271912, 0.09607982635498047, -0.08952377736568451, -0.06099042668938637, -0.027886010706424713, -0.06295885145664215, -0.3414551615715027, -0.29470372200012207, -0.08263284713029861, -0.05352310463786125, 0.12920372188091278, -0.10229494422674179, 0.06519230455160141, -0.3744845986366272, -0.1000746339559555, 0.0332489050924778, -0.07788698375225067, -0.060129426419734955, 0.00722979661077261, -0.14180533587932587, -0.15257243812084198, -0.1697137951850891, -0.015079512260854244, -0.19315464794635773, 0.13775137066841125, 0.0645614042878151, -0.17349950969219208, -0.14896225929260254, -0.11068329215049744, -0.12134335190057755, -0.14580121636390686, 0.11991812288761139, 0.2028714120388031, -0.047735195606946945, -0.017611464485526085, -0.1244441419839859, -0.047276660799980164, -0.005828457418829203, -0.04520198702812195, -0.1372436136007309, -0.05425918847322464, -0.048725392669439316, -0.09793394804000854, 0.16424062848091125, -0.10263241082429886, -0.03548692911863327, -0.09599962830543518, -0.03378536179661751, -0.11843050271272659, -0.21002961695194244, 0.19161511957645416, 0.0029572076164186, -0.061856478452682495, 0.22666308283805847, -0.1445775330066681, -0.00365196424536407, 0.007402120158076286, -0.02847246639430523, -0.05578494444489479, -0.3307671546936035, -0.20944495499134064, -0.01953248865902424, 0.12726694345474243, -0.10116802901029587, -0.09284799546003342, 0.10117717832326889, -0.036647334694862366, -0.11880557984113693, -0.17508912086486816, -0.06744592636823654, -0.06923387944698334, -0.45397424697875977, -0.3536432087421417, -0.13896746933460236, -0.00373473996296525, 0.022256366908550262, 0.005422974471002817, -0.17408142983913422, 0.08875980973243713, -0.18597017228603363, 0.1151295080780983, -0.26063355803489685, 0.04498491436243057, -0.04687036573886871, -0.1149059310555458, -0.02433125674724579, -0.145951509475708, -0.22043348848819733, -0.07081620395183563, -0.1821407675743103, -0.16745182871818542, -0.13945195078849792, -0.044526826590299606, -0.07008906453847885, 0.00729040801525116, -0.32942166924476624, -0.25656235218048096, -0.333235502243042, -0.0841086208820343, 0.09529999643564224, 0.2677503824234009, -0.060067854821681976, -0.01924905925989151, 0.020909007638692856, 0.014260377734899521, -0.08413767069578171, 0.00441782595589757, 0.054421186447143555, 0.006560828536748886, -0.0679551288485527, -0.010651384480297565, 0.02947847545146942, -0.07543111592531204, 0.14351965487003326, -0.12942257523536682, -0.08937151730060577, -0.05266057699918747, -0.08814024180173874, -0.20049339532852173, -0.2431122362613678, -0.039307717233896255, 0.005270089954137802, -0.2227521538734436, -0.039225995540618896, -0.11686357110738754, -0.05031506344676018, -0.0773668959736824, 0.30152180790901184, 0.026603247970342636, 0.39028817415237427, 0.03216952085494995, 0.042911116033792496, -0.04820486158132553, -0.1478922814130783, -0.20209555327892303, 0.07910846918821335, -0.17354340851306915, -0.1827910989522934, 0.239432230591774, 0.17889708280563354, 0.07528259605169296, 0.017671626061201096, -0.12220041453838348, 0.08939182758331299, 0.10552771389484406, -0.07192196696996689, -0.040519118309020996, -0.1612602025270462, -0.15198718011379242, -0.007628273218870163, -0.0702679306268692, -0.02958216890692711, 0.1019008532166481, 0.0018182516796514392, -0.17412857711315155, -0.06749939173460007, -0.09835159778594971, -0.3076959252357483, -0.18478229641914368, -0.0026448224671185017, 0.08228049427270889, 0.06491117924451828, 0.11714936047792435, -0.10302315652370453, 0.21904486417770386, -0.1580950766801834, -0.10321826487779617, -0.20593449473381042, -0.02743038535118103, -0.18424251675605774, 0.3181394934654236, 0.06666474044322968, -0.1436699777841568, -0.07140178978443146, -0.060300275683403015, 0.03318104147911072, 0.2405121624469757, -0.09359101951122284, 0.01800485886633396, -0.04727242887020111, -0.043071966618299484, -0.22844751179218292, -0.024458812549710274, -0.024439513683319092, -0.20863032341003418, 0.4237439036369324, -0.02638757973909378, 0.09578827768564224, -0.1468963474035263, -0.07495125383138657, -0.015421146526932716, -0.01172622386366129, -0.1996930092573166, -0.20851369202136993, -0.14993374049663544, -0.1496915966272354, -0.007626346778124571, 0.002624041400849819, -0.02275899238884449, -0.1477968543767929, -0.44477757811546326, 0.13606327772140503, -0.012884469702839851, -0.10015569627285004, 0.3032184839248657, -0.24288678169250488, -0.24585075676441193, 0.19376081228256226, -0.048281602561473846, -0.0525386705994606, 0.042220525443553925, -0.19002465903759003, -0.055817779153585434, 0.25834375619888306, 0.0596548356115818, 0.007004163693636656, -0.13363248109817505, -0.18423490226268768, 0.05278196930885315, -0.21892184019088745, -0.006535991095006466, -0.022273752838373184, -0.038413502275943756, 0.04479602351784706, -0.06507894396781921, 0.10642866790294647, 0.05161542072892189, -0.015094747766852379, -0.18653267621994019, -0.07752711325883865, -0.1412421464920044, -0.09578990936279297, -0.058876603841781616, -0.17144358158111572, 0.00529312202706933, 0.012025922536849976, -0.2252223938703537, 0.06666198372840881, 0.007070731837302446, 0.05551330745220184, -0.19903653860092163, 0.025678180158138275, 0.04653513804078102, 0.014591796323657036, -0.007843617349863052, 0.0713638886809349, -0.0545976348221302, -0.12822853028774261, -0.12442436814308167, -0.013248400762677193, 0.20931227505207062, 0.08829610049724579, -0.06792426109313965, 0.057547252625226974, -0.05065373331308365, -0.014365365728735924, 0.10508956015110016, 0.3001925051212311, 0.33604273200035095, -0.2729209065437317, -0.03815932944417, 0.09313257038593292, -0.07189127802848816, -0.1488344520330429, 0.25133758783340454, -0.13486553728580475, -0.12649144232273102, 0.02135738730430603, 0.057995717972517014, -0.07854645699262619, -0.21897566318511963, -0.12862128019332886, -0.2470579296350479, -0.4056893289089203, 0.15612903237342834, -0.30180636048316956, -0.14769703149795532, 0.13783694803714752, -0.07212755084037781, 0.16383422911167145, -0.03940773010253906, 0.07812751084566116, -0.07410596311092377, -0.12226670235395432, -0.02975550852715969, 0.14420735836029053, 0.2899121046066284, -0.05295906215906143, -0.003319069743156433, -0.06907632201910019, 0.38403600454330444, -0.008999709971249104, 0.09497818350791931, -0.06364470720291138, -0.24348671734333038, 0.058782197535037994, -0.026898739859461784, -0.2639601230621338, 0.03227780759334564, 0.2961096167564392, 0.10100674629211426, 0.31761634349823, -0.02477192133665085, 0.142222061753273, 0.01775110512971878, 0.05710833519697189, -0.0649910420179367, 0.011529012583196163, -0.12654107809066772, 0.18226507306098938, -0.032807257026433945, 0.09480879455804825, -0.17402267456054688, -0.03928843140602112, 0.053012166172266006, 0.01743065007030964, -0.23129741847515106, -0.20348581671714783, -0.14712148904800415, -0.03985312208533287, -0.1884416788816452, -0.22113639116287231, -0.2839749753475189, -0.14008529484272003, 0.12539726495742798, -0.0025114710442721844, 0.2352335900068283, 0.1329444944858551, -0.21035222709178925, -0.10417246073484421, 0.14000900089740753, 0.09379745274782181, 0.07355508953332901, 0.2364388406276703, 0.05801771581172943, -0.10157568007707596, -0.12857051193714142, -0.32404273748397827, 0.15407754480838776, -0.09890442341566086, -0.12481682747602463, -0.06049615889787674, 0.1343514621257782, 0.0057512689381837845, -0.22280405461788177, -0.04314315691590309, -0.08803224563598633, 0.12052931636571884, 0.17401057481765747, -0.11042339354753494, 0.30219486355781555, 0.21993720531463623, 0.1598159223794937, 0.02857140265405178, 0.30167651176452637, -0.28207826614379883, 0.011542454361915588, 0.29551586508750916, -0.26667678356170654, 0.21031785011291504, -0.2213868796825409, 0.22192884981632233, 0.24914705753326416, -0.13393914699554443, -0.03962738439440727, -0.10197968035936356, 0.0654020830988884, 0.18506422638893127, -0.03686347231268883, 0.18414252996444702, -0.2068927139043808, 0.04281613230705261, -0.19385328888893127, 0.09766623377799988, -0.2456311285495758, -0.2114107459783554, -0.004837083630263805, -0.4175203740596771, -0.045572150498628616, -0.13016299903392792, 0.011744353920221329, -0.04278041049838066, -0.16795775294303894, -0.16164131462574005, -0.16976206004619598, -0.07169199734926224, -0.1058080792427063, 0.03307390585541725, -0.040040068328380585, -0.25717461109161377, 0.08850883692502975, 0.05709483101963997, -0.08898700028657913, -0.07001825422048569, -0.09506653994321823, 0.16436323523521423, -0.061231181025505066, 0.0212269090116024, 0.012338022701442242, 0.1949639618396759, -0.16992948949337006, 0.05291956663131714, -0.09070432186126709, -0.040628451853990555, -0.21020200848579407, 0.1841057538986206, -0.2903088629245758, 0.030360952019691467, -0.23582585155963898, 0.22072860598564148, -0.03740932419896126, -0.12779918313026428, -0.05947350338101387, 0.07341840118169785, -0.1649439036846161, 0.08345403522253036, 0.11382704228162766, -0.13308539986610413, 0.21161986887454987, 0.015511767007410526, -0.10360799729824066, -0.24775627255439758, -0.014047997072339058, 0.01934817060828209, -0.038748178631067276, 0.013512197881937027, 0.018028656020760536, 0.11530158668756485, -0.11483904719352722, 0.11073525995016098, -0.26501163840293884, 0.01432415097951889, 0.02371147833764553, 0.13519620895385742, 0.0595553033053875, 0.2397409975528717, -0.3017938435077667, -0.050054799765348434, -0.056368932127952576, -0.4418395459651947, 0.1278013288974762, 0.04703382030129433, -0.020242927595973015, 0.006831537000834942, -0.2070077508687973, 0.013558309525251389, -0.009380492381751537, 0.020273935049772263, -0.17364464700222015, -0.043554361909627914, -0.2111975997686386, -0.11542002111673355, -0.08306188136339188, -0.09477950632572174, 0.22736425697803497, -0.06614000350236893, -0.39519381523132324, 0.09528180211782455, -0.06472625583410263, 0.02274896390736103, -0.16066893935203552, -0.2562270164489746, -0.18158631026744843, 0.06789407879114151, -0.06908993422985077, -0.3266172409057617, 0.23629719018936157, -0.1646399348974228, -0.10482876747846603, -0.1095777228474617, 0.09991604089736938, 0.3945229649543762, -0.007402282208204269, -0.11500738561153412, 0.07820270955562592, -0.07975772023200989, -0.1469145566225052, -0.11527375876903534, -0.18661387264728546, 0.12528909742832184, -0.527260422706604, 0.1504303216934204, 0.15216749906539917, -0.19876548647880554, -0.004455067217350006, 0.005513733718544245, -0.23777277767658234, -0.20300956070423126, -0.05142870917916298, -0.11069986969232559, -0.1376345157623291, -0.06740070879459381, 0.051500171422958374, 0.06677153706550598, -0.04794599860906601, -0.24394264817237854, -0.13965503871440887, -0.019353462383151054, 0.0003662265371531248, 0.06299977004528046, -0.2279554158449173, 0.13380371034145355, 0.07913576811552048, 0.1083604022860527, -0.13746945559978485, -0.1318640261888504, 0.2462359070777893, 0.04728732630610466, 0.01571754738688469, 0.08386418968439102, 0.221493661403656, 0.05874898284673691, -0.11720521003007889, -0.17607657611370087, -0.011287841945886612, -0.28273728489875793, -0.05455930903553963, 0.16142217814922333, -0.0735459104180336, -0.08379526436328888, -0.19449879229068756, 0.15552817285060883, 0.05866706371307373, 0.03300342336297035, 0.1601673662662506, 0.21449081599712372, -0.29397326707839966, -0.052886176854372025, -0.31355541944503784, -0.09199231117963791, -0.07228723913431168, -0.10061939805746078, -0.33296337723731995, -0.11562687158584595, 0.032951489090919495, -0.13875257968902588, 0.09380494058132172, -0.05941270291805267, 0.18708038330078125, 0.18562351167201996, -0.08812554180622101, 0.1439468264579773, 0.09568560123443604, -0.24714021384716034, -0.178014874458313, -0.27083519101142883, 0.015801385045051575, 0.14463768899440765, -0.18215475976467133, -0.2025110274553299, -0.0927911028265953, 0.01472311932593584, -0.16000817716121674, 0.08237758278846741, -0.05467559024691582, -0.08281110972166061, 0.033085618168115616, -0.09568656235933304, -0.024263078346848488, 0.03194807469844818, -0.19690796732902527, 0.04779895395040512, -0.07869689911603928, 0.12289667129516602, 0.20481659471988678, -0.24173308908939362, -0.003969043958932161, -0.02112419158220291, -0.17699815332889557, 0.13204680383205414, 0.11394660174846649, 0.13580456376075745, -0.32198095321655273, -0.07351116836071014, -0.3537437319755554, 0.15681229531764984, -0.2605731189250946, 0.006790744140744209, -0.1701509803533554, -0.20493048429489136, -0.10978829860687256};

const float bias_raw[128]={0.46540170907974243, -0.07591938972473145, -0.2145615518093109, 0.511992335319519, -0.04664149880409241, 0.3976514935493469, -0.08855267614126205, -0.6269875168800354, 0.12956292927265167, 0.36778271198272705, 0.07237014919519424, 0.20200665295124054, 0.005576586816459894, -0.9584053754806519, 0.2207576185464859, 0.23753350973129272, 0.24447956681251526, -0.05461818352341652, 0.0581158883869648, 0.40481022000312805, 0.5947919487953186, 0.13713328540325165, -0.14336730539798737, 0.6784915328025818, 0.7147891521453857, 0.3424636125564575, -0.05299222469329834, 0.3325560688972473, 0.14583726227283478, -0.16820579767227173, 0.02427588775753975, -0.08141900599002838, 0.34080490469932556, -0.1266467422246933, -0.25363242626190186, -0.7155653834342957, 0.29472532868385315, -0.6037617921829224, 0.2560955286026001, 0.17979036271572113, 0.11951376497745514, 0.13062550127506256, 0.33346179127693176, 0.4681282341480255, -0.0743679329752922, 0.04912784323096275, 0.1785804033279419, 0.6287086606025696, 0.4002295434474945, -0.11884751170873642, 0.054818108677864075, 0.9211373925209045, 0.3874887526035309, 0.06936892122030258, -0.3422620892524719, -0.27836787700653076, -0.12079507112503052, -0.09951873868703842, -0.9680654406547546, 0.6639410853385925, -0.4941783547401428, -0.28106987476348877, 0.24726633727550507, 0.30488845705986023, -0.11248891055583954, 0.032181136310100555, -0.0726977214217186, 0.3937534987926483, -0.11595480889081955, 0.08639874309301376, -0.056635525077581406, -0.697791337966919, -0.162557914853096, -0.03911260515451431, -0.19867204129695892, 0.22770726680755615, 0.10787525773048401, -0.09752979874610901, -0.08072644472122192, 0.2671070396900177, -0.1470133662223816, -0.1471356600522995, -0.07719086110591888, -0.15017947554588318, -0.4402930736541748, -0.1532631367444992, -0.11789561808109283, 0.5247753262519836, 0.7207589149475098, -0.19602347910404205, 0.19692924618721008, 0.16424041986465454, 0.055737342685461044, -0.23933598399162292, -0.6234241127967834, -0.22704383730888367, 0.1318684220314026, 0.19185881316661835, -0.1914030760526657, -0.023874571546912193, 0.6073795557022095, -0.4391995668411255, -0.12778300046920776, -0.11385520547628403, -0.4117286503314972, 0.5630231499671936, 0.26207873225212097, -0.11081735044717789, 0.23084047436714172, -0.25555792450904846, -0.2734597623348236, -0.11957130581140518, 0.06773386895656586, -0.4367905557155609, 0.0033111751545220613, 0.11971836537122726, -0.41806504130363464, 0.44618234038352966, -0.08720896393060684, 0.33594998717308044, 0.15858696401119232, 0.14609642326831818, 0.536086916923523, 0.3192797601222992, -0.007763301022350788, 0.038248784840106964, 0.7926216125488281, 0.28160837292671204};

const int stride_width=1;
const int stride_height=1;
const TfLiteFusedActivation activation=kTfLiteActRelu;
const int dilation_width_factor=1;
const int dilation_height_factor=1;
const int32_t filter_input_channel=32;
const int32_t filter_output_channel=128;
const int filter_height=1;
const int filter_width=1;
const int filter_dims_size=4;
const int32_t filter_dims_raw[4]={128,1,1,32};
const int bias_dims_size=1;
const int32_t bias_dims_raw[1]={128};
const TfLitePadding paddings=kTfLitePaddingSame;
const TfLiteType filter_type=kTfLiteFloat32;
const bool data_supports_multithreaded_kernel=true;

struct OpData {
  // IDs are the arbitrary identifiers used by TF Lite to identify and access
  // memory buffers.
  int im2col_id = kTensorNotAllocated;
  int hwcn_weights_id = kTensorNotAllocated;
  int input_quantized_id = kTensorNotAllocated;
  int scaling_factors_id = kTensorNotAllocated;
  int input_offset_id = kTensorNotAllocated;
  int accum_scratch_id = kTensorNotAllocated;
  // Row sums are used to cache filter sums for hybrid zero-point calculations.
  int row_sums_id = kTensorNotAllocated;

  TfLitePaddingValues padding;
  // The scaling factor from input to output (aka the 'real multiplier') can
  // be represented as a fixed point multiplier plus a left shift.
  int32_t output_multiplier;
  int output_shift;

  // Per channel output multiplier and shift.
  std::vector<int32_t> per_channel_output_multiplier;
  std::vector<int> per_channel_output_shift;

  // The range of the fused activation layer. For example for kNone and
  // uint8_t these would be 0 and 255.
  int32_t output_activation_min;
  int32_t output_activation_max;
  // Indexes are the offset to the memory buffer in the array used to keep track
  // of the allocated temporaries.
  int32_t im2col_index;
  int32_t hwcn_weights_index;
  int32_t input_quantized_index;
  int32_t scaling_factors_index;
  int32_t accum_scratch_index;
  int32_t input_offset_index;
  int32_t row_sums_index;

  bool need_hwcn_weights = false;
  bool have_weights_been_transposed = false;
  bool need_im2col = false;
  // If it's true, it means im2col is needed but gets disabled because the
  // temporary im2col tensor requires too much memory (i.e.
  // >= kMaxIm2colBufferSize);
  bool im2col_oversized = false;

  bool supports_multithreaded_kernel = false;
  bool is_hybrid_per_channel = false;
  bool compute_hybrid_row_sums = true;

  // Number of convolution groups.
  int32_t groups = 1;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
    case TfLitePadding::kTfLitePaddingSame:
      return PaddingType::kSame;
    case TfLitePadding::kTfLitePaddingValid:
      return PaddingType::kValid;
    case TfLitePadding::kTfLitePaddingUnknown:
    default:
      return PaddingType::kNone;
  }
}

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  // This is a builtin op, so we don't use the contents in 'buffer', if any.
  // Instead, we allocate a new object to use as scratch space for im2col, and
  // to carry information from Prepare() to Eval().
  auto* data = new OpData;
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
  eigen_support::IncrementUsageCounter(context);
#endif
  return data;
}

void Free(TfLiteContext* context, void* buffer) {
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
  eigen_support::DecrementUsageCounter(context);
#endif
  delete reinterpret_cast<OpData*>(buffer);
}

// Naive implementation of transpose for floats. Could be optimized to be more
// cache friendly, but for now it's a one-time cost on first run, and we would
// prefer to remove the need to do this at all eventually.
void TransposeFloatTensor(const TfLiteTensor* input, TfLiteTensor* output) {
  const int rows = output->dims->data[1];
  const int cols = output->dims->data[0];
  // const float* input_data = GetTensorData<float>(input);
  const float* input_data = filter_raw;
  float* output_data = GetTensorData<float>(output);
  for (int i = 0; i < rows; ++i) {
    for (int j = 0; j < cols; ++j) {
      const float in_value = input_data[i * cols + j];
      output_data[j * rows + i] = in_value;
    }
  }
}

// Check if im2col needs to be allocated, as some version of optimized Conv dont
// use it. If any change is supporting im2col in any of the Conv versions, then
// it should be updated here as well
bool IsIm2ColRequired(const TfLiteTensor* input, TfLiteConvParams* params,
                      const TfLiteTensor* filter, OpData* data, bool is_hybrid,
                      KernelType kernel_type) {
  // If HWCN weights are required, Im2Col not required
  if (data->need_hwcn_weights) return false;

  // segregate based on dilated conv & non-dialated conv
  const bool need_dilated_im2col =
      params->dilation_width_factor != 1 || params->dilation_height_factor != 1;
  // const bool need_non_dilated_im2col =
  //     params->stride_width != 1 || params->stride_height != 1 ||
  //     filter->dims->data[2] != 1 || filter->dims->data[1] != 1;
  const bool need_non_dilated_im2col =
      stride_width != 1 || stride_height != 1 ||
      filter_width != 1 || filter_height != 1;

  const bool need_im2col = need_dilated_im2col || need_non_dilated_im2col;

  // Return early as basic requirement is not met
  if (!need_im2col) return false;

  // Special case for Hybrid, as it supports only non-dilated im2col currently
  const bool is_hybrid_non_dilated = is_hybrid && need_non_dilated_im2col;
  const bool is_quantized = input->type == kTfLiteUInt8 ||
                            input->type == kTfLiteInt8 ||
                            input->type == kTfLiteInt16;

  switch (kernel_type) {
    case kReference:
      if (is_hybrid) {
        return true;
      } else {
        return false;
      }
    case kGenericOptimized:
    case kCblasOptimized:
      if (is_hybrid && !need_non_dilated_im2col) {
        return false;
      } else {
        return true;
      }
    case kMultithreadOptimized:
      if (is_hybrid_non_dilated || is_quantized ||
          !data->supports_multithreaded_kernel) {
        return true;
      } else {
        return false;
      }
    default:
      return false;
  }
}

// Allocate temporary tensors (`im2col`, `hwcn_weights` if necessary).
// Note: `context->AddTensors` might invalidate pointers to existing tensors.
// Therefore the logic to add tensors are isolated into this function.
static TfLiteStatus AllocateTemporaryTensorsIfRequired(
    TfLiteContext* context, TfLiteNode* node, bool is_hybrid,
    bool is_per_channel, KernelType kernel_type, size_t im2col_bytes) {
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams* params;
  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  // TF_LITE_ENSURE(context, node->inputs->size >= 2);
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  const TfLiteTensor* filter;
  // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));

  // If we're using the optimized multithreaded EigenTensor implementation of
  // convolution, it expects the filter weights to be transposed compared to
  // the normal TF Lite buffer format. Typical TF Lite weights are
  // [filter_count, filter_height, filter_width, input_depth], but for the float
  // implementation we need them as [filter_height, filter_width, input_depth,
  // filter_count]. We get to that format by transposing, and create a temporary
  // buffer to store the results.
  // This path is only used for float processing, so only create the buffer if
  // we're running with that data type.
  data->need_hwcn_weights =
      input->type == kTfLiteFloat32 && data->supports_multithreaded_kernel;

  // We don't always need to allocate im2col. It is only used in some versions
  // of the optimized Conv. This test just mimics something that happens inside
  // optimized_ops.h, in order to avoid a DCHECK(!im2col_data).
  data->need_im2col =
      IsIm2ColRequired(input, params, filter, data, is_hybrid, kernel_type);

  // If im2col_oversized is found to be true, we have to fallback to an
  // execution path (like kReference in float/quantized cases) that doesn't
  // require im2col operation. Therefore, we have to skip checking the hybrid
  // case (but not the hybrid-per-channel one) where there's no such a fallback
  // execution path.
  // TODO(b/178743262): Consider making this check conditioned on the available
  // memory of the system, rather than coupling to the mobile platform check.
  if (IsMobilePlatform() && !(is_hybrid && !is_per_channel) &&
      data->need_im2col && im2col_bytes >= kMaxIm2colBufferSizeMobile) {
    data->need_im2col = false;
    data->im2col_oversized = true;
  }
  int temporaries_count = 0;
  if (data->need_im2col) {
    data->im2col_index = temporaries_count;
    if (data->im2col_id == kTensorNotAllocated) {
      context->AddTensors(context, 1, &data->im2col_id);
    }
    ++temporaries_count;
  }
  if (data->need_hwcn_weights) {
    data->hwcn_weights_index = temporaries_count;
    if (data->hwcn_weights_id == kTensorNotAllocated) {
      context->AddTensors(context, 1, &data->hwcn_weights_id);
    }
    ++temporaries_count;
  }

  if (is_hybrid) {
    // Allocate tensor to store the on-the-fly quantized inputs.
    data->input_quantized_index = temporaries_count;
    if (data->input_quantized_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->input_quantized_id));
    }
    ++temporaries_count;

    // Allocate tensor to store the quantization params computed during
    // on-the-fly input quantization.
    data->scaling_factors_index = temporaries_count;
    if (data->scaling_factors_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->scaling_factors_id));
    }
    ++temporaries_count;

    // Allocate tensor to store the accumulators for the matrix multiply.
    data->accum_scratch_index = temporaries_count;
    if (data->accum_scratch_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->accum_scratch_id));
    }
    ++temporaries_count;
    if (is_per_channel) {
      data->input_offset_index = temporaries_count;
      if (data->input_offset_id == kTensorNotAllocated) {
        TF_LITE_ENSURE_OK(
            context, context->AddTensors(context, 1, &data->input_offset_id));
      }
      ++temporaries_count;

      data->row_sums_index = temporaries_count;
      if (data->row_sums_id == kTensorNotAllocated) {
        TF_LITE_ENSURE_OK(context,
                          context->AddTensors(context, 1, &data->row_sums_id));
      }
      ++temporaries_count;
    }
  }

  TfLiteIntArrayFree(node->temporaries);
  node->temporaries = TfLiteIntArrayCreate(temporaries_count);

  return kTfLiteOk;
}

TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,
                     TfLiteNode* node) {
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  // TfLiteConvParams* params;
  // TfLitePadding paddings=kTfLitePaddingSame;
  // --------------------------------------------------------------------------
  // int stride_width=2;
  // int stride_height=2;
  // TfLiteFusedActivation activation=kTfLiteActRelu6;
  // int dilation_width_factor = 1;
  // int dilation_height_factor = 1;
  // const int32_t filter_input_channel = 3;
  // const int32_t filter_output_channel = 24;
  // int filter_height = 3;
  // int filter_width = 3;
  // TfLiteType filter_type = kTfLiteFloat32;
  // const bool data_supports_multithreaded_kernel=true;
  // --------------------------------------------------------------------------
  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  bool has_bias = false;
  // bool has_bias = 0;
  // Check number of inputs/outputs
  // TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  const TfLiteTensor* filter;
  // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));

  // Check dimensionality of input, filter
  TF_LITE_ENSURE_EQ(context, input->dims->size, 4);
  // TF_LITE_ENSURE_EQ(context, filter->dims->size, 4);
  TF_LITE_ENSURE_EQ(context, filter_dims_size, 4);
  
  // Check input channels matching filter
  // Filter input channel can be a factor of channels of input (grouped conv)
  // or equals (normal conv).
  auto input_channel = input->dims->data[3];
  // auto filter_input_channel = filter->dims->data[3];

  TF_LITE_ENSURE_EQ(context, input_channel % filter_input_channel, 0);
  data->groups = input_channel / filter_input_channel;

  // Check types. (We assume that UINT8 refers to quantized tensors)
  TfLiteType input_type = input->type;
  TF_LITE_ENSURE(context,
                 input_type == kTfLiteFloat32 || input_type == kTfLiteUInt8 ||
                     input_type == kTfLiteInt8 || input_type == kTfLiteInt16);
  TF_LITE_ENSURE_TYPES_EQ(context, output->type, input_type);

  if (input_type == kTfLiteInt16) {
    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);
    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);
  }
  // Filter must have zero zero-points in per-channel quantization.
  if (input_type == kTfLiteInt16 || input_type == kTfLiteInt8) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    for (int i = 0; i < affine_quantization->zero_point->size; ++i) {
      TF_LITE_ENSURE_EQ(context, affine_quantization->zero_point->data[i], 0);
    }
  }

  const TfLiteTensor* bias = nullptr;
  // std::cout << "codes runs here #-2" << std::endl;
  // TODO(ahentz): At this point the optimized versions require 'bias'. We can
  // either change that or document that convolution requires it.

  // TF_LITE_ENSURE(context, has_bias);

  if (has_bias) {
    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 2, &bias));
    if (input_type == kTfLiteUInt8 || input_type == kTfLiteInt8) {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else if (input_type == kTfLiteInt16) {
      TF_LITE_ENSURE(context, (bias->type == kTfLiteInt32) ||
                                  (bias->type == kTfLiteInt64));
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, input_type);
    }
    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 0));
  }

  const bool is_hybrid =
      (input->type == kTfLiteFloat32 &&
       (filter_type == kTfLiteUInt8 || filter_type == kTfLiteInt8));

  if (is_hybrid && filter_type == kTfLiteInt8 &&
      filter->quantization.type == kTfLiteAffineQuantization &&
      filter->quantization.params &&
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)
          ->scale &&
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)
              ->scale->size > 1) {
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    const float scale = affine_quantization->scale->data[0];
    for (int i = 1; i < affine_quantization->scale->size; i++) {
      if (affine_quantization->scale->data[i] != scale) {
        data->is_hybrid_per_channel = true;
        break;
      }
    }
  }
  //----------------------------------------------------------------------------
  // std::cout << "codes runs here #-1" << std::endl;
  // The multi-threaded kernel supports neither dilation nor hybrid kernels, and
  // is incompatible with mutable input filters that might change between evals.
  // data->supports_multithreaded_kernel =
  //     (kernel_type == kMultithreadOptimized) &&
  //     (context->recommended_num_threads != 1) && !is_hybrid &&
  //     (dilation_width_factor == 1) &&
  //     (dilation_height_factor == 1) &&
  //     (filter->allocation_type != kTfLiteArenaRw) && !IsDynamicTensor(filter);
  data->supports_multithreaded_kernel = data_supports_multithreaded_kernel;
  // const char * bool_value = data->supports_multithreaded_kernel ? "true" : "false";
  // std::cout << bool_value << std::endl;    
  // std::cout << "codes runs here #-1.1" << std::endl;

  
  // int channels_in = filter->dims->data[3];
  int channels_in = filter_input_channel;
  // int channels_out = filter->dims->data[0];
  int channels_out = filter_output_channel;
  int width = input->dims->data[2];
  int height = input->dims->data[1];
  // int filter_width = filter->dims->data[2];
  // int filter_height = filter->dims->data[1];
  int batches = input->dims->data[0];
  //----------------------------------------------------------------------------
  // std::cout << "codes runs here #-1.2" << std::endl;
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = paddings;
  int out_width, out_height;
  data->padding = ComputePaddingHeightWidth(
      stride_height, stride_width,
      dilation_height_factor, dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
  // std::cout << "codes runs here #-1.2" << std::endl;
  size_t im2col_type_size;
  TF_LITE_ENSURE_STATUS(GetSizeOfType(context, input->type, &im2col_type_size));
  // Note that we intentionally promote the first multiplicand (i.e. 'batches')
  // to 'size_t' to avoid integer overflow here.
  const size_t im2col_bytes = static_cast<size_t>(batches) * out_height *
                              out_width * channels_in * filter_height *
                              filter_width * im2col_type_size;
  TF_LITE_ENSURE_STATUS(AllocateTemporaryTensorsIfRequired(
      context, node, is_hybrid, data->is_hybrid_per_channel, kernel_type,
      im2col_bytes));
  // std::cout << "codes runs here #0" << std::endl;
  // TF_LITE_ENSURE(context, has_bias);

  // Note that full fixed-point inference requires that all tensors have their
  // parameters set. This is usually done during quantized training or
  // calibration.
  if (input_type != kTfLiteFloat32) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||
                             affine_quantization->scale->size == channels_out));

    data->per_channel_output_multiplier.resize(channels_out);
    data->per_channel_output_shift.resize(channels_out);
    // TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
    //     context, input, filter, bias, output, activation,
    //     &data->output_multiplier, &data->output_shift,
    //     &data->output_activation_min, &data->output_activation_max,
    //     data->per_channel_output_multiplier.data(),
    //     data->per_channel_output_shift.data(), channels_out));
  }
  // std::cout << "codes runs here #0.1" << std::endl;
  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);
  output_size->data[0] = batches;
  output_size->data[1] = out_height;
  output_size->data[2] = out_width;
  output_size->data[3] = channels_out;
  auto output_status = context->ResizeTensor(context, output, output_size);

  if (output_status != kTfLiteOk) return output_status;

  if (data->need_im2col) {
    node->temporaries->data[data->im2col_index] = data->im2col_id;

    TfLiteIntArray* im2col_size = TfLiteIntArrayCreate(4);

    // auto filter_input_channel = filter->dims->data[3];
    im2col_size->data[0] = output_size->data[0];
    im2col_size->data[1] = output_size->data[1];
    im2col_size->data[2] = output_size->data[2];
    im2col_size->data[3] = filter_input_channel * filter_height * filter_width;

    TfLiteTensor* im2col =
        &context->tensors[node->temporaries->data[data->im2col_index]];
    im2col->type = input->type;
    if (is_hybrid) {
      im2col->type = filter_type;
    }
    im2col->allocation_type = kTfLiteArenaRw;
    auto im2col_status = context->ResizeTensor(context, im2col, im2col_size);
    if (im2col_status != kTfLiteOk) return im2col_status;
  }
  // std::cout << "codes runs here #0.2" << std::endl;
  if (data->need_hwcn_weights) {
    node->temporaries->data[data->hwcn_weights_index] = data->hwcn_weights_id;
    TfLiteIntArray* hwcn_weights_size = TfLiteIntArrayCreate(2);

    // Because we're treating the filter weights as a matrix when we do the
    // transpose, we allocate the buffer with a two-dimensional shape, where one
    // dimension is the number of elements in each filter, and the second is the
    // total number of filters.
    // auto filter_input_channel = filter->dims->data[3];
    hwcn_weights_size->data[0] =
        (filter_height * filter_width * filter_input_channel);
    hwcn_weights_size->data[1] = channels_out;

    TfLiteTensor* hwcn_weights =
        &context->tensors[node->temporaries->data[data->hwcn_weights_index]];
    hwcn_weights->type = input_type;
    hwcn_weights->allocation_type = kTfLiteArenaRwPersistent;

    auto hwcn_weights_status =
        context->ResizeTensor(context, hwcn_weights, hwcn_weights_size);
    if (hwcn_weights_status != kTfLiteOk) return hwcn_weights_status;

    // TODO(petewarden): If Resize() is called when the size hasn't actually
    // changed, this will do extra redundant work.
    data->have_weights_been_transposed = false;
  }
  // std::cout << "codes runs here #0.3" << std::endl;
  if (is_hybrid) {
    node->temporaries->data[data->input_quantized_index] =
        data->input_quantized_id;
    TfLiteTensor* input_quantized;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->input_quantized_index,
                                  &input_quantized));
    input_quantized->type = kTfLiteInt8;
    input_quantized->allocation_type = kTfLiteArenaRw;
    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {
      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,
                                                       input_quantized_size));
    }
    // std::cout << "codes runs here #0.4" << std::endl;
    node->temporaries->data[data->scaling_factors_index] =
        data->scaling_factors_id;
    TfLiteTensor* scaling_factors;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->scaling_factors_index,
                                  &scaling_factors));
    scaling_factors->type = kTfLiteFloat32;
    scaling_factors->allocation_type = kTfLiteArenaRw;
    // Only one scale factor per batch is typically necessary. See optimized
    // implementation for why we need to allocate for the height of the inputs
    // flattened to 2D.
    TF_LITE_ENSURE(context, channels_in != 0);
    const int height = NumElements(input) / channels_in;
    int scaling_dims[1] = {height};
    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {
      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);
      scaling_factors_size->data[0] = height;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,
                                                       scaling_factors_size));
    }
    // std::cout << "codes runs here #0.5" << std::endl;
    node->temporaries->data[data->accum_scratch_index] = data->accum_scratch_id;
    TfLiteTensor* accum_scratch;
    TF_LITE_ENSURE_OK(context,
                      GetTemporarySafe(context, node, data->accum_scratch_index,
                                       &accum_scratch));
    accum_scratch->type = kTfLiteInt32;
    accum_scratch->allocation_type = kTfLiteArenaRw;
    const int scratch_width = batches * out_height * out_width;
    int accum_scratch_dims[2] = {channels_out, scratch_width};
    if (!TfLiteIntArrayEqualsArray(accum_scratch->dims, 2,
                                   accum_scratch_dims)) {
      TfLiteIntArray* accum_scratch_size = TfLiteIntArrayCreate(2);
      accum_scratch_size->data[0] = channels_out;
      accum_scratch_size->data[1] = scratch_width;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, accum_scratch,
                                                       accum_scratch_size));
    }
    // std::cout << "codes runs here #0.6" << std::endl;
    if (data->is_hybrid_per_channel) {
      const auto* affine_quantization =
          reinterpret_cast<TfLiteAffineQuantization*>(
              filter->quantization.params);
      TF_LITE_ENSURE_EQ(
          context, affine_quantization->scale->size,
          filter->dims->data[affine_quantization->quantized_dimension]);
      node->temporaries->data[data->input_offset_index] = data->input_offset_id;
      TfLiteTensor* input_offsets;
      TF_LITE_ENSURE_OK(
          context, GetTemporarySafe(context, node, data->input_offset_index,
                                    &input_offsets));
      input_offsets->type = kTfLiteInt32;
      input_offsets->allocation_type = kTfLiteArenaRw;
      // See above comment for the need to allocate for height of inputs.
      TF_LITE_ENSURE(context, channels_in != 0);
      const int height = NumElements(input) / channels_in;
      const int input_offset_dims[1] = {height};
      if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1,
                                     input_offset_dims)) {
        TfLiteIntArray* input_offsets_size = TfLiteIntArrayCreate(1);
        input_offsets_size->data[0] = input_offset_dims[0];
        TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_offsets,
                                                         input_offsets_size));
      }
      node->temporaries->data[data->row_sums_index] = data->row_sums_id;
      TfLiteTensor* row_sums;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));
      row_sums->type = kTfLiteInt32;
      row_sums->allocation_type = kTfLiteArenaRwPersistent;
      // See above comment for the need to allocate for height of inputs.
      const int row_sums_dims[1] = {channels_out};
      if (!TfLiteIntArrayEqualsArray(row_sums->dims, 1, row_sums_dims)) {
        TfLiteIntArray* row_sums_size = TfLiteIntArrayCreate(1);
        row_sums_size->data[0] = row_sums_dims[0];
        TF_LITE_ENSURE_OK(
            context, context->ResizeTensor(context, row_sums, row_sums_size));
      }
    }
  }
  // std::cout << "codes runs here #1" << std::endl;
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return Prepare(kernel_type, context, node);
}

template <KernelType kernel_type>
void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* output) {
  auto input_offset = -input->params.zero_point;
  auto filter_offset = -filter->params.zero_point;
  auto output_offset = output->params.zero_point;

  KernelType effective_kernel_type;
  if ((kernel_type == kMultithreadOptimized ||
       kernel_type == kCblasOptimized) &&
      (params->dilation_width_factor != 1 ||
       params->dilation_height_factor != 1)) {
    // kMultithreadOptimized and kCblasOptimized do not support dilation.
    // Therefore, fallback to optimized.
    effective_kernel_type = kGenericOptimized;
  } else {
    effective_kernel_type = kernel_type;
  }

  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.dilation_width_factor = dilation_width_factor;
  op_params.dilation_height_factor = dilation_height_factor;
  op_params.stride_width = stride_width;
  op_params.stride_height = stride_height;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  switch (effective_kernel_type) {
    case kReference: {
      reference_ops::Conv(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(filter), GetTensorData<uint8_t>(filter),
          GetTensorShape(bias), GetTensorData<int32_t>(bias),
          GetTensorShape(output), GetTensorData<uint8_t>(output),
          GetTensorShape(im2col), GetTensorData<uint8_t>(im2col),
          /* cpu_backend_context = */ nullptr);
      break;
    }
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      // There is only one optimized implementation for Quantized Conv.
      optimized_ops::Conv(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(filter), GetTensorData<uint8_t>(filter),
          GetTensorShape(bias), GetTensorData<int32_t>(bias),
          GetTensorShape(output), GetTensorData<uint8_t>(output),
          GetTensorShape(im2col), GetTensorData<uint8_t>(im2col),
          CpuBackendContext::GetFromContext(context));
      break;
    }
  }
}

template <KernelType kernel_type>
void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, OpData* data,
                             const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             TfLiteTensor* im2col) {
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = stride_height;
  op_params.stride_width = stride_width;
  op_params.dilation_height_factor = dilation_height_factor;
  op_params.dilation_width_factor = dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  switch (effective_kernel_type) {
    case kReference: {
      reference_integer_ops::ConvPerChannel(
          op_params, data->per_channel_output_multiplier.data(),
          data->per_channel_output_shift.data(), GetTensorShape(input),
          GetTensorData<int8>(input), GetTensorShape(filter),
          GetTensorData<int8>(filter), GetTensorShape(bias),
          GetTensorData<int32>(bias), GetTensorShape(output),
          GetTensorData<int8>(output));
      break;
    }
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      optimized_integer_ops::ConvPerChannel(
          op_params, data->per_channel_output_multiplier.data(),
          data->per_channel_output_shift.data(), GetTensorShape(input),
          GetTensorData<int8>(input), GetTensorShape(filter),
          GetTensorData<int8>(filter), GetTensorShape(bias),
          GetTensorData<int32>(bias), GetTensorShape(output),
          GetTensorData<int8>(output), GetTensorShape(im2col),
          GetTensorData<int8>(im2col),
          CpuBackendContext::GetFromContext(context));
      break;
    }
  }
}

template <KernelType kernel_type>
void EvalQuantizedPerChannel16x8(TfLiteContext* context, TfLiteNode* node,
                                 TfLiteConvParams* params, OpData* data,
                                 const TfLiteTensor* input,
                                 const TfLiteTensor* filter,
                                 const TfLiteTensor* bias, TfLiteTensor* output,
                                 TfLiteTensor* im2col) {
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = stride_height;
  op_params.stride_width = stride_width;
  op_params.dilation_height_factor = dilation_height_factor;
  op_params.dilation_width_factor = dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  // To prevent 32bit accum overflow for 16x8 quantization, it enables the
  // optimized path only when zero_point is 0.
  bool has_non_zero_point = input->params.zero_point ||
                            filter->params.zero_point ||
                            output->params.zero_point;

  // Fallback to reference kernel when bias_type is int64 as
  // there is no optimized kernel for int64 bias yet.
  if (bias && bias->type == kTfLiteInt64) {
    reference_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<std::int64_t>(bias), GetTensorShape(output),
        GetTensorData<int16>(output));
  } else if (effective_kernel_type == kReference || has_non_zero_point) {
    reference_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<std::int32_t>(bias), GetTensorShape(output),
        GetTensorData<int16>(output));
  } else {
    optimized_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16_t>(input), GetTensorShape(filter),
        GetTensorData<int8_t>(filter), GetTensorShape(bias),
        GetTensorData<std::int32_t>(bias), GetTensorShape(output),
        GetTensorData<int16_t>(output), GetTensorShape(im2col),
        GetTensorData<int16_t>(im2col),
        CpuBackendContext::GetFromContext(context));
  }
}

template <KernelType kernel_type>
void EvalFloat(TfLiteContext* context, TfLiteNode* node,
               TfLiteConvParams* params, OpData* data,
               const TfLiteTensor* input, const TfLiteTensor* filter,
               const TfLiteTensor* bias, TfLiteTensor* im2col,
               TfLiteTensor* hwcn_weights, TfLiteTensor* output) {

  float output_activation_min, output_activation_max;

//----------------------------------------------------------------
  CalculateActivationRange(activation, &output_activation_min,
                           &output_activation_max);
  KernelType effective_kernel_type = kernel_type;
  // Fall back to the optimized path if multi-threaded conv is unsupported.
  if ((kernel_type == kMultithreadOptimized) &&
      !data->supports_multithreaded_kernel) {
    effective_kernel_type = kGenericOptimized;
  }

  // When im2col is needed (which is implied when 'im2col_oversized' is true),
  // the GEMMM-based optimized path requires im2col data be allocated to ensure
  // the correctness. Therefore, when im2col is disabled because of the
  // oversized temporary im2col tensor, fallback to a non-optimized path is
  // needed.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
    // As detailed by tflite::multithreaded_ops::Conv implementation in
    // multithreaded_conv.h, the Eigen-based execution doesn't need im2col data.
    // Therefore, we could rely on it as a better-optimized fallback than the
    // reference one.
    if (data->supports_multithreaded_kernel) {
      effective_kernel_type = kMultithreadOptimized;
    }
#endif
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(paddings);
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = stride_width;
  op_params.stride_height = stride_height;
  op_params.dilation_width_factor = dilation_width_factor;
  op_params.dilation_height_factor = dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  // std::cout << "codes runs here #5" << std::endl;
  // const int filter_dims_size=4;
  const int32_t* filter_dims_data;
  filter_dims_data = filter_dims_raw;
  const int32_t* bias_dims_data;
  bias_dims_data = bias_dims_raw;

  const float* filter_data;
  filter_data = filter_raw;
  const float* bias_data;
  bias_data = bias_raw;

  switch (effective_kernel_type) {
    case kReference: {
      // std::cout << "codes runs here #6" << std::endl;
      reference_ops::Conv(op_params, GetTensorShape(input),
                          GetTensorData<float>(input), RuntimeShape(filter_dims_size, filter_dims_data),
                          filter_data, RuntimeShape(bias_dims_size, bias_dims_data),
                          bias_data, GetTensorShape(output),
                          GetTensorData<float>(output), GetTensorShape(im2col),
                          GetTensorData<float>(im2col));
      break;
    }
    case kCblasOptimized:
    case kGenericOptimized: {
      // std::cout << "codes runs here #7" << std::endl;
      optimized_ops::Conv(op_params, GetTensorShape(input),
                          GetTensorData<float>(input), RuntimeShape(filter_dims_size, filter_dims_data),
                          filter_data, RuntimeShape(bias_dims_size, bias_dims_data),
                          bias_data, GetTensorShape(output),
                          GetTensorData<float>(output), GetTensorShape(im2col),
                          GetTensorData<float>(im2col),
                          CpuBackendContext::GetFromContext(context));
      break;
    }
    case kMultithreadOptimized: {
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
      
      multithreaded_ops::Conv(
          *eigen_support::GetThreadPoolDevice(context), op_params,
          GetTensorShape(input), GetTensorData<float>(input),
          RuntimeShape(filter_dims_size, filter_dims_data), filter_data, RuntimeShape(bias_dims_size, bias_dims_data),
          bias_data, GetTensorShape(output),
          GetTensorData<float>(output), GetTensorShape(im2col),
          GetTensorData<float>(im2col));
      break;
#else   // !defined(TFLITE_WITH_MULTITHREADED_EIGEN)
      // See Register_CONV_2D: we should never be here when TFLITE_WITH_RUY
      // was enabled. We #if out this code in order to get the corresponding
      // binary size benefits.
      TFLITE_DCHECK(false);
#endif  // defined(TFLITE_WITH_MULTITHREADED_EIGEN)
    }
  }
}

template <KernelType kernel_type>
TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,
                                  TfLiteConvParams* params, OpData* data,
                                  const TfLiteTensor* input,
                                  const TfLiteTensor* filter,
                                  const TfLiteTensor* bias,
                                  TfLiteTensor* im2col, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(activation, &output_activation_min,
                           &output_activation_max);

  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;
  TfLiteTensor* quantized_input_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &quantized_input_tensor));
  int8_t* quantized_input_ptr_batch =
      GetTensorData<int8_t>(quantized_input_tensor);
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);
  TfLiteTensor* input_offset_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_offset_index,
                                     &input_offset_tensor));
  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);

  for (int b = 0; b < batch_size; ++b) {
    const int offset = b * input_size;
    tensor_utils::AsymmetricQuantizeFloats(
        GetTensorData<float>(input) + offset, input_size,
        quantized_input_ptr_batch + offset, &scaling_factors_ptr[b],
        &input_offset_ptr[b]);
  }

  int8_t* im2col_ptr = nullptr;
  int8_t* filter_ptr = nullptr;
  if (im2col != nullptr) {
    im2col_ptr = im2col->data.int8;
  }
  filter_ptr = filter->data.int8;
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.dilation_width_factor = dilation_width_factor;
  op_params.dilation_height_factor = dilation_height_factor;
  op_params.stride_width = stride_width;
  op_params.stride_height = stride_height;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  switch (effective_kernel_type) {
    case kReference:
      reference_ops::HybridConvPerChannel(
          op_params, scaling_factors_ptr, GetTensorShape(input),
          quantized_input_ptr_batch, GetTensorShape(filter), filter_ptr,
          GetTensorShape(bias), GetTensorData<float>(bias),
          GetTensorShape(output), GetTensorData<float>(output),
          GetTensorShape(im2col), im2col_ptr, affine_quantization->scale->data,
          input_offset_ptr);
      break;
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      TfLiteTensor* row_sums;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));
      TfLiteTensor* scratch;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->accum_scratch_index, &scratch));
      optimized_ops::HybridConvPerChannel(
          op_params, scaling_factors_ptr, GetTensorShape(input),
          quantized_input_ptr_batch, GetTensorShape(filter), filter_ptr,
          GetTensorShape(bias), GetTensorData<float>(bias),
          GetTensorShape(output), GetTensorData<float>(output),
          GetTensorShape(im2col), im2col_ptr, affine_quantization->scale->data,
          input_offset_ptr, GetTensorShape(scratch),
          GetTensorData<int32>(scratch), GetTensorData<int32_t>(row_sums),
          &data->compute_hybrid_row_sums,
          CpuBackendContext::GetFromContext(context));
      data->compute_hybrid_row_sums = false;
      break;
    }
  }

  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalHybrid(TfLiteContext* context, TfLiteNode* node,
                        TfLiteConvParams* params, OpData* data,
                        const TfLiteTensor* input, const TfLiteTensor* filter,
                        const TfLiteTensor* bias, TfLiteTensor* im2col,
                        TfLiteTensor* accum_scratch, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(activation, &output_activation_min,
                           &output_activation_max);

  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;

  const float* input_ptr = GetTensorData<float>(input);
  TfLiteTensor* quantized_input_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &quantized_input_tensor));
  int8_t* quantized_input_ptr_batch =
      GetTensorData<int8_t>(quantized_input_tensor);
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);

  // Per-batch input quantization for higher accuracy.
  {
    ruy::profiler::ScopeLabel label("ConvHybridQuantizeInputs");
    for (int b = 0; b < batch_size; ++b) {
      float unused_min, unused_max;
      const int offset = b * input_size;
      tensor_utils::SymmetricQuantizeFloats(
          input_ptr + offset, input_size, quantized_input_ptr_batch + offset,
          &unused_min, &unused_max, &scaling_factors_ptr[b]);
      scaling_factors_ptr[b] *= filter->params.scale;
    }
  }

  switch (kernel_type) {
    case kReference:
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      // There is only one implementation for hybrid kernel.
      ConvParams op_params;
      op_params.padding_type = PaddingType::kSame;
      op_params.padding_values.width = data->padding.width;
      op_params.padding_values.height = data->padding.height;
      op_params.stride_width = stride_width;
      op_params.stride_height = stride_height;
      op_params.dilation_width_factor = dilation_width_factor;
      op_params.dilation_height_factor = dilation_height_factor;
      op_params.float_activation_min = output_activation_min;
      op_params.float_activation_max = output_activation_max;
      if (data->groups == 1) {
        optimized_ops::HybridConv(
            op_params, scaling_factors_ptr, GetTensorShape(input),
            quantized_input_ptr_batch, GetTensorShape(filter),
            GetTensorData<int8_t>(filter), GetTensorShape(bias),
            GetTensorData<float>(bias), GetTensorShape(accum_scratch),
            GetTensorData<int32_t>(accum_scratch), GetTensorShape(output),
            GetTensorData<float>(output), GetTensorShape(im2col),
            GetTensorData<int8_t>(im2col),
            CpuBackendContext::GetFromContext(context));
      } else {
        // This case is handled by (fallbacked to) per channel hybrid group conv
        // and shouldn't hit this branch.
        TF_LITE_KERNEL_LOG(
            context,
            "Group convolution currently not supported for hybrid kernel.");
        return kTfLiteError;
      }
      break;
    }
  }

  return kTfLiteOk;
}

template <KernelType kernel_type, TfLiteType input_type>
TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {
  // std::cout << "codes runs here #2" << std::endl;
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams* params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  const TfLiteTensor* filter;
  const int dims_size = filter_dims_size;

  // std::cout << "filter dim size:" << dims_size << std::endl;
  // const int32_t* dims_data = reinterpret_cast<const int32_t*>(dims->data);
  const int32_t* dims_data = filter_dims_raw;
  // bool has_bias = node->inputs->size == 3;
  // const TfLiteTensor* bias = has_bias ? GetInput(context, node, 2) : nullptr;
  const TfLiteTensor* bias = nullptr;
  TfLiteTensor* im2col =
      data->need_im2col
          ? &context->tensors[node->temporaries->data[data->im2col_index]]
          : nullptr;
  TfLiteTensor* hwcn_weights =
      data->need_hwcn_weights
          ? &context->tensors[node->temporaries->data[data->hwcn_weights_index]]
          : nullptr;

  if (data->need_hwcn_weights && !data->have_weights_been_transposed) {
    TransposeFloatTensor(filter, hwcn_weights);
    data->have_weights_been_transposed = true;
  }
  // std::cout << "codes runs here #3" << std::endl;

  TFLITE_DCHECK_EQ(input_type, input->type);
  switch (input_type) {  // Already know in/outtypes are same.
    case kTfLiteFloat32:
      if (filter_type == kTfLiteUInt8 || filter_type == kTfLiteInt8) {
        if (data->is_hybrid_per_channel ||
            // TODO(b/162870360): Fallback to PerChannel implementation
            // before we have grouped hybrid convolution.
            data->groups != 1) {
          TF_LITE_ENSURE_OK(context, EvalHybridPerChannel<kernel_type>(
                                         context, node, params, data, input,
                                         filter, bias, im2col, output));
        } else {
          TfLiteTensor* accum_scratch =
              &context->tensors[node->temporaries
                                    ->data[data->accum_scratch_index]];
          TF_LITE_ENSURE_OK(context,
                            EvalHybrid<kernel_type>(context, node, params, data,
                                                    input, filter, bias, im2col,
                                                    accum_scratch, output));
        }
      } else {
        EvalFloat<kernel_type>(context, node, params, data, input, filter, bias,
                               im2col, hwcn_weights, output);
      }
      break;
    case kTfLiteUInt8:
      EvalQuantized<kernel_type>(context, node, params, data, input, filter,
                                 bias, im2col, output);
      break;
    case kTfLiteInt8:
      EvalQuantizedPerChannel<kernel_type>(context, node, params, data, input,
                                           filter, bias, output, im2col);
      break;
    case kTfLiteInt16:
      EvalQuantizedPerChannel16x8<kernel_type>(
          context, node, params, data, input, filter, bias, output, im2col);
      break;
    default:
      TF_LITE_KERNEL_LOG(context, "Type %s currently not supported.",
                         TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));

  switch (input->type) {
    case kTfLiteFloat32:
      return EvalImpl<kernel_type, kTfLiteFloat32>(context, node);
    case kTfLiteUInt8:
      return EvalImpl<kernel_type, kTfLiteUInt8>(context, node);
    case kTfLiteInt8:
      return EvalImpl<kernel_type, kTfLiteInt8>(context, node);
    case kTfLiteInt16:
      return EvalImpl<kernel_type, kTfLiteInt16>(context, node);
    default:
      TF_LITE_KERNEL_LOG(context, "Type %s not currently supported.",
                         TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
}

}  // namespace conv

TfLiteRegistration* Register_pfoxdt_REF() {
  static TfLiteRegistration r = {pfoxdt::Init, pfoxdt::Free,
                                 pfoxdt::Prepare<pfoxdt::kReference>,
                                 pfoxdt::Eval<pfoxdt::kReference>};
  return &r;
}

TfLiteRegistration* Register_pfoxdt_GENERIC_OPT() {
  static TfLiteRegistration r = {pfoxdt::Init, pfoxdt::Free,
                                 pfoxdt::Prepare<pfoxdt::kGenericOptimized>,
                                 pfoxdt::Eval<pfoxdt::kGenericOptimized>};
  return &r;
}

TfLiteRegistration* Register_pfoxdt_MULTITHREADED_OPT() {
  static TfLiteRegistration r = {pfoxdt::Init, pfoxdt::Free,
                                 pfoxdt::Prepare<pfoxdt::kMultithreadOptimized>,
                                 pfoxdt::Eval<pfoxdt::kMultithreadOptimized>};
  return &r;
}

// TfLiteRegistration* Register_pfoxdt_CBLAS_OPT() {
//   static TfLiteRegistration r = {pfoxdt::Init, pfoxdt::Free,
//                                  pfoxdt::Prepare<pfoxdt::kCblasOptimized>,
//                                  pfoxdt::Eval<pfoxdt::kCblasOptimized>};
//   return &r;
// }

TfLiteRegistration* Register_pfoxdt() {
#if defined TFLITE_WITH_MULTITHREADED_EIGEN
  return Register_pfoxdt_MULTITHREADED_OPT();
#else
  return Register_pfoxdt_GENERIC_OPT();
#endif
}


}  // namespace builtin
}  // namespace ops
}  // namespace tflite
