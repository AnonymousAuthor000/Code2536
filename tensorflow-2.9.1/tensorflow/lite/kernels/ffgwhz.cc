/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
#include "tensorflow/lite/kernels/internal/optimized/integer_ops/conv.h"

#include <stddef.h>
#include <iostream>
#include <cstdint>
#include <vector>

// Only use multi-threaded Eigen if ruy is disabled.
#if !defined(TFLITE_WITH_RUY)
#define TFLITE_WITH_MULTITHREADED_EIGEN
#endif

#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/common.h"
#include "tensorflow/lite/kernels/cpu_backend_context.h"
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
#include "tensorflow/lite/kernels/eigen_support.h"
#endif
#include "tensorflow/lite/kernels/internal/compatibility.h"
#include "tensorflow/lite/kernels/internal/types.h"
// b/131835803 forces us to include multithreaded_conv.h before optimized_ops.h
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
#include "tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h"
#endif
#include "tensorflow/lite/kernels/internal/optimized/optimized_ops.h"
#include "tensorflow/lite/kernels/internal/quantization_util.h"
#include "tensorflow/lite/kernels/internal/reference/conv.h"
#include "tensorflow/lite/kernels/internal/reference/integer_ops/conv.h"
#include "tensorflow/lite/kernels/internal/tensor.h"
#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
#include "tensorflow/lite/kernels/internal/tensor_utils.h"
#include "tensorflow/lite/kernels/kernel_util.h"
#include "tensorflow/lite/kernels/padding.h"
#include "tensorflow/lite/util.h"

namespace tflite {
namespace ops {
namespace custom {
namespace ffgwhz {

// This file has 4 implementation of Conv.
enum KernelType {
  kReference,
  kGenericOptimized,  // Neon-free
  // kMultithreadOptimized is a mixture of an Eigen-based kernel when threads
  // are available and kGenericOptimized when we must use only one thread.
  kMultithreadOptimized,
  // The kernel uses use CBLAS interface for matrix multiplication.
  // It's fast when an optimized CBLAS implementation is available (e.g. Apple
  // Accelerate Framework), and it's slow when falling back to naive
  // implementation.
  kCblasOptimized,
};

const int kTensorNotAllocated = -1;

static constexpr size_t kMaxIm2colBufferSizeMobile = 1024 * 1024 * 1024;  // 1GB

const float filter_raw[4096]={-0.09838703274726868, -0.0758017897605896, 0.038365673273801804, -0.1287396103143692, 0.16179020702838898, 0.2749735713005066, 0.15117134153842926, -0.17726297676563263, -0.17541655898094177, -0.052529048174619675, -0.01617392525076866, 0.07591161131858826, -0.05402807518839836, -0.04436297342181206, -0.03344080597162247, 0.15059016644954681, 0.028719039633870125, 0.24594971537590027, -0.04908880218863487, 0.09842950850725174, 0.01970800757408142, -0.09953156858682632, 0.26410210132598877, 0.16511516273021698, 0.005190052092075348, 0.01573200710117817, 0.0012166221858933568, -0.004373167175799608, -0.026663735508918762, 0.027404652908444405, 0.020675046369433403, 0.11685249209403992, -0.03470231220126152, -0.0725686252117157, -0.06637933105230331, 0.030929984524846077, 0.0978914350271225, -0.14620697498321533, -0.1489519476890564, -0.003042783821001649, -0.015335993841290474, -0.03220503032207489, 0.17599186301231384, -0.2232833206653595, 0.1061750277876854, -0.016724014654755592, -0.09565193206071854, -0.059053532779216766, -0.14798793196678162, 0.03274393081665039, -0.09425536543130875, -0.04861588776111603, -0.11125381290912628, 0.03399641439318657, -0.30114537477493286, 0.06302276253700256, 0.14953561127185822, 0.036411527544260025, 0.05087145417928696, -0.03940986469388008, -0.03896942734718323, -0.3277497887611389, -0.1425807625055313, -0.1173490434885025, 0.11241212487220764, -0.23081590235233307, -0.05208908021450043, 0.048546478152275085, -0.2449568510055542, -0.14332547783851624, -0.19844664633274078, -0.1834104210138321, 0.11547940224409103, 0.2248094081878662, -0.08905404061079025, -0.0906481146812439, 0.14411550760269165, 0.06379902362823486, -0.06117551401257515, 0.06967964768409729, 0.07424132525920868, -0.1430748999118805, 0.15177640318870544, 0.12269573658704758, -0.0649290382862091, -0.061549074947834015, 0.18811242282390594, 0.10478635877370834, 0.0038470271974802017, -0.03191828727722168, -0.03701852262020111, 0.010659128427505493, 0.14757312834262848, -0.1930602341890335, -0.23183636367321014, 0.0987871065735817, 0.06213018670678139, 0.18336336314678192, -0.15513351559638977, 0.1012231633067131, -0.15507595241069794, -0.10096990317106247, -0.09621480852365494, 0.19641876220703125, -0.09165509045124054, 0.11359978467226028, 0.16991691291332245, -0.06841963529586792, -0.12515993416309357, -0.03857099264860153, 0.0837317407131195, 0.011860286816954613, -0.12959839403629303, 0.13377447426319122, 0.1710028350353241, 0.07010305672883987, -0.07482846081256866, -0.20926937460899353, -0.2344355434179306, -0.042493902146816254, -0.13386288285255432, -0.02381192147731781, 0.08778858184814453, -0.08750037848949432, -0.09028372913599014, 0.06695052236318588, -0.0704948827624321, 0.12279411405324936, -0.07991129904985428, 0.018723629415035248, -0.004304969217628241, 0.021890468895435333, -0.022973231971263885, -0.28328007459640503, 0.1063435897231102, -0.042682945728302, -0.04546945542097092, -0.04202581197023392, -0.012204062193632126, 0.08151358366012573, -0.12543605268001556, 0.07057628035545349, 0.06738662719726562, -0.03850462660193443, 0.13731428980827332, 0.09289772808551788, -0.06795413047075272, -0.1204608902335167, -0.019720373675227165, -0.06106039136648178, -0.0594753660261631, -0.11760036647319794, -0.09225253015756607, -0.012045401148498058, -0.11443539708852768, 0.08107113093137741, -0.032072052359580994, -0.04198858141899109, -0.06432514637708664, 0.2558765709400177, -0.20126070082187653, 0.1535549759864807, 0.23606063425540924, -0.15523287653923035, 0.058437615633010864, -0.11711850017309189, 0.11783535033464432, -0.0814487412571907, -0.31968748569488525, 0.013307780027389526, -0.009260392747819424, -0.10299982875585556, 0.0016992740565910935, -0.13129092752933502, 0.09583914279937744, -0.049788981676101685, -0.017203642055392265, -0.06464660167694092, 0.12266413867473602, -0.08829515427350998, -0.3028620779514313, -0.08698118478059769, 0.036780945956707, -0.060513194650411606, 0.14012733101844788, -0.07960398495197296, 0.04032145068049431, 0.01609434187412262, 0.047670476138591766, 0.004114930052310228, -0.07947012782096863, -0.12122484296560287, 0.12993784248828888, -0.09595228731632233, -0.10855139046907425, -0.05380035936832428, 0.15133249759674072, -0.09598089754581451, -0.08829162269830704, -0.09524167329072952, 0.04531903192400932, 0.05583550035953522, -0.10120484232902527, 0.006024467293173075, -0.15088093280792236, -0.03188421204686165, 0.08798360824584961, -0.004757406190037727, -0.012901952490210533, -0.17451734840869904, -0.052903082221746445, 0.00800841674208641, -0.10016625374555588, 0.1734175831079483, 0.08936901390552521, 0.06343473494052887, -0.02620418183505535, 0.016094107180833817, 0.17048601806163788, 0.23611171543598175, 0.026742834597826004, 0.12615525722503662, -0.04848092421889305, -0.18483710289001465, -0.008773705922067165, -0.037312451750040054, -0.10682526230812073, -0.01644059084355831, -0.04378741607069969, 0.11127087473869324, -0.061902694404125214, -0.007841285318136215, -0.09626477211713791, -0.09789306670427322, -0.01658235676586628, 0.17493733763694763, -0.29400235414505005, -0.03378819301724434, 0.1624125987291336, -0.0015322444960474968, 0.0371226966381073, -0.09069298207759857, -0.05270659551024437, -0.09536144882440567, -0.023926885798573494, -0.044341396540403366, -0.105868861079216, 0.06174040958285332, -0.05525640398263931, -0.08916367590427399, 0.0024018208496272564, 0.1703946441411972, 0.15356191992759705, 0.15468642115592957, 0.07658762484788895, -0.002157514449208975, 0.09032072871923447, 0.003536075120791793, -0.046592552214860916, -0.17110253870487213, -0.13803060352802277, 0.09537404030561447, 0.042826905846595764, -0.12840817868709564, 0.08721313625574112, 0.032430194318294525, 0.1899791657924652, 0.13146227598190308, -0.170733243227005, 0.039701592177152634, 0.005761837121099234, 0.10746703296899796, -0.19248823821544647, 0.04198494926095009, 0.010762395337224007, 0.10384496301412582, -0.17772361636161804, 0.02742897719144821, 0.1869724839925766, 0.057761043310165405, 0.16973206400871277, -0.1097128838300705, -0.14304277300834656, -0.13054633140563965, -0.17772342264652252, 0.18224161863327026, 0.13336655497550964, -0.0897456556558609, 0.2803432047367096, -0.015289049595594406, -0.0275286678224802, 0.2970936596393585, 0.0667169988155365, -0.25074705481529236, 0.14248311519622803, -0.03296288475394249, -0.12427812069654465, -0.006860114634037018, 0.03270382061600685, -0.182742640376091, 0.12072011083364487, -0.23547373712062836, -0.005195675417780876, -0.11515530943870544, -0.08447181433439255, 0.020157339051365852, -0.05834183841943741, 0.3250696361064911, -0.015286828391253948, -0.09456545114517212, 0.000992047251202166, 0.03938046842813492, 0.0025050565600395203, 0.01455923542380333, -0.09861069917678833, 0.11137672513723373, -0.043194908648729324, 0.11560065299272537, 0.003749984549358487, 0.04549931362271309, -0.3028409481048584, -0.0752582922577858, 0.007340640295296907, -0.24295751750469208, -0.08802653849124908, 0.19544008374214172, -0.14059554040431976, -0.09393621236085892, -0.016527336090803146, 0.0403546541929245, -0.08743946254253387, 0.26751670241355896, -0.186113640666008, 0.17052577435970306, 0.0518607459962368, 0.019616128876805305, -0.03504958003759384, -0.06341048330068588, 0.021536169573664665, -0.3205951452255249, -0.03614269196987152, 0.06454475224018097, 0.03439737856388092, 0.12353957444429398, 0.04322618246078491, -0.10179094970226288, 0.1117597222328186, 0.09832756221294403, -0.03109084814786911, 0.08601953089237213, -0.026933927088975906, -0.08569751679897308, -0.0387190543115139, 0.07895264774560928, -0.023303620517253876, 0.2796844244003296, 0.1335178017616272, -0.10888373106718063, 0.06976867467164993, -0.059030041098594666, -0.15343578159809113, 0.15127724409103394, 0.0009097700822167099, -0.10651960968971252, 0.17472632229328156, -0.04681837186217308, 0.02313452586531639, -0.04651019349694252, 0.10424453020095825, 0.07871086150407791, -0.044116705656051636, -0.034919191151857376, -0.18287751078605652, -0.02858222834765911, 0.11907584965229034, -0.03567047044634819, -0.02233138121664524, -0.014039305970072746, 0.026103172451257706, -0.00410861661657691, 0.16610921919345856, 0.18437743186950684, -0.010292800143361092, 0.015685847029089928, 0.001435175072401762, 0.057798121124506, -0.06166795268654823, 0.08497876673936844, 0.14161135256290436, -0.0012257210910320282, 0.004900090396404266, -0.13230657577514648, -0.00041360603063367307, 0.17352065443992615, -0.09828738868236542, -0.21465477347373962, 0.17454299330711365, -0.025858836248517036, -0.005457741674035788, -0.07790626585483551, -0.09292548149824142, -0.0313110314309597, -0.0442621111869812, 0.0658487007021904, 0.08764474093914032, -0.05471854656934738, 0.021203523501753807, -0.08928695321083069, -0.013130832463502884, -0.0013818265870213509, -0.05008618161082268, 0.11100730299949646, -0.025947630405426025, -0.08864880353212357, -0.17029087245464325, -0.06582339853048325, -0.16010649502277374, 0.08404112607240677, -0.07271914929151535, 0.0728178545832634, -0.09929657727479935, 0.062119171023368835, -0.017417186871170998, -0.024754688143730164, -0.009272429160773754, -0.41431257128715515, -0.10709282010793686, 0.1137004867196083, -0.02805117331445217, -0.08244339376688004, 0.06004898250102997, -0.14165480434894562, -0.021056268364191055, 0.01278655044734478, 0.17182323336601257, 0.13512465357780457, 0.10743921250104904, -0.325769305229187, -0.04571188613772392, -0.12426670640707016, -0.08463654667139053, 0.04958541318774223, 0.07700608670711517, -0.07093382626771927, 0.03223605453968048, 0.06636705249547958, 0.3448037803173065, 0.30793750286102295, -0.06413594633340836, 0.0036696612369269133, 0.07059141248464584, 0.13650092482566833, -0.021699173375964165, -0.2392110824584961, -0.11684878170490265, -0.0864509791135788, 0.01641666330397129, 0.048285286873579025, -0.048430103808641434, -0.005104885436594486, 0.0985514223575592, -0.02133261039853096, 0.00811484269797802, -0.06317448616027832, -0.12867355346679688, -0.18142865598201752, 0.039365239441394806, -0.29346805810928345, -0.053658805787563324, -0.11234618723392487, 0.010491338558495045, -0.036808621138334274, -0.016132691875100136, -0.011573323979973793, 0.12310482561588287, 0.11900775134563446, 0.14695222675800323, 0.0662294551730156, -0.061186280101537704, -0.04351099207997322, 0.0882863998413086, -0.10764894634485245, -0.037384264171123505, 0.10114551335573196, -0.027931418269872665, -0.0329531729221344, -0.1479637771844864, -0.050572264939546585, 0.21867527067661285, -0.17024099826812744, 0.10917457193136215, 0.3020872473716736, -0.26721206307411194, 0.08020485192537308, 0.08972635120153427, 0.11503835022449493, 0.3482336401939392, 0.17265091836452484, -0.10793924331665039, 0.027763010933995247, -0.059998951852321625, -0.1358068287372589, -0.11337496340274811, -0.08163317292928696, 0.2052411586046219, -0.014899109490215778, 0.14433583617210388, 0.00809070561081171, 0.2147114872932434, 0.023481054231524467, 0.1557793766260147, 0.16593597829341888, 0.0016359271248802543, 0.07946448773145676, 0.009483646601438522, 0.008792211301624775, -0.012934628874063492, 0.07365310192108154, -0.011039962992072105, 0.07292155921459198, 0.021659160032868385, 0.3829098343849182, -0.37089940905570984, -0.022216711193323135, 0.09356360882520676, 0.0035650499630719423, 0.051699209958314896, 0.04570576176047325, -0.017892207950353622, 0.3582359254360199, 0.06081290543079376, 0.027978263795375824, 0.11785221099853516, 0.008548016659915447, -0.20793627202510834, 0.051595039665699005, 0.061459846794605255, -0.02865980938076973, 0.2552424669265747, 0.03183805197477341, -0.05768517777323723, 0.09103620797395706, -0.2699301242828369, 0.06736109405755997, -0.056672390550374985, 0.09127377718687057, 0.35304296016693115, 0.2740333080291748, -0.1438066065311432, -0.08934048563241959, -0.06584589183330536, 0.0026782427448779345, 0.10133649408817291, -0.20593000948429108, -0.056658536195755005, -0.010406259447336197, -0.043514516204595566, 0.08711060136556625, -0.24569517374038696, -0.02697216533124447, -0.011423340067267418, 0.03995975852012634, 0.22709056735038757, 0.08060338348150253, -0.003997061867266893, -0.0318753644824028, 0.06280601024627686, 0.009780874475836754, 0.01768207550048828, -0.04369756206870079, 0.10133407264947891, 0.0860205739736557, -0.2751842737197876, 0.027836667373776436, 0.0841493234038353, 0.02031644992530346, 0.20809896290302277, -0.21266163885593414, 0.014629540033638477, -0.029512079432606697, 0.006254500709474087, -0.333299845457077, 0.031721312552690506, 0.048605769872665405, -0.07884088903665543, 0.016464756801724434, 0.02258318103849888, 0.10182614624500275, -0.08633558452129364, -0.14217667281627655, -0.11004140228033066, -0.23471538722515106, -0.07438961416482925, 0.016902877017855644, -0.06612849235534668, -0.013323891907930374, -0.07883059978485107, -0.31748414039611816, 0.0040430850349366665, 0.3839673101902008, 0.009745674207806587, 0.017287055030465126, 0.12263701111078262, -0.08430834859609604, -0.11973365396261215, 0.19718120992183685, -0.0020021151285618544, 0.2325979322195053, -0.05735381320118904, 0.0643051341176033, 0.052963271737098694, 0.036158446222543716, -0.014748225919902325, -0.005294740665704012, 0.10194442421197891, -0.17129844427108765, -0.01161116547882557, -0.034122031182050705, 0.21952597796916962, -0.03206241875886917, -0.0742502510547638, 0.03541417047381401, 0.09258865565061569, 0.02065318636596203, -0.1427091360092163, 0.08349217474460602, 0.05245165154337883, 0.007595414761453867, 0.09423299133777618, -0.1367233544588089, 0.2149920016527176, -0.11697553098201752, 0.16411377489566803, 0.007899194955825806, 0.3122623860836029, -0.055506523698568344, -0.06451370567083359, -0.020534774288535118, -0.04893849045038223, 0.06632854789495468, -0.004199789371341467, -0.23592893779277802, 0.32276201248168945, 0.012576373293995857, -0.2076220065355301, 0.058583881705999374, 0.30603861808776855, -0.04337889328598976, -0.2489299178123474, 0.10683717578649521, -0.058492355048656464, 0.1145215630531311, -0.2210695892572403, 0.08026392012834549, 0.12165618687868118, -0.018998950719833374, 0.035888805985450745, -0.0802541971206665, 0.031975358724594116, 0.09605912864208221, -0.21058878302574158, -0.13590969145298004, -0.13841763138771057, 0.10732892900705338, -0.0742657482624054, 0.10465874522924423, 0.06944798678159714, 0.035821449011564255, -0.045524634420871735, -0.23074379563331604, -0.1130756288766861, 0.06484773010015488, -0.03588851913809776, -0.20522081851959229, 0.056918706744909286, -0.2184867411851883, -0.0817091166973114, 0.08861175179481506, -0.09391713887453079, 0.13120503723621368, 0.05119508132338524, -0.10940105468034744, 0.008286779746413231, 0.2361699491739273, -0.30541008710861206, -0.01648147590458393, -0.06324528902769089, 0.029989486560225487, -0.24327430129051208, -0.07451658695936203, 0.10882768034934998, 0.05078161880373955, -0.024152860045433044, -0.0631936639547348, -0.03278650715947151, 0.0724576935172081, 0.04573633894324303, 0.02124219387769699, -0.03478725254535675, -0.08787082880735397, -0.19533051550388336, 0.19980312883853912, 0.023119395598769188, -0.042483601719141006, -0.06183450669050217, 0.09645334631204605, -0.09203818440437317, -0.2177867591381073, -0.11531937122344971, 0.08453365415334702, 0.10738492012023926, 0.009254555217921734, -0.03495611250400543, -0.1430574357509613, 0.03873947635293007, -0.014350500889122486, 0.031450238078832626, -0.1642022430896759, -0.008021427318453789, 0.1532946079969406, -0.12309698760509491, -0.014282438904047012, 0.07610797882080078, -0.14532417058944702, 0.04454208165407181, 0.16153085231781006, -0.2130259871482849, 0.13608641922473907, -0.03327520936727524, -0.005307906772941351, -0.01720879040658474, -0.16380910575389862, 0.04946735128760338, 0.09588149189949036, 0.10083615779876709, -0.16635075211524963, 0.031789861619472504, -0.03351523354649544, 0.189872607588768, 0.06342149525880814, -0.06915140151977539, 0.1360977739095688, 0.020886631682515144, 0.042266156524419785, -0.03542224317789078, 0.027968822047114372, 0.5110987424850464, 0.028923274949193, 0.09292667359113693, 0.09190405905246735, -0.02272825315594673, 0.10355804115533829, 0.19275614619255066, 0.30034303665161133, 0.0370403453707695, -0.020602235570549965, -0.000352058676071465, 0.07748603820800781, 0.1543470025062561, 0.2534668743610382, 0.06934566050767899, -0.07246121019124985, 0.03921699523925781, 0.07998157292604446, -0.05551861226558685, 0.024723943322896957, 0.08849959820508957, 0.02614607661962509, 0.11515634506940842, -0.06660197675228119, 0.04314481467008591, -0.13875316083431244, 0.015723882243037224, -0.14504751563072205, 0.05870726704597473, 0.010372472926974297, -0.039213553071022034, 0.3694887161254883, 0.07706684619188309, -0.03361150249838829, 0.0019427475053817034, 0.09419217705726624, -0.09174174815416336, 0.032527223229408264, 0.06831815093755722, -0.011733414605259895, -0.03717248886823654, 0.001945433672517538, 0.03380391001701355, 0.39965516328811646, 0.060608308762311935, 0.1928418129682541, 0.07616546750068665, 0.017758958041667938, -0.11787343770265579, 0.03778078779578209, -0.06413839757442474, -0.0011000342201441526, -0.3795202672481537, -0.38795900344848633, -0.011088059283792973, 0.14556720852851868, -0.1059289276599884, 0.01856858655810356, 0.06471152603626251, 0.03131939098238945, 0.008868701756000519, -0.0424080416560173, -0.0038469478022307158, -0.20645157992839813, -0.21355611085891724, -0.004093680065125227, -0.11723791062831879, 0.08087772876024246, 0.052002206444740295, 0.03447475656867027, -0.07945021986961365, -0.010364189743995667, -0.016664501279592514, -0.10607274621725082, -0.0454040989279747, -0.1512427181005478, -0.0013807000359520316, -0.0026335231959819794, -0.11388754099607468, 0.08341369032859802, -0.03807172551751137, 0.12961174547672272, -0.04914768785238266, 0.10970280319452286, 0.0516328439116478, 0.19582754373550415, 0.0306687131524086, -0.1449432671070099, -0.12021566182374954, -0.17513538897037506, -0.042465824633836746, -0.10769858211278915, -0.10268913209438324, -0.11013747751712799, -0.07751346379518509, -0.03334672003984451, -0.06064125522971153, 0.03244737535715103, -0.009991596452891827, 0.013343069702386856, -0.13605207204818726, -0.15861336886882782, -0.026368901133537292, 0.06867783516645432, 0.1186177134513855, 0.013349100016057491, -0.013764257542788982, -0.007090981584042311, 0.03418849781155586, 0.3105359375476837, -0.13786321878433228, -0.1333870142698288, -0.0836571455001831, 0.04761507734656334, -0.08406579494476318, 0.043076012283563614, -0.10172633081674576, -0.048542141914367676, -0.13292525708675385, -0.1385117918252945, 0.20298245549201965, -0.010137387551367283, -0.06755716353654861, -0.11295438557863235, -0.001782034058123827, -0.18866723775863647, -0.1392252892255783, 0.2349054515361786, -0.025345193222165108, 0.24854980409145355, 0.2765539884567261, -0.2092548906803131, 0.019672943279147148, -0.1052810475230217, -0.2008168250322342, -0.08558433502912521, 0.3368416726589203, -0.27445825934410095, -0.12975534796714783, -0.08296030014753342, -0.09500502049922943, -0.022713245823979378, 0.012240100651979446, -0.12063656002283096, -0.1793159693479538, -0.06616643816232681, 0.14939232170581818, -0.1875154674053192, -0.05102349445223808, 0.002198966918513179, -0.06311582773923874, -0.0031795441173017025, -0.050556402653455734, -0.047370415180921555, 0.10518601536750793, 0.05612482130527496, 0.2542387545108795, 0.17541426420211792, -0.1295052021741867, -0.17508652806282043, 0.13326837122440338, -0.07091445475816727, -0.08546484261751175, 0.09741795063018799, 0.059670187532901764, 0.0827929750084877, -0.07483215630054474, 0.010502306744456291, -0.10309901833534241, -0.0226444024592638, -0.08958475291728973, -0.014884349890053272, -0.18754243850708008, -0.08742322772741318, 0.052182041108608246, 0.09734083712100983, -0.07539347559213638, 0.029334193095564842, -0.10989771038293839, -0.024309277534484863, 0.2239447981119156, 0.031013092026114464, -0.03426631912589073, 0.05310969427227974, -0.09283152967691422, -0.04783608391880989, 0.020945703610777855, 0.0504937618970871, 0.057782843708992004, -0.04181108623743057, -0.06486422568559647, 0.11652015894651413, 0.27594953775405884, -0.016802283003926277, 0.019046833738684654, 0.054706674069166183, -0.04001579061150551, -0.07366128265857697, -0.17637717723846436, 0.050176024436950684, 0.03686569631099701, 0.05710973963141441, 0.024408701807260513, -0.0021019107662141323, -0.023081127554178238, -0.07196199893951416, -0.3074553608894348, 0.06745259463787079, 0.4669420123100281, -0.021613014861941338, -0.033062685281038284, -0.011833193711936474, -0.08686131983995438, 0.024182552471756935, 0.08790256828069687, 0.3307935893535614, -0.09503020346164703, 0.08929935842752457, -0.007288483437150717, -0.011866915039718151, -0.1742076575756073, -0.00735966581851244, 0.0661088228225708, 0.07149234414100647, 0.03481708839535713, 0.07649923115968704, -0.00757564464583993, -0.15950614213943481, 0.08252217620611191, 0.02887258306145668, -0.05725812166929245, -0.14091478288173676, -0.2602352201938629, 0.02296408824622631, -0.08992942422628403, 0.04283019155263901, -0.0043351370841264725, -0.04971068724989891, 0.02916126139461994, 0.05068721994757652, 0.04520285502076149, -0.29193803668022156, 0.09919209033250809, 0.10725246369838715, -0.047972094267606735, 0.14700116217136383, -0.07866290956735611, 0.2590354084968567, 0.06386759877204895, -0.03068358637392521, 0.14788959920406342, -0.04359747841954231, 0.05503614991903305, 0.04292604699730873, -0.07786746323108673, -0.19659560918807983, 0.05264229327440262, 0.08010043203830719, -0.050268400460481644, 0.023617133498191833, -0.06672406941652298, -0.01520496979355812, -0.001173732103779912, 0.15519209206104279, -0.04253154620528221, -0.03615210950374603, 0.06275922805070877, 0.009902949444949627, -0.109413743019104, -0.05875404551625252, -0.07119982689619064, -0.003246161388233304, 0.012581459246575832, -0.030433252453804016, -0.05947646498680115, 0.17477908730506897, 0.15486742556095123, -0.04410538077354431, -0.09620042145252228, 0.08604366332292557, -0.1669069528579712, 0.12141095846891403, -0.1433604508638382, -0.1752966195344925, 0.013464529998600483, 0.04122741520404816, -0.09939730912446976, -0.19817490875720978, 0.12820641696453094, 0.20831598341464996, -0.08338157832622528, -0.03928922861814499, -0.19259707629680634, 0.003772648051381111, -0.05663200095295906, -0.0038056394550949335, 0.062109630554914474, 0.006994106341153383, 0.012723241932690144, -0.11057354509830475, -0.10146056860685349, 0.33065176010131836, 0.024527011439204216, 0.06563445180654526, 0.14773012697696686, 0.05226658284664154, 0.2725357115268707, 0.0982658714056015, -0.034658659249544144, -0.10406631976366043, -0.11376085877418518, 0.03740382567048073, -0.022567827254533768, -0.06309487670660019, -0.02140192687511444, 0.15265069901943207, -0.00785135105252266, -0.062297433614730835, -0.043545935302972794, 0.08527129143476486, 0.11527207493782043, -0.0746883749961853, -0.059697166085243225, -0.022314108908176422, 0.11305633187294006, -0.1322985738515854, -0.2747851014137268, -0.05912334844470024, 0.01810433156788349, 0.11705438047647476, 0.084666907787323, -0.1383344680070877, -0.18206045031547546, -0.0841156467795372, 0.17737111449241638, 0.10053138434886932, 0.1490546613931656, -0.06597702205181122, 0.014794317074120045, -0.10134916752576828, -0.11761371791362762, 0.08142127096652985, 0.007109313737601042, -0.12398003786802292, -0.0959898829460144, -0.0017806553514674306, -0.12392373383045197, 0.043055202811956406, 0.10705368220806122, -0.022569263353943825, 0.12122278660535812, 0.004401975777000189, -0.12401372194290161, 0.1217801496386528, -0.05163097381591797, 0.09897234290838242, -0.0661366656422615, -0.0652046799659729, 0.12047012895345688, -0.206431582570076, -0.06588644534349442, 0.07491471618413925, -0.17893485724925995, -0.16899289190769196, -0.07723978161811829, 0.15974488854408264, -0.01935596950352192, 0.13761450350284576, -0.4766639471054077, 0.2633959949016571, 0.021954689174890518, 0.05504141375422478, -0.05132677033543587, 0.04909098893404007, -0.011543815955519676, -0.0928315743803978, -0.19993706047534943, -0.06283542513847351, 0.3287180960178375, 0.06548091024160385, 0.12625707685947418, -0.16381514072418213, -0.012218987569212914, 0.04532434418797493, 0.04043400660157204, -0.0938100591301918, 0.0071530588902533054, -0.14038586616516113, 0.2873939275741577, -0.04325738176703453, 0.032822560518980026, 0.027352504432201385, -0.06318821012973785, -0.13851475715637207, -0.21919776499271393, 0.010186005383729935, 0.0006537873996421695, 0.22579224407672882, 0.031459107995033264, -0.019406115636229515, 0.003933041822165251, 0.03439667075872421, 0.14501045644283295, 0.08806032687425613, -0.051944550126791, 0.02021835930645466, 0.010402193292975426, 0.010210278443992138, 0.007182024419307709, -0.020283136516809464, -0.03127184882760048, -0.055958349257707596, 0.23250064253807068, -0.059247903525829315, -0.17806899547576904, -0.014200090430676937, 0.042008232325315475, -0.009183068759739399, -0.04753784090280533, -0.1057400032877922, -0.10407251864671707, 0.006196680944412947, 0.10650934278964996, -0.04601340740919113, 0.059857163578271866, 0.006560663226991892, 0.17945703864097595, 0.0005542885046452284, 0.03627675399184227, 0.04212122783064842, 0.03978457301855087, -0.1467571258544922, -0.06453149020671844, -0.017389703541994095, -0.15657581388950348, -0.11471965909004211, -0.1172168105840683, 0.02072826586663723, 0.017209911718964577, 0.0011044644052162766, -0.1693228781223297, -0.0009001660509966314, 0.08364319801330566, -0.09211035072803497, 0.10974065214395523, -0.05355387553572655, 0.02049499936401844, 0.2625548243522644, -0.019856352359056473, -0.07299336045980453, 0.05671175941824913, -0.19036349654197693, 0.027070654556155205, 0.03983871266245842, -0.0020960867404937744, -0.016483014449477196, -0.06637967377901077, -0.12442629039287567, -0.07548803836107254, -0.07046613097190857, 0.09490609169006348, 0.06655611842870712, -0.09486662596464157, 0.13972699642181396, 0.06324204057455063, 0.011216693557798862, 0.13907472789287567, 0.14009030163288116, -0.15687790513038635, 0.030334321781992912, 0.13459697365760803, 0.06926779448986053, -0.1902015209197998, 0.03748511150479317, 0.2022683024406433, -0.11949573457241058, 0.2969874143600464, -0.04531543329358101, -0.2996984124183655, -0.06556124240159988, -0.10976988822221756, 0.24823999404907227, -0.14678184688091278, 0.34543848037719727, -0.04349668323993683, -0.07141054421663284, 0.16291821002960205, 0.010148183442652225, -0.26366227865219116, 0.054701268672943115, 0.0050706565380096436, 0.12015875428915024, -0.0910331979393959, 0.08015983551740646, 0.005210756324231625, 0.11072918772697449, 0.09229233115911484, 0.06713160872459412, -0.11416503041982651, -0.14932753145694733, 0.07076111435890198, -0.09931164234876633, -0.13459213078022003, 0.10244755446910858, -0.05698908865451813, 0.09463854879140854, 0.03428254649043083, 0.01788260042667389, 0.0518762581050396, -0.10156086832284927, -0.1286812573671341, 0.20751367509365082, 0.003461018903180957, -0.06061336770653725, 0.032995037734508514, -0.1778538078069687, -0.010416724719107151, 0.028039323166012764, 0.08415187150239944, -0.04702521860599518, -0.16514801979064941, -0.013942832127213478, -0.017399964854121208, -0.048098426312208176, 0.027521805837750435, -0.017628105357289314, 0.015717873349785805, 0.038566675037145615, -0.015443055890500546, -0.15065036714076996, 0.4563347101211548, -0.12055692821741104, -0.22140629589557648, -0.03293060511350632, -0.24150486290454865, 0.009037761949002743, 0.08618748933076859, -0.04591497778892517, -0.07813501358032227, 0.037156470119953156, 0.010084197856485844, -0.1823776364326477, -0.06654134392738342, 0.07408411055803299, -0.014072244055569172, -0.06821423768997192, -0.009589404799044132, 0.04896194115281105, -0.10019330680370331, -0.05658935755491257, -0.041533827781677246, 0.058425337076187134, 0.22343574464321136, -0.18486426770687103, -0.11938571184873581, -0.1903335452079773, 0.06224808469414711, 0.07204912602901459, -0.03641340509057045, -0.0737764909863472, -0.09939569979906082, 0.0016021357150748372, 0.08271011710166931, 0.02094956859946251, 0.016038453206419945, 0.21656887233257294, -0.08058276772499084, 0.08854400366544724, 0.09735564142465591, 0.06098547205328941, 0.15470589697360992, -0.12139823287725449, 0.1604946106672287, 0.07539436221122742, -0.028678657487034798, -0.1570473313331604, -0.015686627477407455, -0.16428619623184204, -0.1368750035762787, 0.11237907409667969, -0.19999873638153076, 0.05748284235596657, -0.07940029352903366, -0.20463499426841736, -0.1414090394973755, -0.11646861582994461, 0.10807815194129944, -0.1534658521413803, -0.08678114414215088, -0.12326955795288086, 0.11478161066770554, -0.10503967851400375, -0.23685967922210693, 0.09893863648176193, -0.022979339584708214, -0.20331519842147827, 0.012439511716365814, 0.15094150602817535, 0.026470886543393135, -0.03368720784783363, 0.16856758296489716, -0.0022371583618223667, -0.0007769361836835742, 0.04402162507176399, 0.09050826728343964, -0.009562553837895393, -0.04531004652380943, -0.07511884719133377, -0.1714116781949997, -0.05619337037205696, 0.06307382881641388, -0.06355699896812439, -0.024493079632520676, 0.042109712958335876, 0.060136131942272186, -0.05156974866986275, 0.0027812733314931393, -0.01481952890753746, -0.001112002064473927, 0.0296004768460989, 0.05446654185652733, 0.058175697922706604, 0.5800654888153076, 0.03520641103386879, -0.13684451580047607, 0.01644262485206127, -0.028324605897068977, 0.05141984298825264, 0.14380818605422974, -0.006928278133273125, 0.066100113093853, 0.042896147817373276, 0.16549162566661835, -0.037433553487062454, -0.02388327196240425, -0.04188138246536255, 0.08211853355169296, 0.01696581207215786, -0.012884806841611862, -0.13797485828399658, 0.10173902660608292, -0.17975308001041412, 0.2135019749403, 0.031716663390398026, 0.07892058044672012, -0.012737997807562351, -0.07299446314573288, 0.00028269278118386865, -0.010682206600904465, 0.008110343478620052, 0.12974797189235687, -0.009185905568301678, 0.03224858641624451, -0.06961210072040558, 0.12990185618400574, -0.03776819631457329, -0.05663836747407913, 0.10939911007881165, 0.04305998980998993, -0.07137396186590195, 0.0014732485869899392, -0.029680831357836723, 0.12059879302978516, -0.03364606946706772, -0.007833078503608704, -0.07590719312429428, 0.041389282792806625, -0.07310966402292252, 0.07462294399738312, -0.13634416460990906, -0.06927299499511719, 0.022235482931137085, -0.3906508982181549, 0.06396502256393433, 0.037924569100141525, -0.11425460129976273, -0.05282141640782356, 0.17928320169448853, 0.09980639815330505, 0.22472286224365234, 0.1297207772731781, -0.09170224517583847, 0.11889758706092834, -0.1935567408800125, 0.005091249477118254, 0.07819076627492905, 0.09264019131660461, -0.09328571707010269, -0.14908529818058014, 0.29152002930641174, -0.028140105307102203, 0.03635888174176216, -0.025909829884767532, 0.03230506181716919, -0.058220136910676956, 0.2180473804473877, -0.03238516300916672, -0.06122592091560364, -0.11990600824356079, -0.04102107137441635, -0.1403791904449463, -0.06844094395637512, 0.17996688187122345, 0.025737853720784187, 0.0016787871718406677, 0.13702252507209778, -0.05492599681019783, 0.15897975862026215, -0.09168913960456848, 0.044312264770269394, -0.03154243156313896, 0.0604955293238163, 0.11384645104408264, -0.3824165165424347, -0.043114352971315384, 0.050133973360061646, 0.11792676150798798, -0.10227639228105545, -0.008123315870761871, -0.06145269796252251, 0.0695054903626442, -0.06369037926197052, 0.08363313227891922, 0.13512727618217468, 0.05786217376589775, 0.05699814110994339, -0.20847800374031067, 0.3163162171840668, -0.00929043535143137, -0.1815381795167923, -0.13139252364635468, -0.33363255858421326, -0.22960242629051208, 0.013281638734042645, -0.03362010046839714, -0.04160744324326515, 0.0255388543009758, 0.0388832725584507, 0.02464905194938183, -0.2515626847743988, -0.018864445388317108, -0.03624117001891136, -0.10467197000980377, -0.06259587407112122, 0.03766690567135811, 0.01753307692706585, 0.07361851632595062, -0.07685871422290802, 0.05623512715101242, 0.08378364145755768, 0.019881900399923325, -0.07591048628091812, 0.007203994318842888, -0.1366896778345108, -0.07864692807197571, -0.09222158789634705, 0.12568679451942444, -0.03971456363797188, -0.04762367159128189, 0.1756037324666977, -0.1929495632648468, -0.0932469516992569, 0.017381830140948296, 0.010780418291687965, 0.0156825203448534, 0.17858409881591797, -0.1676037609577179, -0.26239216327667236, 0.04129906743764877, 0.12537333369255066, -0.20582391321659088, 0.08136893808841705, -0.16724355518817902, -0.04650773108005524, -0.04018380120396614, 0.00038626810419373214, -0.00585525669157505, 0.10803280770778656, -0.13664059340953827, -0.06560064852237701, 0.029425280168652534, 0.0012562598567456007, 0.04281887412071228, 0.11142799258232117, -0.12069691717624664, -0.055790599435567856, 1.817535303416662e-05, -0.06569576263427734, -0.09897533059120178, 0.1553717851638794, 0.00023824612435419112, -0.017115438356995583, 0.015418310649693012, -0.04633466154336929, 0.02984015643596649, -0.06656890362501144, 0.051931314170360565, -0.1363765001296997, -0.046436894685029984, -0.3730958104133606, 0.33659565448760986, 0.024111973121762276, -0.10652229934930801, -0.009781043976545334, -0.029616838321089745, -0.10401075333356857, 0.020451439544558525, -0.3631991446018219, -0.11230146884918213, -0.04450669512152672, -0.06658749282360077, -0.0018540211021900177, 0.21565361320972443, -0.11884663254022598, -0.09740327298641205, 0.10368843376636505, -0.1560087949037552, 0.01682489924132824, 0.0583711601793766, -0.06336437165737152, 0.2610732614994049, -0.06869113445281982, -0.021311400458216667, -0.076902836561203, 0.07684119790792465, -0.031682807952165604, -0.2963833212852478, 0.09218776971101761, -0.18363821506500244, 0.015970006585121155, 0.1592799872159958, 0.01113122422248125, 0.0810089260339737, -0.26540112495422363, 0.17862766981124878, -0.2619471549987793, -0.0054005892015993595, 0.09089798480272293, -0.11265100538730621, -0.05193950608372688, 0.31230607628822327, -0.08599656075239182, -0.11715394258499146, -0.09115035086870193, 0.014935757964849472, -0.016121497377753258, -0.0026469186414033175, -0.053474102169275284, -0.03676712512969971, -0.12555566430091858, -0.027614327147603035, 0.31645309925079346, -0.05099024251103401, 0.04588499665260315, -0.09968144446611404, 0.10420193523168564, -0.06903741508722305, 0.03396741300821304, -0.04313233494758606, 0.035279229283332825, -0.0024016776587814093, -0.019912168383598328, -0.06345643848180771, -0.15128006041049957, -0.04367147758603096, -0.032869938760995865, -0.10088285803794861, 0.05357573926448822, -0.030595887452363968, -0.09293308854103088, -0.14894798398017883, 0.05891783535480499, 0.0896606594324112, 0.1416902393102646, 0.25294795632362366, -0.08037493377923965, -0.04448873922228813, -0.018854372203350067, -0.029817719012498856, 0.019829921424388885, -0.16261719167232513, 0.09723402559757233, 0.17947323620319366, -0.3786892294883728, 0.11122329533100128, -0.007284651976078749, -0.17712436616420746, -0.17459873855113983, -0.03231392800807953, -0.15622520446777344, -0.17710024118423462, -0.07927212119102478, 0.034958623349666595, -0.03558220714330673, -0.5798512697219849, 0.5461057424545288, -0.029408248141407967, 0.12830939888954163, 0.09497907757759094, 0.08677438646554947, -0.3279363214969635, -0.10814385861158371, 0.013828192837536335, -0.027384039014577866, -0.16018863022327423, 0.004321347922086716, -0.07293318957090378, 0.09792853146791458, -0.02398999221622944, 0.14711283147335052, 0.02045333944261074, 0.05666573718190193, -0.24340878427028656, 0.18929645419120789, 0.08943501859903336, 0.0966891497373581, 0.022344723343849182, 0.24623264372348785, 0.08525222539901733, 0.08913440257310867, -0.055477987974882126, -0.1896907538175583, -0.1553339809179306, 0.16473478078842163, -0.010465595871210098, -0.011914165690541267, -0.12636545300483704, -0.3361895978450775, -0.012152277864515781, -0.007939816452562809, 0.053895510733127594, 0.09867696464061737, 0.06635130941867828, -0.047877758741378784, -0.12111219763755798, 0.009307065047323704, 0.30893442034721375, -0.19883036613464355, -0.14917071163654327, -0.10101307928562164, -0.06207262724637985, -0.013934542424976826, -0.021871063858270645, -0.0012475276598706841, 0.12153763324022293, 0.01948363147675991, 0.02384280599653721, -0.10807798057794571, 0.13476918637752533, 0.09481088072061539, 0.2555701434612274, 0.09848511964082718, -0.11295729130506516, 0.1508491486310959, 0.06973666697740555, -0.08519101142883301, -0.00905907154083252, 0.02247803658246994, -0.04921470582485199, 0.10792218893766403, 0.01426465529948473, 0.056823257356882095, -0.09405377507209778, 0.08242177218198776, -0.0430031456053257, -0.022586893290281296, 0.13205881416797638, -0.05771186947822571, -0.029235973954200745, -0.07511615008115768, 0.2471056431531906, -0.1394573450088501, 0.1494569182395935, -0.10897596180438995, 0.048727668821811676, 0.010463899001479149, 0.02837267331779003, -0.09433424472808838, 0.03982468694448471, 0.04060254618525505, -0.1552969515323639, 0.12247156351804733, -0.05004658177495003, 0.23286958038806915, -0.2529385983943939, 0.024541456252336502, 0.17630739510059357, -0.05951051041483879, -0.29775041341781616, -0.043652765452861786, 0.24069471657276154, -0.11399832367897034, 0.05827225372195244, -0.14805713295936584, 0.14730583131313324, -0.04860958084464073, -0.09872199594974518, 0.02647075057029724, -0.02568354830145836, 0.05473429337143898, -0.046860601752996445, -0.034841522574424744, 0.191466823220253, 0.1431388556957245, 0.08044882118701935, -0.09020070731639862, 0.07558562606573105, -0.030888322740793228, -0.13932864367961884, -0.1474868506193161, 0.060186177492141724, 0.3434444069862366, 0.06003020703792572, -0.0676509365439415, -0.10533539950847626, 0.015512285754084587, -0.06609947979450226, -0.14792437851428986, -0.0804450586438179, 0.27823591232299805, -0.04531217738986015, -0.03169230371713638, 0.08669502288103104, -0.10599535703659058, -0.05325211212038994, -0.08478879183530807, 0.012938814237713814, 0.19852344691753387, 0.012683816254138947, 0.1395767629146576, -0.12126046419143677, -0.04636409506201744, 0.12578700482845306, 0.2924354672431946, -0.09527131170034409, 0.29970431327819824, -0.030284948647022247, -0.05800580605864525, 0.005210525821894407, -0.04082750901579857, -0.015612614341080189, 0.1995980143547058, 0.08574748039245605, -0.034681860357522964, 0.05945014953613281, -0.20899389684200287, 0.11654208600521088, -0.012784166261553764, -0.06346286833286285, 0.26064684987068176, -0.16815540194511414, -0.10680167376995087, -0.10160519927740097, 0.06784118711948395, 0.09291583299636841, 0.23556551337242126, 0.08525513857603073, -0.048418015241622925, 0.02601500041782856, 0.09683957695960999, 0.09363260865211487, -0.1576075553894043, -0.06593203544616699, -0.1747095286846161, 0.13731501996517181, -0.10939338803291321, 0.06845933943986893, 0.05135326460003853, -0.18952403962612152, -0.051553089171648026, -0.02317926287651062, -0.014642058871686459, -0.11170358210802078, 0.1448398232460022, 0.05375444516539574, 0.09921322762966156, -0.2023172527551651, -0.07937783002853394, -0.020715292543172836, 0.01709149032831192, -0.1268787682056427, -0.012112081050872803, -0.09016778320074081, 0.041902270168066025, 0.022108037024736404, -0.12343619018793106, 0.09726264327764511, -0.04093952476978302, -0.11687935143709183, -0.001990594668313861, -0.03334391489624977, -0.07603123039007187, -0.14165684580802917, -0.010530877858400345, -0.1429489105939865, 0.004523031413555145, 0.04269249364733696, 0.01594822108745575, -0.09756512939929962, 0.01132600661367178, 0.08026355504989624, -0.12810081243515015, -0.10244543850421906, 0.03985656425356865, -0.04280983284115791, 0.18271604180335999, 0.1581072211265564, 0.10100088268518448, -0.010590926744043827, -0.1470005363225937, -0.09337730705738068, -0.12220863252878189, 0.1688324213027954, 0.08997984230518341, -0.004077543038874865, 0.032441895455121994, 0.22804971039295197, 0.15763302147388458, -0.20032384991645813, -0.01868172362446785, 0.03227021172642708, 0.03731154277920723, -0.06519707292318344, 0.08991660922765732, -0.06070557236671448, -0.03880182281136513, -0.005432565696537495, -0.061966005712747574, -0.04149346798658371, 0.04334728419780731, 0.008665775880217552, -0.0739140510559082, -0.14265285432338715, 0.1824110597372055, 0.056370850652456284, -0.04181951656937599, -0.002174857072532177, -0.012588771991431713, -0.40615129470825195, 0.025975599884986877, -0.03762953728437424, 0.05320311337709427, -0.15100279450416565, 0.03272387757897377, 0.04482245817780495, -0.10582838207483292, 0.06381872296333313, 0.04893074184656143, 0.2192355990409851, 0.057463061064481735, -0.22803360223770142, 0.1323021501302719, -0.07939973473548889, -0.31951016187667847, 0.24164581298828125, 0.07791795581579208, -0.1409277468919754, 0.03204907476902008, -0.2606050968170166, -0.1814097762107849, 0.054633498191833496, -0.03934427723288536, 0.0057016280479729176, 0.013017430901527405, 0.1231822818517685, 0.083570197224617, -0.2726173400878906, 0.06580638140439987, -0.16141828894615173, 0.06391241401433945, -0.17083288729190826, 0.07572688907384872, -0.14253613352775574, -0.006186089478433132, 0.07457666844129562, -0.04028567671775818, -0.013313133269548416, 0.1684769243001938, -0.07279682159423828, -0.1001395657658577, 0.06428945064544678, -0.05172213166952133, 0.0016210904577746987, -0.054369378834962845, -0.17938624322414398, -0.013600959442555904, 0.02019435167312622, 0.08072645217180252, 0.04637781158089638, -0.0585428923368454, -0.12294664233922958, -0.0448073111474514, 0.013766355812549591, -0.07152057439088821, 0.09758058935403824, -0.014747429639101028, 0.23030641674995422, 0.07708662003278732, 0.06897547841072083, 0.01431945338845253, -0.09310047328472137, 0.07958928495645523, -0.05541882663965225, 0.05054722726345062, 0.11653550714254379, 0.04597032070159912, -0.10014892369508743, 0.10198797285556793, 0.018081119284033775, 0.0923922136425972, -0.11135037988424301, -0.16796021163463593, -0.10949854552745819, 0.07161987572908401, -0.19198398292064667, 0.010056366212666035, -0.17574907839298248, -0.046558234840631485, -0.062217265367507935, 0.02118694595992565, -0.041816409677267075, 0.05542030557990074, -0.02472907118499279, -0.2218625545501709, -0.0608096607029438, -0.0755830779671669, 0.026344887912273407, 0.0032858632039278746, -0.00014353824371937662, 0.02532111667096615, -0.008980532176792622, -0.04541211947798729, -0.16414418816566467, -0.15801870822906494, -0.17559437453746796, -0.012761731632053852, -0.12811696529388428, -0.02941771410405636, -0.15088465809822083, -0.1184990406036377, -0.09972774237394333, -0.08619669079780579, 0.020580150187015533, 0.18059039115905762, 0.16271036863327026, 0.0418907105922699, -0.09932966530323029, 0.011438922956585884, -0.0220189169049263, 0.23357775807380676, -0.2263021618127823, -0.11126108467578888, -0.0744578167796135, -0.17037178575992584, -0.09810525923967361, -0.1397712677717209, -0.01891198195517063, -0.004994241055101156, -0.049613308161497116, 0.24274437129497528, 0.024299418553709984, 0.12555643916130066, 0.05759718269109726, -0.028406620025634766, 0.03909367695450783, 0.02761904150247574, -0.010883910581469536, -0.14521151781082153, -0.07529369741678238, -0.2568073868751526, -0.2628307044506073, -0.0946246013045311, -0.1664399802684784, 0.2799839973449707, -0.12138599157333374, -0.3275875151157379, -0.08436768501996994, -0.08335264772176743, 0.11847980320453644, -0.20358842611312866, -0.03032960370182991, -0.13281279802322388, 0.08806571364402771, -0.04181335121393204, 0.1266525834798813, -0.19653111696243286, -0.3103560209274292, -0.09607259184122086, 0.2396089881658554, -0.011659407056868076, -0.13263602554798126, -0.12015197426080704, 0.016694486141204834, 0.030953524634242058, 0.0375729575753212, 0.0434563010931015, -0.03621913492679596, -0.14511892199516296, 0.10518399626016617, -0.2277417629957199, 0.11922163516283035, -0.017859186977148056, -0.05469886586070061, 0.02853747271001339, -0.04157323017716408, -0.09032084047794342, -0.13500146567821503, 0.003270083339884877, -0.15119577944278717, -0.0555662102997303, 0.027358705177903175, -0.0021843891590833664, 0.03684993460774422, 0.05703103169798851, -0.056526146829128265, -0.10272417217493057, -0.009762720204889774, -0.06834566593170166, -0.10575178265571594, -0.016071336343884468, 0.12174370884895325, -0.06041176617145538, 0.20693883299827576, 0.10792726278305054, -0.043076615780591965, 0.005564451217651367, 0.008207173086702824, -0.11378825455904007, -0.024905724450945854, -0.009166448377072811, 0.07625142484903336, 0.04201563820242882, 0.001469730748794973, -0.04310043528676033, -0.0019563063979148865, 0.018265724182128906, -0.04455145448446274, -0.04348108544945717, -0.10249558836221695, 0.0504387691617012, 0.00840640440583229, 0.046870507299900055, -0.012812777422368526, 0.03365479037165642, -0.06637393683195114, -0.03456733748316765, 0.016790758818387985, 0.07073939591646194, 0.0012607281096279621, -0.0621134415268898, 0.07072246819734573, 0.0741465613245964, 0.07671710848808289, 0.011880132369697094, -0.09947802871465683, -0.060331910848617554, 0.2259184718132019, -0.025855178013443947, -0.03566258028149605, 0.14651274681091309, 0.07064832001924515, 0.032348476350307465, -0.04328452795743942, -0.06597822904586792, -0.0825563594698906, 0.01417460571974516, 0.031887151300907135, -0.11802895367145538, 0.04034942016005516, -0.003016962669789791, -0.1781657338142395, 0.018210526555776596, 0.013066653162240982, 0.10549560189247131, -0.08199422806501389, 0.06656716018915176, -0.023278990760445595, -0.046945970505476, 0.020257651805877686, 0.1994011402130127, 0.039566684514284134, -0.0933748260140419, -0.05317428708076477, 0.040495917201042175, 0.0568481907248497, -0.07584358751773834, -0.040300313383340836, -0.059829358011484146, 0.02897987700998783, -0.014071071520447731, 0.045574262738227844, 0.15033173561096191, -0.02964836359024048, 0.03830502927303314, -0.12198574841022491, 0.016241680830717087, -0.05862923711538315, 0.045909661799669266, 0.06671012938022614, 0.06067529320716858, -0.04314294829964638, -0.038589172065258026, -0.04389162361621857, -0.05594269558787346, -0.0289495550096035, -0.11258020251989365, -0.04670976102352142, -0.10429870337247849, -0.007003858685493469, 0.05245957896113396, -0.1431257277727127, -0.04697573930025101, 0.0550142303109169, 0.018098926171660423, -0.03095567785203457, -0.019804805517196655, 0.1424136459827423, -0.0247822105884552, 0.10519760847091675, 0.1426062434911728, -0.08254382014274597, -0.07395898550748825, -0.033751342445611954, 0.02662714570760727, -0.007793521508574486, 0.09313864260911942, 0.040496475994586945, -0.10175295919179916, 0.0958404466509819, -0.024953212589025497, -0.05254669487476349, -0.09280676394701004, -0.041145987808704376, -0.022403711453080177, -0.11767441779375076, -0.02421317994594574, -0.020797228440642357, -0.09121745079755783, 0.012018203735351562, -0.023368721827864647, -0.04256126284599304, -0.08591710776090622, 0.09700775146484375, -0.037308841943740845, -0.15027569234371185, -0.043895237147808075, 0.03090275637805462, -0.10729145258665085, 0.06189201399683952, 0.03089699149131775, 0.1646135449409485, 0.07091770321130753, -0.08532530814409256, 0.022873131558299065, 0.0022889019455760717, 0.14334169030189514, 0.05947353318333626, -0.019231872633099556, 0.09854287654161453, 0.013517170213162899, 0.007321714423596859, -0.19425737857818604, 0.028571177273988724, -0.048783622682094574, -0.07310526072978973, -0.03166715055704117, 0.06883891671895981, -0.0640265941619873, -0.05466965213418007, -0.13452167809009552, -0.06456614285707474, -0.057092539966106415, -0.04219534620642662, -0.05098413676023483, 0.02593890391290188, -0.03256932646036148, -0.09284452348947525, 0.014123518019914627, -0.10182345658540726, 0.026636289432644844, 0.047291308641433716, 0.03428269550204277, 0.13265855610370636, 0.03704410046339035, -0.012462731450796127, 0.011336171999573708, -0.006928649730980396, 0.01627914048731327, 0.022082731127738953, -0.02705664373934269, 0.10842892527580261, 0.09154026955366135, 0.037009358406066895, -0.09046948701143265, 0.06683512032032013, -0.011663352139294147, 0.01950198784470558, 0.02435838244855404, 0.11060649901628494, -0.14359332621097565, -0.05807221680879593, 0.051352329552173615, 0.08871003240346909, -0.16892926394939423, -0.022761397063732147, -0.12616050243377686, 0.15613622963428497, 0.1184171587228775, 0.04167221114039421, -0.1811220943927765, 0.0582360178232193, 0.08654186129570007, -0.1402091234922409, 0.016684547066688538, -0.006001371890306473, 0.034358859062194824, -0.04182659089565277, 0.09635971486568451, -0.02368122525513172, -0.001956804422661662, 0.07781974971294403, 0.03378657251596451, -0.05140171945095062, 0.02552609145641327, 0.128556489944458, -0.1373169720172882, 0.06638443470001221, 0.09339264035224915, -0.06224796175956726, 0.04180925711989403, -0.15064693987369537, -0.1168803721666336, 0.07010888308286667, 0.09788002818822861, -0.042760495096445084, 0.07268022000789642, 0.08739209175109863, 0.12972491979599, -0.08969898521900177, 0.07535175234079361, 0.08491407334804535, 0.02551983669400215, 0.011871776543557644, 0.025277959182858467, 0.12855952978134155, -0.053198739886283875, -0.14865227043628693, -0.005527593661099672, -0.0444348081946373, -0.1524178385734558, 0.023304639384150505, 0.014270789921283722, -0.13626860082149506, 0.00045540821156464517, -0.05240041762590408, 0.0035642061848193407, -0.0976826548576355, -0.023946335539221764, -0.041310667991638184, 0.014315485022962093, -0.011031320318579674, -0.04257446527481079, -0.15589255094528198, -0.004163343925029039, 0.048882804811000824, 0.06293606758117676, -0.11384309828281403, 0.014285529032349586, -0.05699056014418602, 0.02193351462483406, -0.023700790479779243, -0.10746385157108307, -0.011592629365622997, 0.13483311235904694, 0.07881007343530655, 0.01840212196111679, 0.04661361128091812, 0.012753591872751713, 0.16781741380691528, 0.027529513463377953, 0.1761791855096817, -0.04937106370925903, -0.04799569398164749, -0.053879085928201675, 0.11449947953224182, 0.05053666606545448, -0.030204711481928825, 0.024957643821835518, -0.008864957839250565, -0.002106214640662074, -0.04138527438044548, 0.012777145951986313, 0.009730172343552113, -0.0855078473687172, -0.06578627228736877, -0.05971036106348038, 0.06889063119888306, 0.030407739803195, -0.08863962441682816, 0.006645906250923872, -0.11644776910543442, 0.006970837712287903, 0.0306231826543808, -0.06514594703912735, -0.19834205508232117, 0.15287868678569794, 0.018890583887696266, 0.018555045127868652, 0.021814852952957153, 0.010216127149760723, -0.07820116728544235, 0.03247959911823273, 0.07379662245512009, -0.04606015235185623, -0.25655972957611084, 0.049813300371170044, -0.012722267769277096, -0.0024723054375499487, 0.03478160500526428, 0.03084629215300083, 0.05583029240369797, 0.002108694054186344, -0.04025198519229889, -0.04128272458910942, 0.012376957572996616, -0.03143318369984627, 0.12131015211343765, 0.07465764880180359, 0.08988257497549057, -0.013586494140326977, -0.011363841593265533, 0.05933484807610512, 0.019304456189274788, -0.021449631080031395, 0.05937866121530533, 0.1615702360868454, 0.0471651665866375, -0.026967529207468033, 0.028058070689439774, -0.040389303117990494, -0.15407904982566833, -0.03508917987346649, 0.009675644338130951, -0.009592359885573387, 0.04701997712254524, 0.06324297189712524, -0.05519190430641174, -0.024187838658690453, 0.09967747330665588, 0.14848364889621735, -0.03737042099237442, 0.07458671182394028, -0.026392443105578423, 0.015978001058101654, 0.05677666515111923, -0.05456707254052162, -0.044617149978876114, 0.07811947166919708, -0.012811741791665554, -0.014024701900780201, 0.11049647629261017, 0.026402205228805542, -0.10485654324293137, -0.046506911516189575, -0.00632943632081151, -0.06637793779373169, 0.02348884753882885, 0.023926198482513428, 0.012686151079833508, -0.04627931863069534, 0.04974652826786041, -0.0472392663359642, 0.03134199231863022, 0.019834231585264206, -0.002945526037365198, 0.05394243821501732, -0.017241884022951126, -0.018424656242132187, 0.01616559363901615, 0.08378653973340988, 0.04551912099123001, 0.0947113111615181, -0.2660057544708252, -0.14837314188480377, 0.022396288812160492, -0.016056003049016, 0.17638586461544037, -0.039813149720430374, 0.11689767986536026, -0.040194764733314514, 0.0680287629365921, -0.030955720692873, -0.034720826894044876, 0.08419226109981537, -0.058468692004680634, -0.03992592915892601, -0.01858438365161419, -0.017475031316280365, 0.046895578503608704, -0.06948687136173248, 0.16170497238636017, -0.0898313894867897, 0.05254926159977913, 0.05497963726520538, -0.031069768592715263, -0.04502006247639656, -0.07742679864168167, 0.01817207597196102, -0.00753629207611084, 0.03223789110779762, -0.021762602031230927, -0.062460657209157944, 0.053430069237947464, 0.28361624479293823, -0.17556552588939667, -0.011383173055946827, 0.07073138654232025, 0.06948679685592651, -0.059318918734788895, 0.059045907109975815, -0.02919694036245346, -0.07354239374399185, 0.024977480992674828, 0.011885251849889755, 0.09559788554906845, 0.03646310046315193, -0.07619933038949966, 0.07071098685264587, 0.016812939196825027, 0.06223384290933609, -0.09118376672267914, -0.026341019198298454, 0.0037113488651812077, 0.236020028591156, -0.030748002231121063, -0.08992372453212738, 0.09943259507417679, 0.12362919002771378, 0.03476937487721443, -0.16279228031635284, 0.03439674153923988, 0.030043086037039757, 0.02705947682261467, -0.023409875109791756, 0.012695955112576485, -0.0054346420802176, -0.025032147765159607, -0.02512074075639248, -0.08994252234697342, 0.08960826694965363, 0.053362634032964706, 0.12181219458580017, -0.008246039971709251, -0.018364831805229187, 0.04278592765331268, 0.026713591068983078, -0.07312731444835663, -0.01888061873614788, 0.11663202941417694, -0.06647902727127075, -0.01238397229462862, -0.06469396501779556, 0.08406265079975128, -0.11688823252916336, 0.11172617971897125, -0.010983610525727272, -0.04012570530176163, -0.02966124378144741, 0.044335607439279556, 0.06890202313661575, -0.06106274947524071, 0.0917048379778862, 0.10809702426195145, 0.040286608040332794, 0.010073607787489891, 0.05116698890924454, 0.14768657088279724, 0.003826957428827882, 0.05183827131986618, -0.00955942552536726, -0.03111986629664898, -0.04740912839770317, 0.11503191292285919, -0.08080361038446426, 0.03779030963778496, 0.0018750396557152271, -0.21287083625793457, -0.06456352770328522, 0.05952685698866844, 0.05856217443943024, -0.10765790194272995, 0.023170726373791695, 0.018939312547445297, 0.015362097881734371, 0.0003836852265521884, 0.18434938788414001, -0.12167688459157944, -0.05886955186724663, -0.14799869060516357, 0.08951891958713531, 0.1325637847185135, -0.06584257632493973, -0.03579075634479523, -0.05417517572641373, 0.046609293669462204, -0.04621356725692749, 0.00534342834725976, 0.12080574780702591, -0.08492139726877213, 0.028812719509005547, -0.12447088211774826, 0.07536084204912186, -0.024003224447369576, 0.05719837173819542, 0.02013307809829712, -0.0006300107925198972, 0.013508393429219723, 0.04672766849398613, 0.013002307154238224, 0.029011497274041176, 0.09845466911792755, 0.0049549369141459465, -0.009144661016762257, 0.004897878039628267, 0.013256894424557686, 0.08372318744659424, -0.013285767287015915, 0.004416473209857941, -0.08716698735952377, 0.04875711724162102, 0.06565834581851959, 0.04919832572340965, -0.01906511001288891, 0.046885423362255096, 0.023846881464123726, 0.0010198737727478147, 0.04517725855112076, -0.03372013941407204, 0.01062086969614029, -0.08604899048805237, 0.08215479552745819, -0.009137307293713093, 0.04693977162241936, 0.056615862995386124, 0.07396707683801651, 0.04088911786675453, 0.13570134341716766, 0.07312827557325363, 0.2506495416164398, 0.12816950678825378, 0.23549148440361023, 0.008374258875846863, -0.3909949064254761, 0.21213652193546295, 0.36621519923210144, -0.060555439442396164, 0.0006166099919937551, 0.06300685554742813, -0.10800441354513168, 0.03276800736784935, 0.1573350578546524, 0.07519491761922836, 0.053298983722925186, 0.10803667455911636, -0.1571657806634903, 0.34706181287765503, 0.12063784152269363, 0.15131734311580658, -0.021711938083171844, -0.10843349248170853, -0.06741689890623093, -0.35403138399124146, -0.13208940625190735, -0.23971673846244812, 0.09860879927873611, 0.051093894988298416, 0.036878801882267, -0.10494199395179749, 0.0025462948251515627, -0.10099413245916367, -0.022305777296423912, 0.000465129705844447, 0.038636282086372375, 0.009117537178099155, 0.0071242316626012325, -0.1264774650335312, -0.00018335638742428273, 0.0007761479937471449, -0.04522530734539032, -0.08254889398813248, 0.034818388521671295, 0.016152622178196907, 0.07283458858728409, -0.105751171708107, -0.018492519855499268, 0.01754373498260975, -0.045912135392427444, -0.07394423335790634, 0.17993347346782684, 0.05755597725510597, -0.1069154143333435, 0.026661712676286697, 0.0029420177452266216, 0.023894615471363068, 0.00937105342745781, 0.1046893447637558, 0.004950308706611395, 0.07424761354923248, -0.04812164977192879, 0.006099936552345753, -0.06969717890024185, -0.059342145919799805, -0.04744035378098488, -0.052921194583177567, 0.022338859736919403, -0.03399037569761276, -0.011761322617530823, -0.07282216846942902, 0.01292034424841404, -0.015195644460618496, -0.016190283000469208, -0.03170879930257797, 0.04132169485092163, 0.07925951480865479, 0.05988682433962822, -0.008809327147901058, -0.08012057095766068, -0.013562199659645557, 0.01402778085321188, 0.126139298081398, -0.07505090534687042, 0.03366214781999588, -0.02899404615163803, 0.09347406029701233, 0.005781966261565685, -0.05998251959681511, -0.025666896253824234, 0.24136734008789062, 0.08246950060129166, 0.14642778038978577, 0.012684805318713188, -0.019302362576127052, -0.031191697344183922, -0.007110514212399721, 0.10266289114952087, 0.075932078063488, 0.11113890260457993, 0.0844615176320076, -0.07803737372159958, -0.00022038666065782309, 0.11947932094335556, -0.004787549376487732, 0.019931485876441002, 0.05905819311738014, 0.015902116894721985, 0.05622633546590805, -0.01482828613370657, 0.033564742654561996, 0.051345065236091614, -0.03447549790143967, -0.008014590479433537, 0.06959089636802673, 0.12797829508781433, 0.015294582583010197, 0.02577693946659565, 0.07525212317705154, -3.7219448131509125e-05, 0.015660030767321587, 0.10085547715425491, 0.0029156517703086138, 0.03646520897746086, 0.014110601507127285, 0.029690712690353394, 0.039369143545627594, -0.07398510724306107, 0.004927674774080515, -0.030876662582159042, -0.05191115289926529, -0.051584821194410324, 0.0040560634806752205, -0.028415050357580185, -0.08234695345163345, -0.008277248591184616, 0.018907202407717705, -0.041844673454761505, 0.006037939805537462, 0.04133323207497597, -0.02030491828918457, 0.028169795870780945, 0.10711383819580078, 0.035283397883176804, -0.08073119074106216, 0.017871128395199776, 0.08887659758329391, 0.003829988883808255, 0.09658913314342499, -0.0070548290386796, 0.02412850223481655, 0.014659060165286064, 0.0070144846104085445, 0.0249553881585598, 0.02561470866203308, 0.005141288973391056, 0.02429463341832161, -0.02150905132293701, -0.08926606923341751, 0.08907445520162582, 0.013840973377227783, 0.006555045023560524, 0.014804958365857601, -0.10902810841798782, -0.09651767462491989, 0.020087245851755142, -0.09137801826000214, -0.05145881697535515, 0.00469941645860672, 0.02000301517546177, -0.008641943335533142, 0.0011649609077721834, -0.051771294325590134, -0.0003283894620835781, -0.14779870212078094, 0.03618280217051506, 0.04298734292387962, 0.16653071343898773, -0.04021429643034935, 0.07156694680452347, 0.036487095057964325, -0.04181164503097534, -0.0360138937830925, 0.01844479888677597, 0.247618168592453, -0.03035617060959339, 0.021725216880440712, -0.03671465441584587, 0.02952747233211994, -0.00183953030500561, -0.03485238179564476, -0.023133253678679466, -0.05418369919061661, -0.03167332336306572, 0.005154751241207123, 0.02700394205749035, -0.07504380494356155, 0.03789861127734184, -0.029277605935931206, -0.05383835360407829, -0.026208877563476562, -0.0340128093957901, -0.036781538277864456, 0.002875562757253647, -0.05192811042070389, -0.023260241374373436, -0.08347123861312866, -0.029539229348301888, 0.03349896892905235, 0.026961667463183403, 0.14732305705547333, -0.041287150233983994, 0.0029998417012393475, -0.014896007254719734, 0.031026290729641914, 0.006703187245875597, 0.10232044756412506, -0.022591136395931244, 0.11125639826059341, -0.11713067442178726, 0.012158758006989956, -0.023083584383130074, 0.0054221139289438725, -0.16103647649288177, -0.21642081439495087, -0.1917485147714615, 0.003624202683568001, 0.03823898360133171, 0.07076060026884079, 0.00960500631481409, -0.016788402572274208, -0.04329511523246765, -0.05512453615665436, -0.051043059676885605, -0.19907085597515106, 0.10799478739500046, 0.09047887474298477, -0.09715768694877625, 0.002788458950817585, -0.013214887119829655, -0.020606856793165207, -0.011164556257426739, -0.003247438697144389, 0.006614213809370995, 0.20430134236812592, 0.07734649628400803, -0.028362492099404335, -0.0037458837032318115, -0.15782780945301056, 0.028496429324150085, 0.08868084102869034, 0.08676024526357651, 0.016634777188301086, -0.009714649990200996, -0.16501180827617645, -0.04783162102103233, 0.13811443746089935, -0.048272889107465744, 0.15308904647827148, -0.16002513468265533, -0.24250370264053345, -0.058072302490472794, -0.0802784338593483, 0.05288975313305855, 0.12788714468479156, -0.07630710303783417, 0.09186704456806183, -0.23197157680988312, -0.09211850166320801, 0.12435610592365265, -0.04596969857811928, -0.14022643864154816, 0.07832921296358109, -0.014601439237594604, 0.07591914385557175, -0.1120310127735138, -0.10884848982095718, 0.021631019189953804, 0.04044358432292938, -0.06856672465801239, 0.04417245835065842, -0.06704264879226685, 0.014327064156532288, 0.15548956394195557, -0.05198628827929497, -0.019543662667274475, 0.0011582518927752972, 0.017161550000309944, -0.0035405100788921118, -0.07002624869346619, 0.006008343771100044, -0.07778528332710266, -0.10115234553813934, -0.01166246086359024, 0.09531929343938828, -0.013575433753430843, 0.006577933207154274, 0.02279382385313511, 0.07238686829805374, -0.09061834216117859, -0.14076614379882812, -0.04861893132328987, 0.1466071754693985, 0.004336806945502758, -0.06377210468053818, 0.02768750861287117, 0.08513542264699936, -0.018274495378136635, -0.13200902938842773, 0.03674863651394844, -0.005934501998126507, 0.04382097348570824, -0.007916199043393135, 0.008574624545872211, -0.021244805306196213, 0.006252380087971687, -0.023754596710205078, -0.022219672799110413, 0.06682778149843216, -0.056775715202093124, -0.07064683735370636, -0.03833048790693283, 0.010350359603762627, 0.020719893276691437, -0.02747797966003418, -0.02292703092098236, 0.028574194759130478, 0.048001792281866074, -0.007927730679512024, -0.08112740516662598, 0.024126360192894936, 0.02280566468834877, 0.1397939771413803, 0.03836198151111603, -0.009522887878119946, 0.08834584057331085, -0.04524201899766922, -0.0936964750289917, -0.05336209386587143, -0.03867313265800476, 0.012298752553761005, -0.06137802451848984, -0.04100929945707321, -0.026200152933597565, -0.0023123514838516712, -0.06046774983406067, -0.014872471801936626, 0.05414830893278122, 0.037190426141023636, -0.07247164100408554, -0.017545955255627632, 0.03614431247115135, -0.012096349149942398, -0.08153434097766876, -0.016676051542162895, 0.07106537371873856, 0.08098863810300827, -0.04503674432635307, -0.04451907053589821, 0.04199625924229622, -0.06280972808599472, -0.022308189421892166, 0.062305912375450134, -0.07386261224746704, -0.1297028809785843, 0.07770726829767227, -0.051173705607652664, 0.09918593615293503, 0.021146422252058983, -0.05681246519088745, 0.013729482889175415, -0.022367913275957108, -0.04003184661269188, -0.0014082358684390783, 0.03836845979094505, -0.059831652790308, -0.03395463898777962, -0.05631116405129433, -0.062013402581214905, 0.014655666425824165, 0.052525799721479416, -0.019143249839544296, 0.01902424544095993, -0.13668052852153778, -0.13098451495170593, 0.0567016527056694, -0.14690518379211426, -0.08964209258556366, 0.07965251058340073, 0.11057238280773163, -0.03390498086810112, -0.08904067426919937, -0.13473398983478546, -0.022015200927853584, -0.05086861550807953, -0.019824044778943062, -0.12016677111387253, -0.14110834896564484, -0.003925603348761797, 0.08501574397087097, -0.0943058505654335, 0.051728200167417526, 0.05857972428202629, 0.066552072763443, 0.0037499177269637585, -0.05953463912010193, 0.009755363687872887, 0.006220906972885132, -0.01758626662194729, 0.050447944551706314, 0.047402795404195786, 0.04062540829181671, 0.05561988055706024, -0.011611886322498322, -0.04170740768313408, 0.09038247168064117, -0.04408852756023407, -0.0060100252740085125, 0.06352502852678299, -0.12063018232584, -0.0900057926774025, 0.09439221024513245, 0.08705546706914902, -0.10163657367229462, -0.010790310800075531, -0.022469835355877876, -0.04295248165726662, 0.021070901304483414, 0.12086933851242065, -0.05539621040225029, -0.0473310649394989, -0.08429645001888275, 0.08260573446750641, 0.10184106230735779, -0.035601112991571426, -0.01122224610298872, 0.03951141610741615, 0.048516128212213516, -0.017061205580830574, 0.03859557583928108, 0.05522161349654198, -0.025062862783670425, 0.047195445746183395, -0.11021532118320465, 0.03610238432884216, -0.014639676548540592, -0.035865418612957, 0.07482001185417175, 0.08372613042593002, -0.10984525084495544, -0.08258678764104843, -0.1381530910730362, -0.06269468367099762, 0.039673320949077606, 0.004009830299764872, -0.11141811311244965, -0.0003659828507807106, 0.03239979222416878, 0.03500967472791672, -0.023404577746987343, -0.044432058930397034, -0.008298667147755623, -0.07499228417873383, -0.16561540961265564, -0.03811471536755562, 0.15258407592773438, 0.07893560826778412, -0.025347616523504257, -0.022326840087771416, -0.003469731891527772, -0.03872821852564812, 0.08435587584972382, -0.1118355467915535, 0.033451225608587265, 0.0946144163608551, -0.015114791691303253, -0.07568138092756271, -0.005970400758087635, 0.013114812783896923, -0.06842225044965744, -0.05965276435017586, -0.0799897238612175, -0.09046652913093567, 0.04579578712582588, 0.011676750145852566, 0.17037205398082733, -0.04911908507347107, -0.11244010180234909, -0.012429242953658104, -0.008628621697425842, -0.06010271608829498, 0.06086147949099541, -0.09370366483926773, 0.01105891726911068, 0.06651702523231506, 0.31482353806495667, 0.07366856187582016, -0.05927513912320137, 0.014412526041269302, -0.007043946068733931, 0.127049520611763, -0.02652885392308235, 0.01378125511109829, -0.04538265988230705, -0.08480355143547058, 0.10592877864837646, 0.06892695277929306, 0.11077713966369629, 0.029888568446040154, 8.845117554301396e-05, -0.10534638166427612, 0.013518447056412697, 0.009276901371777058, -0.03695761412382126, 0.01064499095082283, -0.009204338304698467, -0.058256760239601135, 0.013753226026892662, -0.11322944611310959, -0.011884212493896484, -0.08965175598859787, -0.013006065972149372, -0.045943573117256165, 0.03527852147817612, -0.05060993880033493, 0.08626870810985565, 0.06732366234064102, -0.013927151449024677, 0.1056893989443779, 0.03960055857896805, -0.0027247697580605745, 0.19898390769958496, 0.02533046528697014, -0.0018312109168618917, 0.0343414731323719, -0.0030435174703598022, 0.01656949892640114, 0.02997005358338356, 0.006603846326470375, 0.1092849001288414, 0.04409389570355415, 0.04234164208173752, -0.0030688289552927017, -0.011959785595536232, -0.011404440738260746, 0.035385407507419586, -0.025234347209334373, 0.0002887579903472215, 0.10862131416797638, 0.012918791733682156, 0.04128769412636757, -0.026770293712615967, 0.04285198450088501, 0.03582870960235596, 0.03454640880227089, -0.04884893819689751, 0.08358578383922577, 0.08964549005031586, 0.03528447449207306, 0.21701441705226898, -0.0934290736913681, -0.08266667276620865, 0.01597602292895317, 0.05061360448598862, 0.07715652137994766, -0.05824163928627968, 0.01427173800766468, -0.0015493165701627731, 0.0037245715502649546, -0.046234313398599625, -0.04257061704993248, 0.07606559991836548, 0.08404701948165894, -0.05658300593495369, 0.07288582623004913, -0.01257704570889473, -0.014580362476408482, -0.09625062346458435, -0.046362027525901794, -0.14859171211719513, -0.08470675349235535, -0.02599993720650673, -0.02276097610592842, 0.0012157600140199065, 0.019836537539958954, -0.0038882195949554443, 0.07007349282503128, -0.0406232513487339, -0.10781230032444, -0.059326548129320145, 0.11314266175031662, -0.15418945252895355, 0.11240044981241226, 0.10254836082458496, 0.03419141471385956, -0.04000242054462433, -0.015007893554866314, 0.08161119371652603, -0.08000420778989792, -0.004066554829478264, 0.038813795894384384, -0.002286482835188508, 0.15113429725170135, 0.045587074011564255, 0.07918863743543625, 0.03560197725892067, -0.05637528374791145, 0.1432885378599167, -0.07567933201789856, 0.06735953688621521, -0.028542300686240196, -0.012472419068217278, -0.06309394538402557, 0.09472692757844925, 0.10504569113254547, -0.12936566770076752, -0.024863842874765396, 0.003553731832653284, -0.004976784344762564, 0.02300563082098961, 0.04597163200378418, -0.1188674122095108, -0.0371999628841877, -0.07366485148668289, 0.04196916148066521, 0.06339462101459503, -0.10940349847078323, -0.10265219211578369, -0.03068799339234829, 0.0012416550889611244, -0.005823186598718166, -0.019395658746361732, 0.10504372417926788, 0.03265557438135147, 0.11401018500328064, -0.1012597307562828, -0.020990366116166115, -0.0034181117080152035, -0.015627657994627953, 0.019960472360253334, 0.08373226970434189, 0.10301350057125092, 0.09116528183221817, 0.04201284050941467, -0.015520229004323483, -0.11879093945026398, 0.08299220353364944, 0.17125298082828522, -0.00834215059876442, 0.017344152554869652, 0.0827280655503273, -0.11469738930463791, -0.07299749553203583, 0.03861508518457413, 0.047824468463659286, -0.013402332551777363, 0.019492443650960922, 0.005209987983107567, 0.13960272073745728, -0.04264470562338829, -0.026476217433810234, 0.02030821144580841, -0.08108795434236526, -0.010674617253243923, -0.11112233251333237, -0.007521620020270348, -0.09819507598876953, 0.009172014892101288, -0.05939776450395584, -0.02767268382012844, 0.04831276088953018, -0.06636461615562439, 0.010185201652348042, -0.13877838850021362, -0.02265070751309395, -0.038445644080638885, 0.09552747756242752, 0.11409962177276611, 0.08298327773809433, -0.05974775552749634, 0.021364612504839897, -0.0005693943239748478, 0.06861286610364914, -0.058045390993356705, -0.14575907588005066, 0.026460712775588036, 0.03395237773656845, 0.15265236794948578, 0.06447970122098923, -0.03630957007408142, -0.05520040541887283, -0.00470937741920352, 0.1012396439909935, 0.039352912455797195, -0.08699585497379303, 0.008193248882889748, -0.013529402203857899, 0.06987496465444565, 0.2313564568758011, -0.0756567120552063, -0.025344261899590492, -0.017539171501994133, -0.10009312629699707, 0.12769043445587158, -0.03576791286468506, 0.015509909950196743, 0.02719942107796669, -0.17595656216144562, -0.05604209378361702, 0.04520651698112488, 0.1640234887599945, -0.07945649325847626, 0.013986840844154358, 0.001275735441595316, 0.0016920717898756266, 0.01705234870314598, 0.17426329851150513, -0.05283518508076668, -0.005146613344550133, -0.12285519391298294, 0.07054351270198822, 0.09780160337686539, -0.05180739611387253, -0.08378916233778, -0.04424167796969414, -0.011274791322648525, -0.04015427455306053, -0.06921884417533875, 0.1458023339509964, -0.07488389313220978, 0.035743553191423416, -0.05070064216852188, 0.020646516233682632, -0.03878813982009888, 0.020417748019099236, 0.12295237928628922, 0.07628943771123886, 0.024419894441962242, -0.02383713610470295, 0.0592220276594162, -0.05075791850686073, -0.12285268306732178, 0.0022059273906052113, 0.11693058907985687, -0.057114727795124054, -0.03742861747741699, 0.03540167957544327, -0.10495148599147797, -0.11312871426343918, 0.06839390844106674, -0.013310914859175682, -0.054981932044029236, -0.03777381777763367, -0.04036293178796768, 0.0564369298517704, 0.0067128390073776245, 0.006626491900533438, -0.009988158009946346, -0.05570421367883682, -0.050456371158361435, -0.06046328693628311, -0.08494202792644501, -0.09462055563926697, 0.08288159966468811, -0.03976166993379593, 0.029879434034228325, -0.008572426624596119, -0.048437487334012985, 0.004346803296357393, 0.004771508742123842, 0.018230382353067398, -0.14135703444480896, -0.04208168014883995, 0.009778589010238647, 0.04947106912732124, -0.03724173083901405, -0.04012921079993248, -0.014712106436491013, 0.011536505073308945, -0.013348208740353584, -0.19310012459754944, -0.042796790599823, -0.10339202731847763, 0.018537264317274094, 0.043293170630931854, 0.12207811325788498, -0.022556183859705925, -0.04652540013194084, 0.13630850613117218, 0.0010388022055849433, -0.05649811029434204, 0.028773291036486626, 0.20556451380252838, 0.04712187498807907, 0.14152787625789642, -0.06928466260433197, 0.019072677940130234, 0.04155842587351799, -0.18440352380275726, 0.12014632672071457, -0.0770796462893486, -0.06516712158918381, -0.030118925496935844, -0.24090087413787842, -0.10037928819656372, 0.10347110033035278, 0.046261016279459, -0.11451297253370285, 0.08618702739477158, 0.034427136182785034, 0.10246530175209045, -0.04199959710240364, 0.12611764669418335, -0.030662596225738525, 0.0006600713240914047, -0.16605770587921143, 0.07230816036462784, 0.12444967776536942, 0.013988617807626724, 0.045318763703107834, -0.02868899516761303, 0.053081121295690536, -0.03099251724779606, 0.020367339253425598, 0.0899476706981659, -0.08656652271747589, 0.0633135512471199, -0.015258505009114742, 0.025790542364120483, -0.08315160870552063, -0.036942530423402786, -0.10129348933696747, 0.03179040178656578, 0.04379722848534584, -0.056681375950574875, -0.01397128589451313, -0.014563909731805325, 0.04066702350974083, 0.028381910175085068, -0.1096670851111412, 0.13042838871479034, 0.05526650324463844, 0.13613201677799225, -0.11735701560974121, -0.07822603732347488, 0.07960028201341629, 0.10191814601421356, -0.03710174560546875, -0.10554961115121841, 0.08577948808670044, 0.08323052525520325, 0.01118867564946413, -0.02174154482781887, -0.051271744072437286, -0.12939149141311646, 0.00924005638808012, -0.05916912481188774, -0.06044134125113487, 0.09284739941358566, -0.04947144165635109, -0.0062010809779167175, -0.1260945200920105, 0.03168961778283119, 0.06248786300420761, 0.022606495767831802, -0.09045064449310303, 0.05097091197967529, -0.12703344225883484, -0.05598367378115654, 0.05805398151278496, -0.021527213975787163, -0.053532883524894714, 0.025372326374053955, 0.025126295164227486, 0.0747092217206955, -0.08010555803775787, -0.028285788372159004, -0.050967469811439514, -0.14898614585399628, -0.1259913444519043, 0.0992807000875473, 0.15756060183048248, -0.012141118757426739, 0.03556400164961815, 0.037739694118499756, 0.008857881650328636, -0.18515467643737793, 0.03055463917553425, 0.1364666223526001, -0.07150956988334656, 0.03348368778824806, 0.016696495935320854, -0.07265159487724304, -0.0662432461977005, -0.06411431729793549, -0.12186083197593689, 0.10283225029706955, -0.04019883647561073, 0.08032133430242538, -0.045300330966711044, -0.04983887821435928, -0.03283001855015755, 0.0583585686981678, 0.12555429339408875, -0.09207870066165924, -0.010336731560528278, 0.042681992053985596, 0.007135248277336359, 0.0478992685675621, 0.2461615353822708, -0.2467227578163147, -0.09286171942949295, -0.16531983017921448, 0.1492365598678589, 0.40794649720191956, -0.2338637411594391, 0.1096605658531189, -0.008082078769803047, -0.04364520683884621, 0.051683735102415085, 0.2707604467868805, 0.04840219393372536, 0.08824047446250916, 0.11940208077430725, 0.1526443511247635, 0.0800112932920456, -0.14154444634914398, 0.1881718635559082, 0.23382939398288727, 0.18479548394680023, 0.20667336881160736, 0.16348715126514435, -0.018569327890872955, -0.317003458738327, 0.3047018349170685, 0.39181870222091675, 0.027695804834365845, 0.009670015424489975, 0.06738562881946564, -0.0662764385342598, 0.02630443312227726, 0.2860633134841919, 0.009846334345638752, -0.03683452308177948, 0.16754035651683807, -0.04598301649093628, 0.28940486907958984, 0.14453193545341492, 0.2665891945362091, 0.048744555562734604, -0.23827387392520905, -0.012792523019015789, -0.4272480010986328, -0.05844362825155258, -0.24127061665058136, 0.2193225920200348, 0.09374050796031952, -0.02674342878162861, -0.17687852680683136, 0.11129851639270782, -0.06727945059537888, -0.11882010847330093, -0.005110940430313349, -0.10550151020288467, -0.07780405879020691, 0.14641191065311432, 0.2978176772594452, -0.14095951616764069, -0.04630468785762787, 0.014313949272036552, -0.007634974084794521, 0.02452637255191803, 0.04953714460134506, 0.017927320674061775, -0.08451167494058609, 0.037369199097156525, 0.117136649787426, 0.03507007658481598, -0.03067607432603836, -0.044663749635219574, 0.03028395026922226, -0.031945694237947464, -0.025488121435046196, 0.04993807524442673, -0.10189990699291229, -0.02562408335506916, 0.015170138329267502, -0.08719195425510406, 0.034182775765657425, 0.03297089412808418, -0.08663507550954819, -0.0890483483672142, -0.010785119608044624, -0.010560368187725544, -0.024822885170578957, -0.002964634681120515, 0.044418711215257645, 0.10714294016361237, 0.061524368822574615, -0.16942645609378815, 0.02777201496064663, 0.038104306906461716, 0.013577540405094624, -0.008724723942577839, 0.07570547610521317, -0.050688955932855606, -0.09533026069402695, 0.01603642851114273, -0.05660014972090721, -0.016179410740733147, -5.2951516408938915e-05, -0.2029043436050415, -0.09715808928012848, 0.017354613170027733, 0.0186338908970356, 0.06035700812935829, -0.028781550005078316, 0.07521393895149231, 0.10429619997739792, -0.08695810288190842, 0.03021911531686783, -0.015449726954102516, 0.07318303734064102, -0.1814737319946289, 0.06113358587026596, -0.08249681442975998, -0.07937975227832794, -0.07959494739770889, 0.1266235113143921, -0.0345483273267746, 0.1376141458749771, -0.022074265405535698, 0.024639667943120003, 0.003006847808137536, 0.07046372443437576, -0.01105515006929636, -0.11803894490003586, -0.061170466244220734, -0.028013641014695168, 0.116319440305233, -0.10728108137845993, 0.03845598176121712, 0.04720903933048248, -0.07097171992063522, 0.05326399579644203, 0.01618417538702488, -0.09564954787492752, 0.039260488003492355, -0.005385173950344324, 0.03219166398048401, 0.02460695244371891, 0.009355482645332813, -0.046183083206415176, -0.03258200362324715, 0.0419132374227047, 0.20524020493030548, -4.310946314944886e-05, -0.06093413010239601, 0.10097818821668625, 0.0017015376361086965, -0.21101315319538116, 0.15191365778446198, -0.021617010235786438, -0.10631640255451202, -0.01474724430590868, -0.008419585414230824, -0.03484473004937172, -0.03145131841301918, -0.042779210954904556, 0.03078123927116394, 0.16834264993667603, -0.15426519513130188, 0.00963660329580307, 0.11893689632415771, -0.04733635485172272, -0.02196113020181656, -0.03104379028081894, 0.03660757839679718, -0.09721577912569046, -0.005371687468141317, -0.03717717155814171, 0.029212474822998047, -0.05609942600131035, -0.05787601321935654, 0.0036149919033050537, 0.04774872213602066, -0.01065562479197979, 0.08017168939113617, -0.011521529406309128, 0.051214799284935, 0.04335251450538635, -0.013965207152068615, -0.03565763682126999, -0.08323109894990921, 0.08885767310857773, -0.008243533782660961, 0.06744565814733505, -0.0008852580795064569, -0.033053312450647354, 0.04766809567809105, 0.0004054473538417369, -0.0734877809882164, 0.03259868174791336, 0.17394383251667023, -0.02806807868182659, -0.03663633018732071, 0.028074361383914948, -0.08347984403371811, -0.07744519412517548, 0.03545190766453743, -0.019382068887352943, -0.019391119480133057, -0.04635905101895332, 0.015292241238057613, -0.0013122499221935868, -0.00995792169123888, -0.007981727831065655, -0.07644233852624893, 0.11781200021505356, 0.21150319278240204, 0.04808120056986809, 0.42062246799468994, 0.17980526387691498, 0.14718246459960938, 0.14197741448879242, -0.34738364815711975, 0.07917362451553345, 0.4295678734779358, 0.04575897753238678, -0.0006339792744256556, 0.040682677179574966, -0.05767584592103958, 0.005303435958921909, 0.2732883393764496, -0.05472024157643318, 0.001524879364296794, 0.03210333734750748, -0.04830268770456314, 0.3944551944732666, -0.07801824808120728, 0.33112961053848267, 0.022196251899003983, -0.08579584211111069, -0.04200625419616699, 0.03406547009944916, -0.0887732207775116, -0.42171788215637207, 0.16520309448242188, 0.15989139676094055, -0.06132744252681732, -0.247758686542511, 0.020367437973618507, 0.04844217002391815, -0.08269727230072021, -0.06461454182863235, -0.08204346150159836, -0.06150805950164795, -0.037786033004522324, -0.016505014151334763, -0.03995087742805481, 0.049974627792835236, 0.028463689610362053, 0.046973105520009995, -0.1009279191493988, -0.08733434230089188, -0.017300371080636978, 0.11804885417222977, -0.044664107263088226, -0.06018853187561035, 0.09431304782629013, 0.074220210313797, -0.16901415586471558, -0.06039508432149887, 0.004314173012971878, -0.004410002380609512, 0.06838193535804749, 0.18225671350955963, -0.038814518600702286, 0.02007867768406868, -0.09850918501615524, -0.07958396524190903, -0.05134967342019081, 0.045222822576761246, 0.003502147039398551, 0.10227509588003159, 0.20503535866737366, 0.1839708685874939, 0.2620939612388611, 0.24653297662734985, -0.4150110185146332, 0.41190120577812195, 0.520535409450531, 0.02366621233522892, -0.03454937785863876, 0.15506669878959656, -0.16754092276096344, 0.0554254949092865, 0.21752724051475525, 0.03804284706711769, 0.12152416259050369, 0.10026481747627258, -0.08225734531879425, 0.49932464957237244, -0.035841841250658035, 0.19477719068527222, 0.12069680541753769, -0.07224670797586441, -0.06380891799926758, -0.16391964256763458, 0.006712404079735279, -0.38410109281539917, 0.1382216513156891, 0.123792864382267, -0.0030175382271409035, -0.15456801652908325, 0.0432131290435791, -0.08842349052429199, -0.037200745195150375, -0.07098230719566345, 0.0018186590168625116, -0.12075307220220566, 0.05904022976756096, 0.07387739419937134, -0.05365109443664551, -0.04261411726474762, -0.02095247432589531, 0.010341677814722061, 0.02312706969678402, -0.1697942316532135, -0.056435104459524155, 0.3178757131099701, -0.03084450215101242, -0.022907303646206856, 0.06792350858449936, 0.015050948597490788, -0.03672489896416664, 0.023258550092577934, -0.03871021792292595, 0.007775542791932821, -0.034878987818956375, -0.09335386008024216, -0.03826074302196503, 0.022879676893353462, -0.08515064418315887, -0.10800021886825562, -0.006332742050290108, 0.00705763278529048, -0.07986253499984741, 0.10744058340787888, -0.026677783578634262, -0.05011487007141113, -0.023655394092202187, 0.057317543774843216, 0.08225028961896896, -0.031723782420158386, -0.08430953323841095, -0.10089660435914993, 0.008804415352642536, 0.08104649931192398, -0.21069952845573425, -0.022863028571009636, -0.08542925864458084, 0.1244228407740593, 0.0062364195473492146, -0.07072573155164719, 0.035157106816768646, 0.09975707530975342, 0.009886241517961025, -0.11974591761827469, -0.06052813678979874, -0.0762496367096901, 0.03577118739485741, -0.03791523352265358, -0.008165087550878525, 0.11952269822359085, -0.007477113977074623, -0.06672202050685883, -0.004312081728130579, 0.09856997430324554, -0.04724833741784096, -0.03576478362083435, -0.03620566427707672, -0.030153213068842888, -0.14076852798461914, -0.06169770285487175, 0.01813463866710663, -0.09648821502923965, -0.06557988375425339, -0.02463170886039734, 0.007374846376478672, 0.09423408657312393, -0.08129902929067612, -0.17542047798633575, -0.0941489189863205, -0.03193480148911476, -0.044977348297834396, 0.008695922791957855, 0.1468406468629837, 0.05655878037214279, 0.1940165013074875, 0.1593226045370102, -0.06248530000448227, -0.09457520395517349, 0.02503052167594433, 0.08360518515110016, 0.00246539618819952, 0.017492739483714104, 0.026254042983055115, -0.004964066203683615, 0.011029175482690334, -0.13468822836875916, -0.035378582775592804, -0.007069415412843227, -0.07790743559598923, -0.07923807948827744, -0.04651526361703873, -0.044060271233320236, 0.021595459431409836, 0.08564671128988266, -0.05679862201213837, 0.03547628968954086, 0.017528070136904716, 0.015281397849321365, 0.030011245980858803, -0.04088849946856499, -0.3254859149456024, 0.170314759016037, 0.030430210754275322, -0.04811922460794449, 0.032957497984170914, 0.06453660130500793, -0.008737821131944656, 0.012404532171785831, -0.1380278617143631, -0.00043048293446190655, 0.05022008717060089, -0.005740444175899029, -0.05243321880698204, 0.02248476818203926, 0.048499852418899536, -0.03340478986501694, -0.04799903184175491, -0.01172679290175438, -0.07078242301940918, -0.0054763988591730595, 0.021132223308086395, -0.05403687059879303, -0.03452039882540703, 0.04055328294634819, 0.01658528670668602, 0.0579388290643692, -0.0652504712343216, 0.03734929487109184, 0.04788917675614357, 0.04078136384487152, 0.02226562611758709, 0.050587523728609085, 0.10565080493688583, 0.13068288564682007, 0.0199566837400198, -0.1232520267367363, 0.045608460903167725, -0.0009324136772193015, -0.06577293574810028, -0.20319871604442596, 0.012090632691979408, 0.046156276017427444, 0.03756984323263168, -0.018289094790816307, -0.002280652290210128, 0.038173090666532516, -0.06355294585227966, -0.060295600444078445, -0.057669006288051605, 0.14902959764003754, 0.016599729657173157, -0.048068705946207047, 0.0005012884503230453, 0.019552571699023247, 0.13746751844882965, -0.12736071646213531, 0.12198645621538162, 0.006669219117611647, -0.0879465788602829, -0.058274272829294205, -0.021189942955970764, -0.04945468530058861, 0.06685078889131546, 0.13328908383846283, 0.04224950447678566, 0.03349665552377701, -0.06195710226893425, 0.02517208456993103, 0.047728318721055984, 0.1189078688621521, -0.017425334081053734, 0.0587548092007637, -0.0017743903445079923, -0.05122644826769829, -0.0029958889354020357, -0.12708640098571777, 0.0338447168469429, 0.10040906816720963, -0.022297324612736702, -0.05283321812748909, 0.04823129624128342, -0.07616347074508667, 0.011725468561053276, 0.11696846038103104, 0.03420683369040489, -0.01891104318201542, 0.08960028737783432, 0.016170993447303772, -0.02945665828883648, -0.017253221943974495, -0.01604238525032997, 0.010675507597625256, 0.05150023475289345, -0.0206211619079113, 0.025578446686267853, -0.04389360547065735, 0.19639822840690613, 0.04360785707831383, -0.05793934315443039, -0.08327214419841766, -0.04137936979532242, 0.019895747303962708, -0.08324563503265381, 0.019334308803081512, 0.00014803784142713994, -0.04986513406038284, -0.021740619093179703, -0.07042555510997772, -0.021647699177265167, -0.0562538281083107, 0.027431519702076912, -0.10511014610528946, -0.032728493213653564, -0.06888581812381744, -0.025492947548627853, -0.04089943692088127, -0.002112593734636903, -0.10571321845054626, -0.05184510722756386, -0.001496551325544715, 0.02650056965649128, -0.06650222837924957, -0.06709759682416916, -0.017821598798036575, 0.020075326785445213, -0.025242960080504417, 0.023290107026696205, -0.09320494532585144, 0.02486838959157467, 0.20412452518939972, -0.0249243825674057, -0.019089672714471817, 0.04772001877427101, -0.06248965859413147, -0.05429398640990257, 0.06549316644668579, -0.022012954577803612, 0.0728159099817276, 0.003195172641426325, -0.03232402727007866, 0.05554135516285896, 0.2167768031358719, -0.05123596265912056, -0.08142539113759995, 0.02145317569375038, -0.06186346709728241, -0.1014668345451355, -0.01805821806192398, -0.034051842987537384, -0.07872197031974792, -0.028151480481028557, 0.06438350677490234, 0.125448539853096, -0.025666221976280212, -0.02946118637919426, 0.0006308745942078531, -0.010308546014130116, -0.04232248663902283, 0.03572377189993858, -0.10345649719238281, -0.04539157822728157, -0.13456904888153076, -0.08745072036981583, 0.06624414026737213, -0.04110124707221985, -0.0025719476398080587, -0.07290978729724884, -0.02345944754779339, 0.04849318787455559, -0.03473051264882088, 0.002975386567413807, 0.22033050656318665, -0.019727300852537155, 0.004591731820255518, -0.04228481277823448, 0.20027530193328857, -0.0214891005307436, -0.07840397208929062};

const float bias_raw[32]={0.30490046739578247, 0.42458513379096985, 0.5748352408409119, 0.009756925515830517, 0.8325185179710388, 0.423364520072937, -0.535932183265686, 0.3959062695503235, 0.9680278897285461, -0.08847866952419281, 0.4691682457923889, -0.12076983600854874, 0.3557964265346527, 0.3365042507648468, 0.44952312111854553, 0.1056288629770279, 0.7119337916374207, 0.554286539554596, -0.7598451972007751, 0.2866745889186859, 0.5228756666183472, 0.25028669834136963, 0.436085045337677, -0.2192278653383255, 0.16099804639816284, 0.09569856524467468, 0.2906353771686554, -0.43109941482543945, 0.08707807958126068, 0.07917040586471558, 0.36638131737709045, 0.00036906631430611014};

const int stride_width=1;
const int stride_height=1;
const TfLiteFusedActivation activation=kTfLiteActRelu;
const int dilation_width_factor=1;
const int dilation_height_factor=1;
const int32_t filter_input_channel=128;
const int32_t filter_output_channel=32;
const int filter_height=1;
const int filter_width=1;
const int filter_dims_size=4;
const int32_t filter_dims_raw[4]={32,1,1,128};
const int bias_dims_size=1;
const int32_t bias_dims_raw[1]={32};
const TfLitePadding paddings=kTfLitePaddingSame;
const TfLiteType filter_type=kTfLiteFloat32;
const bool data_supports_multithreaded_kernel=true;

struct OpData {
  // IDs are the arbitrary identifiers used by TF Lite to identify and access
  // memory buffers.
  int im2col_id = kTensorNotAllocated;
  int hwcn_weights_id = kTensorNotAllocated;
  int input_quantized_id = kTensorNotAllocated;
  int scaling_factors_id = kTensorNotAllocated;
  int input_offset_id = kTensorNotAllocated;
  int accum_scratch_id = kTensorNotAllocated;
  // Row sums are used to cache filter sums for hybrid zero-point calculations.
  int row_sums_id = kTensorNotAllocated;

  TfLitePaddingValues padding;
  // The scaling factor from input to output (aka the 'real multiplier') can
  // be represented as a fixed point multiplier plus a left shift.
  int32_t output_multiplier;
  int output_shift;

  // Per channel output multiplier and shift.
  std::vector<int32_t> per_channel_output_multiplier;
  std::vector<int> per_channel_output_shift;

  // The range of the fused activation layer. For example for kNone and
  // uint8_t these would be 0 and 255.
  int32_t output_activation_min;
  int32_t output_activation_max;
  // Indexes are the offset to the memory buffer in the array used to keep track
  // of the allocated temporaries.
  int32_t im2col_index;
  int32_t hwcn_weights_index;
  int32_t input_quantized_index;
  int32_t scaling_factors_index;
  int32_t accum_scratch_index;
  int32_t input_offset_index;
  int32_t row_sums_index;

  bool need_hwcn_weights = false;
  bool have_weights_been_transposed = false;
  bool need_im2col = false;
  // If it's true, it means im2col is needed but gets disabled because the
  // temporary im2col tensor requires too much memory (i.e.
  // >= kMaxIm2colBufferSize);
  bool im2col_oversized = false;

  bool supports_multithreaded_kernel = false;
  bool is_hybrid_per_channel = false;
  bool compute_hybrid_row_sums = true;

  // Number of convolution groups.
  int32_t groups = 1;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
    case TfLitePadding::kTfLitePaddingSame:
      return PaddingType::kSame;
    case TfLitePadding::kTfLitePaddingValid:
      return PaddingType::kValid;
    case TfLitePadding::kTfLitePaddingUnknown:
    default:
      return PaddingType::kNone;
  }
}

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  // This is a builtin op, so we don't use the contents in 'buffer', if any.
  // Instead, we allocate a new object to use as scratch space for im2col, and
  // to carry information from Prepare() to Eval().
  auto* data = new OpData;
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
  eigen_support::IncrementUsageCounter(context);
#endif
  return data;
}

void Free(TfLiteContext* context, void* buffer) {
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
  eigen_support::DecrementUsageCounter(context);
#endif
  delete reinterpret_cast<OpData*>(buffer);
}

// Naive implementation of transpose for floats. Could be optimized to be more
// cache friendly, but for now it's a one-time cost on first run, and we would
// prefer to remove the need to do this at all eventually.
void TransposeFloatTensor(const TfLiteTensor* input, TfLiteTensor* output) {
  const int rows = output->dims->data[1];
  const int cols = output->dims->data[0];
  // const float* input_data = GetTensorData<float>(input);
  const float* input_data = filter_raw;
  float* output_data = GetTensorData<float>(output);
  for (int i = 0; i < rows; ++i) {
    for (int j = 0; j < cols; ++j) {
      const float in_value = input_data[i * cols + j];
      output_data[j * rows + i] = in_value;
    }
  }
}

// Check if im2col needs to be allocated, as some version of optimized Conv dont
// use it. If any change is supporting im2col in any of the Conv versions, then
// it should be updated here as well
bool IsIm2ColRequired(const TfLiteTensor* input, TfLiteConvParams* params,
                      const TfLiteTensor* filter, OpData* data, bool is_hybrid,
                      KernelType kernel_type) {
  // If HWCN weights are required, Im2Col not required
  if (data->need_hwcn_weights) return false;

  // segregate based on dilated conv & non-dialated conv
  const bool need_dilated_im2col =
      params->dilation_width_factor != 1 || params->dilation_height_factor != 1;
  // const bool need_non_dilated_im2col =
  //     params->stride_width != 1 || params->stride_height != 1 ||
  //     filter->dims->data[2] != 1 || filter->dims->data[1] != 1;
  const bool need_non_dilated_im2col =
      stride_width != 1 || stride_height != 1 ||
      filter_width != 1 || filter_height != 1;

  const bool need_im2col = need_dilated_im2col || need_non_dilated_im2col;

  // Return early as basic requirement is not met
  if (!need_im2col) return false;

  // Special case for Hybrid, as it supports only non-dilated im2col currently
  const bool is_hybrid_non_dilated = is_hybrid && need_non_dilated_im2col;
  const bool is_quantized = input->type == kTfLiteUInt8 ||
                            input->type == kTfLiteInt8 ||
                            input->type == kTfLiteInt16;

  switch (kernel_type) {
    case kReference:
      if (is_hybrid) {
        return true;
      } else {
        return false;
      }
    case kGenericOptimized:
    case kCblasOptimized:
      if (is_hybrid && !need_non_dilated_im2col) {
        return false;
      } else {
        return true;
      }
    case kMultithreadOptimized:
      if (is_hybrid_non_dilated || is_quantized ||
          !data->supports_multithreaded_kernel) {
        return true;
      } else {
        return false;
      }
    default:
      return false;
  }
}

// Allocate temporary tensors (`im2col`, `hwcn_weights` if necessary).
// Note: `context->AddTensors` might invalidate pointers to existing tensors.
// Therefore the logic to add tensors are isolated into this function.
static TfLiteStatus AllocateTemporaryTensorsIfRequired(
    TfLiteContext* context, TfLiteNode* node, bool is_hybrid,
    bool is_per_channel, KernelType kernel_type, size_t im2col_bytes) {
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams* params;
  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  // TF_LITE_ENSURE(context, node->inputs->size >= 2);
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  const TfLiteTensor* filter;
  // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));

  // If we're using the optimized multithreaded EigenTensor implementation of
  // convolution, it expects the filter weights to be transposed compared to
  // the normal TF Lite buffer format. Typical TF Lite weights are
  // [filter_count, filter_height, filter_width, input_depth], but for the float
  // implementation we need them as [filter_height, filter_width, input_depth,
  // filter_count]. We get to that format by transposing, and create a temporary
  // buffer to store the results.
  // This path is only used for float processing, so only create the buffer if
  // we're running with that data type.
  data->need_hwcn_weights =
      input->type == kTfLiteFloat32 && data->supports_multithreaded_kernel;

  // We don't always need to allocate im2col. It is only used in some versions
  // of the optimized Conv. This test just mimics something that happens inside
  // optimized_ops.h, in order to avoid a DCHECK(!im2col_data).
  data->need_im2col =
      IsIm2ColRequired(input, params, filter, data, is_hybrid, kernel_type);

  // If im2col_oversized is found to be true, we have to fallback to an
  // execution path (like kReference in float/quantized cases) that doesn't
  // require im2col operation. Therefore, we have to skip checking the hybrid
  // case (but not the hybrid-per-channel one) where there's no such a fallback
  // execution path.
  // TODO(b/178743262): Consider making this check conditioned on the available
  // memory of the system, rather than coupling to the mobile platform check.
  if (IsMobilePlatform() && !(is_hybrid && !is_per_channel) &&
      data->need_im2col && im2col_bytes >= kMaxIm2colBufferSizeMobile) {
    data->need_im2col = false;
    data->im2col_oversized = true;
  }
  int temporaries_count = 0;
  if (data->need_im2col) {
    data->im2col_index = temporaries_count;
    if (data->im2col_id == kTensorNotAllocated) {
      context->AddTensors(context, 1, &data->im2col_id);
    }
    ++temporaries_count;
  }
  if (data->need_hwcn_weights) {
    data->hwcn_weights_index = temporaries_count;
    if (data->hwcn_weights_id == kTensorNotAllocated) {
      context->AddTensors(context, 1, &data->hwcn_weights_id);
    }
    ++temporaries_count;
  }

  if (is_hybrid) {
    // Allocate tensor to store the on-the-fly quantized inputs.
    data->input_quantized_index = temporaries_count;
    if (data->input_quantized_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->input_quantized_id));
    }
    ++temporaries_count;

    // Allocate tensor to store the quantization params computed during
    // on-the-fly input quantization.
    data->scaling_factors_index = temporaries_count;
    if (data->scaling_factors_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->scaling_factors_id));
    }
    ++temporaries_count;

    // Allocate tensor to store the accumulators for the matrix multiply.
    data->accum_scratch_index = temporaries_count;
    if (data->accum_scratch_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->accum_scratch_id));
    }
    ++temporaries_count;
    if (is_per_channel) {
      data->input_offset_index = temporaries_count;
      if (data->input_offset_id == kTensorNotAllocated) {
        TF_LITE_ENSURE_OK(
            context, context->AddTensors(context, 1, &data->input_offset_id));
      }
      ++temporaries_count;

      data->row_sums_index = temporaries_count;
      if (data->row_sums_id == kTensorNotAllocated) {
        TF_LITE_ENSURE_OK(context,
                          context->AddTensors(context, 1, &data->row_sums_id));
      }
      ++temporaries_count;
    }
  }

  TfLiteIntArrayFree(node->temporaries);
  node->temporaries = TfLiteIntArrayCreate(temporaries_count);

  return kTfLiteOk;
}

TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,
                     TfLiteNode* node) {
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  // TfLiteConvParams* params;
  // TfLitePadding paddings=kTfLitePaddingSame;
  // --------------------------------------------------------------------------
  // int stride_width=2;
  // int stride_height=2;
  // TfLiteFusedActivation activation=kTfLiteActRelu6;
  // int dilation_width_factor = 1;
  // int dilation_height_factor = 1;
  // const int32_t filter_input_channel = 3;
  // const int32_t filter_output_channel = 24;
  // int filter_height = 3;
  // int filter_width = 3;
  // TfLiteType filter_type = kTfLiteFloat32;
  // const bool data_supports_multithreaded_kernel=true;
  // --------------------------------------------------------------------------
  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  bool has_bias = false;
  // bool has_bias = 0;
  // Check number of inputs/outputs
  // TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  const TfLiteTensor* filter;
  // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));

  // Check dimensionality of input, filter
  TF_LITE_ENSURE_EQ(context, input->dims->size, 4);
  // TF_LITE_ENSURE_EQ(context, filter->dims->size, 4);
  TF_LITE_ENSURE_EQ(context, filter_dims_size, 4);
  
  // Check input channels matching filter
  // Filter input channel can be a factor of channels of input (grouped conv)
  // or equals (normal conv).
  auto input_channel = input->dims->data[3];
  // auto filter_input_channel = filter->dims->data[3];

  TF_LITE_ENSURE_EQ(context, input_channel % filter_input_channel, 0);
  data->groups = input_channel / filter_input_channel;

  // Check types. (We assume that UINT8 refers to quantized tensors)
  TfLiteType input_type = input->type;
  TF_LITE_ENSURE(context,
                 input_type == kTfLiteFloat32 || input_type == kTfLiteUInt8 ||
                     input_type == kTfLiteInt8 || input_type == kTfLiteInt16);
  TF_LITE_ENSURE_TYPES_EQ(context, output->type, input_type);

  if (input_type == kTfLiteInt16) {
    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);
    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);
  }
  // Filter must have zero zero-points in per-channel quantization.
  if (input_type == kTfLiteInt16 || input_type == kTfLiteInt8) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    for (int i = 0; i < affine_quantization->zero_point->size; ++i) {
      TF_LITE_ENSURE_EQ(context, affine_quantization->zero_point->data[i], 0);
    }
  }

  const TfLiteTensor* bias = nullptr;
  // std::cout << "codes runs here #-2" << std::endl;
  // TODO(ahentz): At this point the optimized versions require 'bias'. We can
  // either change that or document that convolution requires it.

  // TF_LITE_ENSURE(context, has_bias);

  if (has_bias) {
    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 2, &bias));
    if (input_type == kTfLiteUInt8 || input_type == kTfLiteInt8) {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else if (input_type == kTfLiteInt16) {
      TF_LITE_ENSURE(context, (bias->type == kTfLiteInt32) ||
                                  (bias->type == kTfLiteInt64));
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, input_type);
    }
    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 0));
  }

  const bool is_hybrid =
      (input->type == kTfLiteFloat32 &&
       (filter_type == kTfLiteUInt8 || filter_type == kTfLiteInt8));

  if (is_hybrid && filter_type == kTfLiteInt8 &&
      filter->quantization.type == kTfLiteAffineQuantization &&
      filter->quantization.params &&
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)
          ->scale &&
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)
              ->scale->size > 1) {
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    const float scale = affine_quantization->scale->data[0];
    for (int i = 1; i < affine_quantization->scale->size; i++) {
      if (affine_quantization->scale->data[i] != scale) {
        data->is_hybrid_per_channel = true;
        break;
      }
    }
  }
  //----------------------------------------------------------------------------
  // std::cout << "codes runs here #-1" << std::endl;
  // The multi-threaded kernel supports neither dilation nor hybrid kernels, and
  // is incompatible with mutable input filters that might change between evals.
  // data->supports_multithreaded_kernel =
  //     (kernel_type == kMultithreadOptimized) &&
  //     (context->recommended_num_threads != 1) && !is_hybrid &&
  //     (dilation_width_factor == 1) &&
  //     (dilation_height_factor == 1) &&
  //     (filter->allocation_type != kTfLiteArenaRw) && !IsDynamicTensor(filter);
  data->supports_multithreaded_kernel = data_supports_multithreaded_kernel;
  // const char * bool_value = data->supports_multithreaded_kernel ? "true" : "false";
  // std::cout << bool_value << std::endl;    
  // std::cout << "codes runs here #-1.1" << std::endl;

  
  // int channels_in = filter->dims->data[3];
  int channels_in = filter_input_channel;
  // int channels_out = filter->dims->data[0];
  int channels_out = filter_output_channel;
  int width = input->dims->data[2];
  int height = input->dims->data[1];
  // int filter_width = filter->dims->data[2];
  // int filter_height = filter->dims->data[1];
  int batches = input->dims->data[0];
  //----------------------------------------------------------------------------
  // std::cout << "codes runs here #-1.2" << std::endl;
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = paddings;
  int out_width, out_height;
  data->padding = ComputePaddingHeightWidth(
      stride_height, stride_width,
      dilation_height_factor, dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
  // std::cout << "codes runs here #-1.2" << std::endl;
  size_t im2col_type_size;
  TF_LITE_ENSURE_STATUS(GetSizeOfType(context, input->type, &im2col_type_size));
  // Note that we intentionally promote the first multiplicand (i.e. 'batches')
  // to 'size_t' to avoid integer overflow here.
  const size_t im2col_bytes = static_cast<size_t>(batches) * out_height *
                              out_width * channels_in * filter_height *
                              filter_width * im2col_type_size;
  TF_LITE_ENSURE_STATUS(AllocateTemporaryTensorsIfRequired(
      context, node, is_hybrid, data->is_hybrid_per_channel, kernel_type,
      im2col_bytes));
  // std::cout << "codes runs here #0" << std::endl;
  // TF_LITE_ENSURE(context, has_bias);

  // Note that full fixed-point inference requires that all tensors have their
  // parameters set. This is usually done during quantized training or
  // calibration.
  if (input_type != kTfLiteFloat32) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||
                             affine_quantization->scale->size == channels_out));

    data->per_channel_output_multiplier.resize(channels_out);
    data->per_channel_output_shift.resize(channels_out);
    // TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
    //     context, input, filter, bias, output, activation,
    //     &data->output_multiplier, &data->output_shift,
    //     &data->output_activation_min, &data->output_activation_max,
    //     data->per_channel_output_multiplier.data(),
    //     data->per_channel_output_shift.data(), channels_out));
  }
  // std::cout << "codes runs here #0.1" << std::endl;
  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);
  output_size->data[0] = batches;
  output_size->data[1] = out_height;
  output_size->data[2] = out_width;
  output_size->data[3] = channels_out;
  auto output_status = context->ResizeTensor(context, output, output_size);

  if (output_status != kTfLiteOk) return output_status;

  if (data->need_im2col) {
    node->temporaries->data[data->im2col_index] = data->im2col_id;

    TfLiteIntArray* im2col_size = TfLiteIntArrayCreate(4);

    // auto filter_input_channel = filter->dims->data[3];
    im2col_size->data[0] = output_size->data[0];
    im2col_size->data[1] = output_size->data[1];
    im2col_size->data[2] = output_size->data[2];
    im2col_size->data[3] = filter_input_channel * filter_height * filter_width;

    TfLiteTensor* im2col =
        &context->tensors[node->temporaries->data[data->im2col_index]];
    im2col->type = input->type;
    if (is_hybrid) {
      im2col->type = filter_type;
    }
    im2col->allocation_type = kTfLiteArenaRw;
    auto im2col_status = context->ResizeTensor(context, im2col, im2col_size);
    if (im2col_status != kTfLiteOk) return im2col_status;
  }
  // std::cout << "codes runs here #0.2" << std::endl;
  if (data->need_hwcn_weights) {
    node->temporaries->data[data->hwcn_weights_index] = data->hwcn_weights_id;
    TfLiteIntArray* hwcn_weights_size = TfLiteIntArrayCreate(2);

    // Because we're treating the filter weights as a matrix when we do the
    // transpose, we allocate the buffer with a two-dimensional shape, where one
    // dimension is the number of elements in each filter, and the second is the
    // total number of filters.
    // auto filter_input_channel = filter->dims->data[3];
    hwcn_weights_size->data[0] =
        (filter_height * filter_width * filter_input_channel);
    hwcn_weights_size->data[1] = channels_out;

    TfLiteTensor* hwcn_weights =
        &context->tensors[node->temporaries->data[data->hwcn_weights_index]];
    hwcn_weights->type = input_type;
    hwcn_weights->allocation_type = kTfLiteArenaRwPersistent;

    auto hwcn_weights_status =
        context->ResizeTensor(context, hwcn_weights, hwcn_weights_size);
    if (hwcn_weights_status != kTfLiteOk) return hwcn_weights_status;

    // TODO(petewarden): If Resize() is called when the size hasn't actually
    // changed, this will do extra redundant work.
    data->have_weights_been_transposed = false;
  }
  // std::cout << "codes runs here #0.3" << std::endl;
  if (is_hybrid) {
    node->temporaries->data[data->input_quantized_index] =
        data->input_quantized_id;
    TfLiteTensor* input_quantized;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->input_quantized_index,
                                  &input_quantized));
    input_quantized->type = kTfLiteInt8;
    input_quantized->allocation_type = kTfLiteArenaRw;
    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {
      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,
                                                       input_quantized_size));
    }
    // std::cout << "codes runs here #0.4" << std::endl;
    node->temporaries->data[data->scaling_factors_index] =
        data->scaling_factors_id;
    TfLiteTensor* scaling_factors;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->scaling_factors_index,
                                  &scaling_factors));
    scaling_factors->type = kTfLiteFloat32;
    scaling_factors->allocation_type = kTfLiteArenaRw;
    // Only one scale factor per batch is typically necessary. See optimized
    // implementation for why we need to allocate for the height of the inputs
    // flattened to 2D.
    TF_LITE_ENSURE(context, channels_in != 0);
    const int height = NumElements(input) / channels_in;
    int scaling_dims[1] = {height};
    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {
      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);
      scaling_factors_size->data[0] = height;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,
                                                       scaling_factors_size));
    }
    // std::cout << "codes runs here #0.5" << std::endl;
    node->temporaries->data[data->accum_scratch_index] = data->accum_scratch_id;
    TfLiteTensor* accum_scratch;
    TF_LITE_ENSURE_OK(context,
                      GetTemporarySafe(context, node, data->accum_scratch_index,
                                       &accum_scratch));
    accum_scratch->type = kTfLiteInt32;
    accum_scratch->allocation_type = kTfLiteArenaRw;
    const int scratch_width = batches * out_height * out_width;
    int accum_scratch_dims[2] = {channels_out, scratch_width};
    if (!TfLiteIntArrayEqualsArray(accum_scratch->dims, 2,
                                   accum_scratch_dims)) {
      TfLiteIntArray* accum_scratch_size = TfLiteIntArrayCreate(2);
      accum_scratch_size->data[0] = channels_out;
      accum_scratch_size->data[1] = scratch_width;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, accum_scratch,
                                                       accum_scratch_size));
    }
    // std::cout << "codes runs here #0.6" << std::endl;
    if (data->is_hybrid_per_channel) {
      const auto* affine_quantization =
          reinterpret_cast<TfLiteAffineQuantization*>(
              filter->quantization.params);
      TF_LITE_ENSURE_EQ(
          context, affine_quantization->scale->size,
          filter->dims->data[affine_quantization->quantized_dimension]);
      node->temporaries->data[data->input_offset_index] = data->input_offset_id;
      TfLiteTensor* input_offsets;
      TF_LITE_ENSURE_OK(
          context, GetTemporarySafe(context, node, data->input_offset_index,
                                    &input_offsets));
      input_offsets->type = kTfLiteInt32;
      input_offsets->allocation_type = kTfLiteArenaRw;
      // See above comment for the need to allocate for height of inputs.
      TF_LITE_ENSURE(context, channels_in != 0);
      const int height = NumElements(input) / channels_in;
      const int input_offset_dims[1] = {height};
      if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1,
                                     input_offset_dims)) {
        TfLiteIntArray* input_offsets_size = TfLiteIntArrayCreate(1);
        input_offsets_size->data[0] = input_offset_dims[0];
        TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_offsets,
                                                         input_offsets_size));
      }
      node->temporaries->data[data->row_sums_index] = data->row_sums_id;
      TfLiteTensor* row_sums;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));
      row_sums->type = kTfLiteInt32;
      row_sums->allocation_type = kTfLiteArenaRwPersistent;
      // See above comment for the need to allocate for height of inputs.
      const int row_sums_dims[1] = {channels_out};
      if (!TfLiteIntArrayEqualsArray(row_sums->dims, 1, row_sums_dims)) {
        TfLiteIntArray* row_sums_size = TfLiteIntArrayCreate(1);
        row_sums_size->data[0] = row_sums_dims[0];
        TF_LITE_ENSURE_OK(
            context, context->ResizeTensor(context, row_sums, row_sums_size));
      }
    }
  }
  // std::cout << "codes runs here #1" << std::endl;
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return Prepare(kernel_type, context, node);
}

template <KernelType kernel_type>
void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* output) {
  auto input_offset = -input->params.zero_point;
  auto filter_offset = -filter->params.zero_point;
  auto output_offset = output->params.zero_point;

  KernelType effective_kernel_type;
  if ((kernel_type == kMultithreadOptimized ||
       kernel_type == kCblasOptimized) &&
      (params->dilation_width_factor != 1 ||
       params->dilation_height_factor != 1)) {
    // kMultithreadOptimized and kCblasOptimized do not support dilation.
    // Therefore, fallback to optimized.
    effective_kernel_type = kGenericOptimized;
  } else {
    effective_kernel_type = kernel_type;
  }

  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.dilation_width_factor = dilation_width_factor;
  op_params.dilation_height_factor = dilation_height_factor;
  op_params.stride_width = stride_width;
  op_params.stride_height = stride_height;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  switch (effective_kernel_type) {
    case kReference: {
      reference_ops::Conv(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(filter), GetTensorData<uint8_t>(filter),
          GetTensorShape(bias), GetTensorData<int32_t>(bias),
          GetTensorShape(output), GetTensorData<uint8_t>(output),
          GetTensorShape(im2col), GetTensorData<uint8_t>(im2col),
          /* cpu_backend_context = */ nullptr);
      break;
    }
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      // There is only one optimized implementation for Quantized Conv.
      optimized_ops::Conv(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(filter), GetTensorData<uint8_t>(filter),
          GetTensorShape(bias), GetTensorData<int32_t>(bias),
          GetTensorShape(output), GetTensorData<uint8_t>(output),
          GetTensorShape(im2col), GetTensorData<uint8_t>(im2col),
          CpuBackendContext::GetFromContext(context));
      break;
    }
  }
}

template <KernelType kernel_type>
void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, OpData* data,
                             const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             TfLiteTensor* im2col) {
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = stride_height;
  op_params.stride_width = stride_width;
  op_params.dilation_height_factor = dilation_height_factor;
  op_params.dilation_width_factor = dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  switch (effective_kernel_type) {
    case kReference: {
      reference_integer_ops::ConvPerChannel(
          op_params, data->per_channel_output_multiplier.data(),
          data->per_channel_output_shift.data(), GetTensorShape(input),
          GetTensorData<int8>(input), GetTensorShape(filter),
          GetTensorData<int8>(filter), GetTensorShape(bias),
          GetTensorData<int32>(bias), GetTensorShape(output),
          GetTensorData<int8>(output));
      break;
    }
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      optimized_integer_ops::ConvPerChannel(
          op_params, data->per_channel_output_multiplier.data(),
          data->per_channel_output_shift.data(), GetTensorShape(input),
          GetTensorData<int8>(input), GetTensorShape(filter),
          GetTensorData<int8>(filter), GetTensorShape(bias),
          GetTensorData<int32>(bias), GetTensorShape(output),
          GetTensorData<int8>(output), GetTensorShape(im2col),
          GetTensorData<int8>(im2col),
          CpuBackendContext::GetFromContext(context));
      break;
    }
  }
}

template <KernelType kernel_type>
void EvalQuantizedPerChannel16x8(TfLiteContext* context, TfLiteNode* node,
                                 TfLiteConvParams* params, OpData* data,
                                 const TfLiteTensor* input,
                                 const TfLiteTensor* filter,
                                 const TfLiteTensor* bias, TfLiteTensor* output,
                                 TfLiteTensor* im2col) {
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = stride_height;
  op_params.stride_width = stride_width;
  op_params.dilation_height_factor = dilation_height_factor;
  op_params.dilation_width_factor = dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  // To prevent 32bit accum overflow for 16x8 quantization, it enables the
  // optimized path only when zero_point is 0.
  bool has_non_zero_point = input->params.zero_point ||
                            filter->params.zero_point ||
                            output->params.zero_point;

  // Fallback to reference kernel when bias_type is int64 as
  // there is no optimized kernel for int64 bias yet.
  if (bias && bias->type == kTfLiteInt64) {
    reference_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<std::int64_t>(bias), GetTensorShape(output),
        GetTensorData<int16>(output));
  } else if (effective_kernel_type == kReference || has_non_zero_point) {
    reference_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<std::int32_t>(bias), GetTensorShape(output),
        GetTensorData<int16>(output));
  } else {
    optimized_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16_t>(input), GetTensorShape(filter),
        GetTensorData<int8_t>(filter), GetTensorShape(bias),
        GetTensorData<std::int32_t>(bias), GetTensorShape(output),
        GetTensorData<int16_t>(output), GetTensorShape(im2col),
        GetTensorData<int16_t>(im2col),
        CpuBackendContext::GetFromContext(context));
  }
}

template <KernelType kernel_type>
void EvalFloat(TfLiteContext* context, TfLiteNode* node,
               TfLiteConvParams* params, OpData* data,
               const TfLiteTensor* input, const TfLiteTensor* filter,
               const TfLiteTensor* bias, TfLiteTensor* im2col,
               TfLiteTensor* hwcn_weights, TfLiteTensor* output) {

  float output_activation_min, output_activation_max;

//----------------------------------------------------------------
  CalculateActivationRange(activation, &output_activation_min,
                           &output_activation_max);
  KernelType effective_kernel_type = kernel_type;
  // Fall back to the optimized path if multi-threaded conv is unsupported.
  if ((kernel_type == kMultithreadOptimized) &&
      !data->supports_multithreaded_kernel) {
    effective_kernel_type = kGenericOptimized;
  }

  // When im2col is needed (which is implied when 'im2col_oversized' is true),
  // the GEMMM-based optimized path requires im2col data be allocated to ensure
  // the correctness. Therefore, when im2col is disabled because of the
  // oversized temporary im2col tensor, fallback to a non-optimized path is
  // needed.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
    // As detailed by tflite::multithreaded_ops::Conv implementation in
    // multithreaded_conv.h, the Eigen-based execution doesn't need im2col data.
    // Therefore, we could rely on it as a better-optimized fallback than the
    // reference one.
    if (data->supports_multithreaded_kernel) {
      effective_kernel_type = kMultithreadOptimized;
    }
#endif
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(paddings);
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = stride_width;
  op_params.stride_height = stride_height;
  op_params.dilation_width_factor = dilation_width_factor;
  op_params.dilation_height_factor = dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  // std::cout << "codes runs here #5" << std::endl;
  // const int filter_dims_size=4;
  const int32_t* filter_dims_data;
  filter_dims_data = filter_dims_raw;
  const int32_t* bias_dims_data;
  bias_dims_data = bias_dims_raw;

  const float* filter_data;
  filter_data = filter_raw;
  const float* bias_data;
  bias_data = bias_raw;

  switch (effective_kernel_type) {
    case kReference: {
      // std::cout << "codes runs here #6" << std::endl;
      reference_ops::Conv(op_params, GetTensorShape(input),
                          GetTensorData<float>(input), RuntimeShape(filter_dims_size, filter_dims_data),
                          filter_data, RuntimeShape(bias_dims_size, bias_dims_data),
                          bias_data, GetTensorShape(output),
                          GetTensorData<float>(output), GetTensorShape(im2col),
                          GetTensorData<float>(im2col));
      break;
    }
    case kCblasOptimized:
    case kGenericOptimized: {
      // std::cout << "codes runs here #7" << std::endl;
      optimized_ops::Conv(op_params, GetTensorShape(input),
                          GetTensorData<float>(input), RuntimeShape(filter_dims_size, filter_dims_data),
                          filter_data, RuntimeShape(bias_dims_size, bias_dims_data),
                          bias_data, GetTensorShape(output),
                          GetTensorData<float>(output), GetTensorShape(im2col),
                          GetTensorData<float>(im2col),
                          CpuBackendContext::GetFromContext(context));
      break;
    }
    case kMultithreadOptimized: {
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
      
      multithreaded_ops::Conv(
          *eigen_support::GetThreadPoolDevice(context), op_params,
          GetTensorShape(input), GetTensorData<float>(input),
          RuntimeShape(filter_dims_size, filter_dims_data), filter_data, RuntimeShape(bias_dims_size, bias_dims_data),
          bias_data, GetTensorShape(output),
          GetTensorData<float>(output), GetTensorShape(im2col),
          GetTensorData<float>(im2col));
      break;
#else   // !defined(TFLITE_WITH_MULTITHREADED_EIGEN)
      // See Register_CONV_2D: we should never be here when TFLITE_WITH_RUY
      // was enabled. We #if out this code in order to get the corresponding
      // binary size benefits.
      TFLITE_DCHECK(false);
#endif  // defined(TFLITE_WITH_MULTITHREADED_EIGEN)
    }
  }
}

template <KernelType kernel_type>
TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,
                                  TfLiteConvParams* params, OpData* data,
                                  const TfLiteTensor* input,
                                  const TfLiteTensor* filter,
                                  const TfLiteTensor* bias,
                                  TfLiteTensor* im2col, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(activation, &output_activation_min,
                           &output_activation_max);

  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;
  TfLiteTensor* quantized_input_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &quantized_input_tensor));
  int8_t* quantized_input_ptr_batch =
      GetTensorData<int8_t>(quantized_input_tensor);
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);
  TfLiteTensor* input_offset_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_offset_index,
                                     &input_offset_tensor));
  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);

  for (int b = 0; b < batch_size; ++b) {
    const int offset = b * input_size;
    tensor_utils::AsymmetricQuantizeFloats(
        GetTensorData<float>(input) + offset, input_size,
        quantized_input_ptr_batch + offset, &scaling_factors_ptr[b],
        &input_offset_ptr[b]);
  }

  int8_t* im2col_ptr = nullptr;
  int8_t* filter_ptr = nullptr;
  if (im2col != nullptr) {
    im2col_ptr = im2col->data.int8;
  }
  filter_ptr = filter->data.int8;
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.dilation_width_factor = dilation_width_factor;
  op_params.dilation_height_factor = dilation_height_factor;
  op_params.stride_width = stride_width;
  op_params.stride_height = stride_height;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  switch (effective_kernel_type) {
    case kReference:
      reference_ops::HybridConvPerChannel(
          op_params, scaling_factors_ptr, GetTensorShape(input),
          quantized_input_ptr_batch, GetTensorShape(filter), filter_ptr,
          GetTensorShape(bias), GetTensorData<float>(bias),
          GetTensorShape(output), GetTensorData<float>(output),
          GetTensorShape(im2col), im2col_ptr, affine_quantization->scale->data,
          input_offset_ptr);
      break;
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      TfLiteTensor* row_sums;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));
      TfLiteTensor* scratch;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->accum_scratch_index, &scratch));
      optimized_ops::HybridConvPerChannel(
          op_params, scaling_factors_ptr, GetTensorShape(input),
          quantized_input_ptr_batch, GetTensorShape(filter), filter_ptr,
          GetTensorShape(bias), GetTensorData<float>(bias),
          GetTensorShape(output), GetTensorData<float>(output),
          GetTensorShape(im2col), im2col_ptr, affine_quantization->scale->data,
          input_offset_ptr, GetTensorShape(scratch),
          GetTensorData<int32>(scratch), GetTensorData<int32_t>(row_sums),
          &data->compute_hybrid_row_sums,
          CpuBackendContext::GetFromContext(context));
      data->compute_hybrid_row_sums = false;
      break;
    }
  }

  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalHybrid(TfLiteContext* context, TfLiteNode* node,
                        TfLiteConvParams* params, OpData* data,
                        const TfLiteTensor* input, const TfLiteTensor* filter,
                        const TfLiteTensor* bias, TfLiteTensor* im2col,
                        TfLiteTensor* accum_scratch, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(activation, &output_activation_min,
                           &output_activation_max);

  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;

  const float* input_ptr = GetTensorData<float>(input);
  TfLiteTensor* quantized_input_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &quantized_input_tensor));
  int8_t* quantized_input_ptr_batch =
      GetTensorData<int8_t>(quantized_input_tensor);
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);

  // Per-batch input quantization for higher accuracy.
  {
    ruy::profiler::ScopeLabel label("ConvHybridQuantizeInputs");
    for (int b = 0; b < batch_size; ++b) {
      float unused_min, unused_max;
      const int offset = b * input_size;
      tensor_utils::SymmetricQuantizeFloats(
          input_ptr + offset, input_size, quantized_input_ptr_batch + offset,
          &unused_min, &unused_max, &scaling_factors_ptr[b]);
      scaling_factors_ptr[b] *= filter->params.scale;
    }
  }

  switch (kernel_type) {
    case kReference:
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      // There is only one implementation for hybrid kernel.
      ConvParams op_params;
      op_params.padding_type = PaddingType::kSame;
      op_params.padding_values.width = data->padding.width;
      op_params.padding_values.height = data->padding.height;
      op_params.stride_width = stride_width;
      op_params.stride_height = stride_height;
      op_params.dilation_width_factor = dilation_width_factor;
      op_params.dilation_height_factor = dilation_height_factor;
      op_params.float_activation_min = output_activation_min;
      op_params.float_activation_max = output_activation_max;
      if (data->groups == 1) {
        optimized_ops::HybridConv(
            op_params, scaling_factors_ptr, GetTensorShape(input),
            quantized_input_ptr_batch, GetTensorShape(filter),
            GetTensorData<int8_t>(filter), GetTensorShape(bias),
            GetTensorData<float>(bias), GetTensorShape(accum_scratch),
            GetTensorData<int32_t>(accum_scratch), GetTensorShape(output),
            GetTensorData<float>(output), GetTensorShape(im2col),
            GetTensorData<int8_t>(im2col),
            CpuBackendContext::GetFromContext(context));
      } else {
        // This case is handled by (fallbacked to) per channel hybrid group conv
        // and shouldn't hit this branch.
        TF_LITE_KERNEL_LOG(
            context,
            "Group convolution currently not supported for hybrid kernel.");
        return kTfLiteError;
      }
      break;
    }
  }

  return kTfLiteOk;
}

template <KernelType kernel_type, TfLiteType input_type>
TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {
  // std::cout << "codes runs here #2" << std::endl;
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams* params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  const TfLiteTensor* filter;
  const int dims_size = filter_dims_size;

  // std::cout << "filter dim size:" << dims_size << std::endl;
  // const int32_t* dims_data = reinterpret_cast<const int32_t*>(dims->data);
  const int32_t* dims_data = filter_dims_raw;
  // bool has_bias = node->inputs->size == 3;
  // const TfLiteTensor* bias = has_bias ? GetInput(context, node, 2) : nullptr;
  const TfLiteTensor* bias = nullptr;
  TfLiteTensor* im2col =
      data->need_im2col
          ? &context->tensors[node->temporaries->data[data->im2col_index]]
          : nullptr;
  TfLiteTensor* hwcn_weights =
      data->need_hwcn_weights
          ? &context->tensors[node->temporaries->data[data->hwcn_weights_index]]
          : nullptr;

  if (data->need_hwcn_weights && !data->have_weights_been_transposed) {
    TransposeFloatTensor(filter, hwcn_weights);
    data->have_weights_been_transposed = true;
  }
  // std::cout << "codes runs here #3" << std::endl;

  TFLITE_DCHECK_EQ(input_type, input->type);
  switch (input_type) {  // Already know in/outtypes are same.
    case kTfLiteFloat32:
      if (filter_type == kTfLiteUInt8 || filter_type == kTfLiteInt8) {
        if (data->is_hybrid_per_channel ||
            // TODO(b/162870360): Fallback to PerChannel implementation
            // before we have grouped hybrid convolution.
            data->groups != 1) {
          TF_LITE_ENSURE_OK(context, EvalHybridPerChannel<kernel_type>(
                                         context, node, params, data, input,
                                         filter, bias, im2col, output));
        } else {
          TfLiteTensor* accum_scratch =
              &context->tensors[node->temporaries
                                    ->data[data->accum_scratch_index]];
          TF_LITE_ENSURE_OK(context,
                            EvalHybrid<kernel_type>(context, node, params, data,
                                                    input, filter, bias, im2col,
                                                    accum_scratch, output));
        }
      } else {
        EvalFloat<kernel_type>(context, node, params, data, input, filter, bias,
                               im2col, hwcn_weights, output);
      }
      break;
    case kTfLiteUInt8:
      EvalQuantized<kernel_type>(context, node, params, data, input, filter,
                                 bias, im2col, output);
      break;
    case kTfLiteInt8:
      EvalQuantizedPerChannel<kernel_type>(context, node, params, data, input,
                                           filter, bias, output, im2col);
      break;
    case kTfLiteInt16:
      EvalQuantizedPerChannel16x8<kernel_type>(
          context, node, params, data, input, filter, bias, output, im2col);
      break;
    default:
      TF_LITE_KERNEL_LOG(context, "Type %s currently not supported.",
                         TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));

  switch (input->type) {
    case kTfLiteFloat32:
      return EvalImpl<kernel_type, kTfLiteFloat32>(context, node);
    case kTfLiteUInt8:
      return EvalImpl<kernel_type, kTfLiteUInt8>(context, node);
    case kTfLiteInt8:
      return EvalImpl<kernel_type, kTfLiteInt8>(context, node);
    case kTfLiteInt16:
      return EvalImpl<kernel_type, kTfLiteInt16>(context, node);
    default:
      TF_LITE_KERNEL_LOG(context, "Type %s not currently supported.",
                         TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
}

}  // namespace conv

TfLiteRegistration* Register_ffgwhz_REF() {
  static TfLiteRegistration r = {ffgwhz::Init, ffgwhz::Free,
                                 ffgwhz::Prepare<ffgwhz::kReference>,
                                 ffgwhz::Eval<ffgwhz::kReference>};
  return &r;
}

TfLiteRegistration* Register_ffgwhz_GENERIC_OPT() {
  static TfLiteRegistration r = {ffgwhz::Init, ffgwhz::Free,
                                 ffgwhz::Prepare<ffgwhz::kGenericOptimized>,
                                 ffgwhz::Eval<ffgwhz::kGenericOptimized>};
  return &r;
}

TfLiteRegistration* Register_ffgwhz_MULTITHREADED_OPT() {
  static TfLiteRegistration r = {ffgwhz::Init, ffgwhz::Free,
                                 ffgwhz::Prepare<ffgwhz::kMultithreadOptimized>,
                                 ffgwhz::Eval<ffgwhz::kMultithreadOptimized>};
  return &r;
}

// TfLiteRegistration* Register_ffgwhz_CBLAS_OPT() {
//   static TfLiteRegistration r = {ffgwhz::Init, ffgwhz::Free,
//                                  ffgwhz::Prepare<ffgwhz::kCblasOptimized>,
//                                  ffgwhz::Eval<ffgwhz::kCblasOptimized>};
//   return &r;
// }

TfLiteRegistration* Register_ffgwhz() {
#if defined TFLITE_WITH_MULTITHREADED_EIGEN
  return Register_ffgwhz_MULTITHREADED_OPT();
#else
  return Register_ffgwhz_GENERIC_OPT();
#endif
}


}  // namespace builtin
}  // namespace ops
}  // namespace tflite
