/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
#include "tensorflow/lite/kernels/internal/optimized/integer_ops/conv.h"

#include <stddef.h>
#include <iostream>
#include <cstdint>
#include <vector>

// Only use multi-threaded Eigen if ruy is disabled.
#if !defined(TFLITE_WITH_RUY)
#define TFLITE_WITH_MULTITHREADED_EIGEN
#endif

#include "tensorflow/lite/c/builtin_op_data.h"
#include "tensorflow/lite/c/common.h"
#include "tensorflow/lite/kernels/cpu_backend_context.h"
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
#include "tensorflow/lite/kernels/eigen_support.h"
#endif
#include "tensorflow/lite/kernels/internal/compatibility.h"
#include "tensorflow/lite/kernels/internal/types.h"
// b/131835803 forces us to include multithreaded_conv.h before optimized_ops.h
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
#include "tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h"
#endif
#include "tensorflow/lite/kernels/internal/optimized/optimized_ops.h"
#include "tensorflow/lite/kernels/internal/quantization_util.h"
#include "tensorflow/lite/kernels/internal/reference/conv.h"
#include "tensorflow/lite/kernels/internal/reference/integer_ops/conv.h"
#include "tensorflow/lite/kernels/internal/tensor.h"
#include "tensorflow/lite/kernels/internal/tensor_ctypes.h"
#include "tensorflow/lite/kernels/internal/tensor_utils.h"
#include "tensorflow/lite/kernels/kernel_util.h"
#include "tensorflow/lite/kernels/padding.h"
#include "tensorflow/lite/util.h"

namespace tflite {
namespace ops {
namespace custom {
namespace muonzn {

// This file has 4 implementation of Conv.
enum KernelType {
  kReference,
  kGenericOptimized,  // Neon-free
  // kMultithreadOptimized is a mixture of an Eigen-based kernel when threads
  // are available and kGenericOptimized when we must use only one thread.
  kMultithreadOptimized,
  // The kernel uses use CBLAS interface for matrix multiplication.
  // It's fast when an optimized CBLAS implementation is available (e.g. Apple
  // Accelerate Framework), and it's slow when falling back to naive
  // implementation.
  kCblasOptimized,
};

const int kTensorNotAllocated = -1;

static constexpr size_t kMaxIm2colBufferSizeMobile = 1024 * 1024 * 1024;  // 1GB

const float filter_raw[4096]={-0.1948818862438202, -0.05321884900331497, -0.008604067377746105, -0.14387263357639313, 0.08375237137079239, -0.19958114624023438, -0.005219192244112492, 0.19053110480308533, 0.11833713948726654, 0.23139432072639465, 0.061856456100940704, -0.4471380412578583, 0.025086399167776108, -0.16053758561611176, 0.018385950475931168, 0.31069469451904297, 0.2625197172164917, -0.27863484621047974, 0.3295533061027527, -0.18635766208171844, 0.35512295365333557, 0.0015079351142048836, 0.059918925166130066, 0.1361948847770691, -0.25568825006484985, 0.16793176531791687, 0.39847689867019653, -0.25242120027542114, -0.16290196776390076, 0.14489440619945526, 0.0049453116953372955, -0.051426198333501816, -0.16420792043209076, 0.17343862354755402, 0.07842114567756653, -0.11129199713468552, -0.08043596148490906, 0.09102209657430649, -0.20450705289840698, 0.25136351585388184, 0.26645347476005554, 0.15922917425632477, 0.12693999707698822, 0.07643824815750122, 0.043967656791210175, 0.11045018583536148, -0.18420971930027008, 0.09888648986816406, -0.16263693571090698, 0.11896507441997528, -0.10294849425554276, 0.21441218256950378, -0.06226612627506256, -0.21651580929756165, 0.009120536036789417, -0.44953635334968567, -0.0953415036201477, 0.10565787553787231, -0.008937269449234009, 0.1290827840566635, -0.3401004374027252, -0.16443504393100739, 0.14132322371006012, -0.2206544280052185, 0.14186306297779083, -0.11168625950813293, -0.0072721149772405624, -0.29708993434906006, -0.10146332532167435, 0.04765753075480461, -0.18333542346954346, 0.16834264993667603, -0.043507497757673264, -0.19610071182250977, -0.08459439873695374, -0.2063852697610855, -0.36130350828170776, 0.10292913019657135, 0.3223873972892761, -0.12217981368303299, 0.2577713429927826, -0.03611677139997482, -0.06600967794656754, 0.011680016294121742, 0.2345985621213913, -0.4029510021209717, -0.2923205494880676, -0.36517471075057983, 0.18438155949115753, -0.1957901269197464, -0.04320523515343666, 0.19058005511760712, 0.018129877746105194, 0.030444670468568802, -0.027941694483160973, -0.0414145402610302, -0.014245209284126759, -0.08899765461683273, -0.2071986049413681, -0.02033117413520813, 0.26797083020210266, -0.03511345386505127, -0.022831402719020844, -0.03914676606655121, -0.06001005694270134, 0.0570068433880806, -0.24037490785121918, -0.542264997959137, -0.05497404932975769, -0.15967504680156708, -0.25880923867225647, -0.3876636028289795, 0.2328035980463028, 0.03491809219121933, -0.28704774379730225, 0.021400537341833115, 0.15206551551818848, 0.229082390666008, 0.4180360734462738, -0.30709028244018555, 0.08227866142988205, -0.2521187961101532, 0.19091053307056427, 0.09082242846488953, 0.012131866998970509, 0.22833719849586487, 0.0016636463114991784, -0.17327162623405457, 0.2548619210720062, -0.04142381623387337, -0.10919096320867538, -0.268957257270813, 0.1450149565935135, 0.06056831032037735, -0.010802694596350193, -0.2109590470790863, 0.052805520594120026, 0.02802620269358158, -0.0870133712887764, 0.15240886807441711, -0.051725685596466064, -0.08666595071554184, -0.07758227735757828, -0.12629224359989166, -0.016897032037377357, 0.10951105505228043, -0.14969347417354584, -0.09584373980760574, -0.11489414423704147, 0.13530318439006805, 0.23754583299160004, -0.155141681432724, 0.06533201783895493, 0.4161446690559387, -0.1098514199256897, 0.18278385698795319, -0.06497548520565033, -0.06897682696580887, -0.17959119379520416, -0.15745823085308075, 0.06825878471136093, -0.23526518046855927, -0.21576760709285736, -0.10247674584388733, 0.022090334445238113, 0.04800328612327576, 0.004814701620489359, 0.09276992827653885, -0.2652834355831146, -0.11519815027713776, -0.20216067135334015, -0.05052332580089569, 0.1636795550584793, 0.07662737369537354, 0.09405717253684998, -0.2537209093570709, -0.03659222275018692, 0.35612812638282776, -0.1023949682712555, 0.31692954897880554, 0.008694859221577644, 0.08626604080200195, -0.08866185694932938, -0.17093941569328308, -0.35317596793174744, -0.3248637914657593, 0.3028419017791748, -0.18789394199848175, -0.19840465486049652, -0.09392284601926804, 0.2355065941810608, -0.03615972399711609, -0.14188148081302643, -0.10640781372785568, -0.06948796659708023, -0.02809489704668522, 0.03307950496673584, -0.0880713239312172, 0.2010211944580078, -0.13407520949840546, 0.025842705741524696, -0.21505025029182434, 0.10924023389816284, 0.12258387356996536, 0.0852382555603981, 0.15188394486904144, 0.010792453773319721, -0.17902082204818726, 0.24003538489341736, -0.27391311526298523, -0.09511523693799973, -0.15246625244617462, 0.009561261162161827, -0.1122022494673729, 0.15099212527275085, -0.008933665230870247, 0.03679237142205238, -0.3644196093082428, 0.21597014367580414, 0.018756093457341194, -0.05977163836359978, -0.04130777344107628, 0.08727414906024933, 0.2030470073223114, -0.37655335664749146, 0.02516746148467064, -0.12174248695373535, -0.0880427435040474, 0.07257644832134247, -0.2838958501815796, 0.04381886124610901, 0.0532495453953743, 0.03649987280368805, -0.37495845556259155, 0.2258790135383606, -0.030421681702136993, 0.049421463161706924, 0.1103549599647522, -0.07388830184936523, 0.35383278131484985, -0.15103544294834137, 0.03845589980483055, 0.15624013543128967, 0.21085666120052338, 0.2754221260547638, -0.04679451137781143, 0.4822911024093628, 0.14066162705421448, 0.09007991850376129, 0.05216115340590477, -0.15868183970451355, -0.17336492240428925, 0.10436791926622391, -0.06752981245517731, -0.3889825642108917, -0.05756453052163124, -0.37462759017944336, -0.07296113669872284, -0.42404136061668396, -0.13700483739376068, 0.02734864503145218, -0.2097770869731903, 0.30498456954956055, -0.14078162610530853, -0.30268147587776184, -0.026468705385923386, -0.2238689810037613, 0.06124631687998772, -0.007655367720872164, -0.18065014481544495, 0.05671592056751251, -0.0387253537774086, -0.2637638449668884, 0.0142709044739604, -0.4516168236732483, 0.16541025042533875, -0.24247637391090393, -0.03796245902776718, -0.0707566887140274, 0.05058542266488075, 0.08433445543050766, 0.06476272642612457, 0.16282489895820618, -0.007532482501119375, 0.20213544368743896, 0.12483229488134384, 0.36292535066604614, -0.04907291755080223, -0.09624120593070984, 0.048916324973106384, -0.2595977783203125, 0.06341242045164108, 0.07467107474803925, -0.33911359310150146, -0.08386990427970886, -0.01989855244755745, 0.05654919147491455, -0.15621623396873474, 0.10361897945404053, -0.0904209092259407, -0.011042661964893341, 0.06284536421298981, -0.15470127761363983, -0.26542115211486816, -0.043195463716983795, -0.09657814353704453, 0.08571548014879227, -0.09246401488780975, 0.2035493701696396, 0.17722082138061523, -0.10882911086082458, -0.27108845114707947, -0.3819831311702728, -0.30929094552993774, -0.20791636407375336, -0.19931158423423767, 0.07216337323188782, -0.2545521855354309, -0.12235935777425766, -0.33658933639526367, 0.03519904613494873, 0.14929966628551483, -0.08148927986621857, 0.14326032996177673, -0.12981878221035004, -0.11430893838405609, -0.05757435783743858, 0.08812259137630463, -0.2415788620710373, -0.1214195266366005, -0.12195856869220734, 0.17113454639911652, -0.054952044039964676, 0.35844430327415466, -0.046676624566316605, 0.23065412044525146, 0.2183195948600769, -0.053893424570560455, 0.01400218065828085, -0.06900490075349808, 0.12540411949157715, 0.27866101264953613, 0.19245769083499908, 0.01900717057287693, 0.07565326243638992, 0.23155751824378967, 0.029089616611599922, -0.39552807807922363, -0.09848923981189728, 0.22522129118442535, 0.31582626700401306, -0.09988199919462204, -0.3291357457637787, 0.16782602667808533, 0.016637679189443588, -0.12951499223709106, 0.032908838242292404, -0.005368743557482958, -0.21969401836395264, -0.41525793075561523, 0.07319415360689163, -0.06737413257360458, 0.289193332195282, -0.23977656662464142, -0.1273217350244522, -0.0320366695523262, -0.3977161943912506, -0.5653395056724548, 0.07409386336803436, -0.013825665228068829, 0.409282922744751, 0.08776392787694931, 0.0765380784869194, -0.008073680102825165, -0.7395275235176086, -0.11993127316236496, -0.06543077528476715, -0.15342722833156586, -0.1927863210439682, 0.10476072877645493, -0.17225757241249084, -0.07574178278446198, 0.052755728363990784, -0.026864582672715187, 0.16631881892681122, 0.21845491230487823, 0.32264700531959534, -0.06579294800758362, 0.07777134329080582, -0.1601700633764267, 0.038431838154792786, 0.3553144335746765, -0.007494412828236818, -0.30053919553756714, 0.5555480122566223, 0.11364275962114334, -0.14185911417007446, -0.12770996987819672, -0.1526206135749817, -0.4871025085449219, 0.25775450468063354, -0.05834048241376877, -0.11805688589811325, -0.08565372973680496, -0.10879213362932205, 0.45917198061943054, 0.1908385306596756, 0.35468727350234985, 0.02435866929590702, -0.3051048219203949, 0.10067444294691086, 0.1735590398311615, 0.014875808730721474, 0.21893544495105743, -0.2587089538574219, -0.15502923727035522, 0.27289626002311707, -0.09828253835439682, -0.1679133176803589, -0.28029903769493103, 0.24659065902233124, 0.10518763214349747, -0.21367178857326508, -0.1931939423084259, -0.04318062588572502, -0.1262330859899521, -0.37494319677352905, 0.07943692058324814, 0.1786678433418274, 0.159516841173172, 0.06191843003034592, 0.31155821681022644, 0.06410885602235794, -0.2432909905910492, -0.177899569272995, -0.07746396958827972, -0.13406799733638763, -0.30559292435646057, -0.06731435656547546, 0.11390262097120285, 0.25701236724853516, -0.25250187516212463, -0.17397375404834747, -0.20188745856285095, -0.08659522235393524, 0.13202600181102753, -0.0469818040728569, 0.5603512525558472, -0.06709297746419907, -0.2762761414051056, 0.1919030249118805, -0.18145053088665009, -0.08620452135801315, 0.09608229249715805, 0.1590515673160553, -0.10337857902050018, -0.023634491488337517, -0.08377218246459961, 0.0038072846364229918, -0.24959884583950043, -0.3006477653980255, 0.14354251325130463, 0.1452491730451584, 0.20074640214443207, -0.03340951353311539, -0.05182931199669838, -0.06540025025606155, -0.08461505174636841, -0.3000597357749939, 0.13707147538661957, 0.5777219533920288, -0.20659253001213074, 0.12319084256887436, 0.358737587928772, 0.09969088435173035, -0.10556847602128983, 0.21949468553066254, -0.11742094904184341, 0.05203761160373688, -0.14372766017913818, 0.12305746227502823, 0.04089043289422989, 0.2819075882434845, -0.6326582431793213, -0.0208444781601429, 0.2808918356895447, 0.2393091917037964, 0.09742610901594162, -0.29871803522109985, -0.02584698796272278, 0.2854551374912262, 0.21359878778457642, 0.17257142066955566, -0.10654199868440628, -0.044177889823913574, -0.04947627708315849, 0.1027921661734581, 0.022973379120230675, 0.13058161735534668, 0.17466992139816284, 0.12096398323774338, -0.17893271148204803, 0.15567782521247864, -0.048009682446718216, 0.1800897866487503, -0.3297135829925537, -0.047125011682510376, -0.2494509071111679, -0.050643932074308395, 0.066087506711483, 0.02137123793363571, -0.11605750769376755, -0.24169473350048065, -0.03374776989221573, -0.11163295805454254, 0.05580819398164749, -0.1678636223077774, 0.14439676702022552, -0.11639860272407532, -0.18336637318134308, 0.31197553873062134, -0.04133479669690132, 0.09015168249607086, 0.22238172590732574, -0.09306320548057556, 0.19538505375385284, 0.061894673854112625, 0.06744008511304855, -0.02242736890912056, -0.8612552285194397, -0.2556282579898834, -0.02661789208650589, -0.06247754395008087, -0.0929814800620079, 0.0066771963611245155, 0.23556064069271088, -0.0856032595038414, -0.1347178816795349, -0.019960472360253334, -0.033850111067295074, -0.32869699597358704, -0.11260784417390823, 0.31216251850128174, 0.07466068863868713, 0.08831752091646194, 0.033204585313797, 0.5930077433586121, 0.07668762654066086, -0.29704901576042175, 0.4153839945793152, -0.4339352548122406, 0.09469856321811676, 0.06093871593475342, -0.0727057158946991, 0.13153426349163055, 0.01361203845590353, 0.10039132088422775, 0.1419738233089447, -0.08682872354984283, 0.016939954832196236, -0.5765938758850098, 0.0255209282040596, 0.3007985055446625, 0.09057935327291489, 0.05400076508522034, 0.17195551097393036, -0.10012705624103546, 0.04609358683228493, -0.03975999355316162, -0.04150405526161194, 0.03157973662018776, 0.057431209832429886, -0.18004678189754486, -0.02045314945280552, 0.14995001256465912, -0.18806307017803192, -0.12629659473896027, 0.24146869778633118, 0.227861225605011, 0.0022885706275701523, -0.06336186081171036, 0.045594941824674606, -0.01450724620372057, 0.10901177674531937, 0.2366340607404709, -0.04122425615787506, 0.14402051270008087, 0.0943273976445198, 0.007917605340480804, 0.12618663907051086, 0.45065805315971375, 0.06904678791761398, -0.12780389189720154, 0.060434482991695404, -0.15996573865413666, 0.35270199179649353, 0.059757571667432785, 0.05214158818125725, -0.10956282913684845, 0.08523603528738022, 0.09222762286663055, 0.23760050535202026, 0.45864811539649963, -0.13005606830120087, -0.07129906117916107, -0.14942343533039093, -0.13133594393730164, 0.007722780108451843, 0.0007991878082975745, -0.28888261318206787, -0.010716096498072147, -0.006166643463075161, -0.007811620365828276, 0.016837378963828087, 0.30687108635902405, -0.022780979052186012, 0.09022034704685211, -0.1653306782245636, 0.025767019018530846, -0.3275972604751587, -0.02515736036002636, -0.12303368002176285, 0.22749954462051392, -0.10570114105939865, -0.08465633541345596, -0.19062335789203644, 0.07353625446557999, -0.04595739021897316, 0.01597566530108452, 0.07260342687368393, -0.04985975846648216, 0.07560208439826965, 0.023350823670625687, 0.1668185144662857, 0.0042588189244270325, -0.44224196672439575, -0.14742179214954376, -0.09168432652950287, -0.011721380054950714, -0.0782531127333641, 0.06714343279600143, -0.28941383957862854, -0.20249305665493011, -0.5962398052215576, -0.24353261291980743, -0.3320316672325134, -0.19549061357975006, 0.11782288551330566, -0.10133041441440582, -0.18916447460651398, -0.07551292330026627, -0.2650432586669922, 0.3736515939235687, 0.2566086947917938, 0.3979489207267761, 0.11607697606086731, -0.3836289644241333, -0.023628579452633858, -0.3141974210739136, -0.017488576471805573, -0.0769570842385292, -0.07342349737882614, 0.460212767124176, -0.15105240046977997, -0.26047399640083313, 0.16053959727287292, -0.0021966896019876003, -0.0141759617254138, -0.21014003455638885, 0.4624127447605133, -0.010963888838887215, -0.09105968475341797, -0.1547958105802536, -0.41898298263549805, -0.17006830871105194, 0.17132806777954102, 0.026366660371422768, -0.21862280368804932, 0.24191990494728088, -0.1502096951007843, 0.33763980865478516, 0.2905917763710022, -0.07227197289466858, -0.21672044694423676, 0.09479434043169022, 0.23984752595424652, 0.08658131957054138, -0.1786871999502182, 0.18172390758991241, -0.11898067593574524, -0.009966460056602955, 0.11576221883296967, 0.322247713804245, 0.13363614678382874, -0.1327153891324997, 0.09438621997833252, 0.29741913080215454, 0.20564715564250946, 0.290645033121109, 0.17771831154823303, 0.299516499042511, -0.040955040603876114, 0.08135386556386948, 0.1430491954088211, 0.04087193310260773, -0.17063745856285095, 0.01733073592185974, -0.21537701785564423, 0.4517916142940521, -0.1534687727689743, 0.211427241563797, 0.03729119151830673, -0.05485345795750618, -0.7386201024055481, -0.15270893275737762, -0.29499706625938416, 0.1840919703245163, -0.47490552067756653, -0.23340721428394318, -0.02577679418027401, -0.02488837018609047, -0.04651341214776039, 0.018093032762408257, 0.32397106289863586, -0.22663988173007965, 0.09236124902963638, -0.13974547386169434, -0.09552951902151108, 0.43852707743644714, -0.18295663595199585, -0.06260869652032852, 0.07069551199674606, -0.004181094001978636, -0.06293788552284241, 0.044189825654029846, 0.03868335485458374, -0.26629456877708435, 0.25035879015922546, 0.1458214968442917, 0.04212729632854462, 0.023359648883342743, -0.041290637105703354, -0.0921710729598999, 0.008263861760497093, 0.07590539753437042, 0.35673099756240845, -0.25089380145072937, 0.06105055660009384, 0.07803495973348618, 0.03665003553032875, 0.14278258383274078, 0.01897497847676277, -0.17199267446994781, -0.11113007366657257, -0.06512299925088882, 0.1565372794866562, -0.44632846117019653, 0.24833643436431885, 0.021955419331789017, 0.19567054510116577, 0.196846604347229, -0.11005938798189163, -0.1638423055410385, 0.018053673207759857, -0.13787205517292023, 0.26932260394096375, -0.28558120131492615, 0.11066703498363495, 0.15993651747703552, 0.05212641879916191, 0.16913358867168427, 0.18103469908237457, 0.025068266317248344, 0.15393979847431183, 0.05879276245832443, -0.06729494780302048, 0.0010361007880419493, 0.06233721971511841, 0.04117773845791817, 0.09833668917417526, -0.11759921908378601, -0.07490091025829315, 0.001557044917717576, 0.1282341480255127, 0.2601083219051361, 0.10610617697238922, -0.17607533931732178, -0.14792101085186005, 0.1650012880563736, -0.09311096370220184, 0.31026390194892883, -0.06761696189641953, 0.018268801271915436, -0.24268721044063568, 0.053611237555742264, -0.007862970232963562, 0.01961500570178032, -0.13163349032402039, 0.11056351661682129, 0.06328080594539642, 0.08017787337303162, 0.12236206233501434, -0.17589612305164337, 0.13683673739433289, -0.21042576432228088, -0.017741812393069267, 0.211137056350708, 0.2659670114517212, 0.32758015394210815, 0.10076908767223358, -0.043435655534267426, -0.12691469490528107, 0.036165717989206314, -0.15846893191337585, -0.03467061370611191, 0.05228034406900406, -0.009644455276429653, 0.08440051227807999, 0.018493328243494034, -0.09548968076705933, -0.12575359642505646, 0.22980837523937225, -0.0430716946721077, 0.13366925716400146, 0.1402987688779831, -0.1043364629149437, 0.31968268752098083, -0.06729606539011002, -0.015731098130345345, -0.16550320386886597, -0.11131904274225235, -0.17978402972221375, 0.17245347797870636, 0.17969340085983276, -0.2142248898744583, 0.0015751339960843325, 0.04113267734646797, -0.3207208514213562, 0.03349602594971657, 0.009213138371706009, 0.5168876051902771, -0.015989527106285095, -0.11518518626689911, 0.16042208671569824, 0.1642979234457016, -0.04716799035668373, -0.24239856004714966, 0.03342732787132263, 0.058942876756191254, -0.06914335489273071, -0.0014687097864225507, -0.10946554690599442, -0.0656384751200676, -0.004339572507888079, -0.04230587184429169, 0.05933418124914169, -0.018447641283273697, 0.17824473977088928, 0.06049065664410591, -0.2041754275560379, 0.14367952942848206, 0.0454786978662014, -0.009855614975094795, 0.3321116268634796, 0.1090690866112709, -0.07273244857788086, -0.25619906187057495, 0.11800507456064224, 0.029068896546959877, -0.10605604201555252, -0.0011353606823831797, 0.20365110039710999, 0.37405940890312195, -0.19419917464256287, 0.25840508937835693, 0.019048403948545456, 0.14244256913661957, 0.26984283328056335, 0.12813545763492584, 0.3270513117313385, 0.06800860166549683, 0.07739110291004181, -0.052277643233537674, -0.020640850067138672, 0.028295090422034264, -0.12239809334278107, -0.0008725697407498956, -0.039737045764923096, -0.18901939690113068, -0.08427686244249344, -0.10172750055789948, -0.10695343464612961, -0.27060186862945557, -0.10722105205059052, -0.12699827551841736, 0.1830994337797165, -0.18929770588874817, 0.16991332173347473, 0.25249797105789185, -0.1813810169696808, 0.17742440104484558, 0.027724314481019974, -0.03695269301533699, -0.07429037243127823, -0.17617088556289673, -0.11069665849208832, -0.07757894694805145, -0.24845586717128754, 0.26133105158805847, -0.18753543496131897, -0.3305147886276245, 0.09271807223558426, 0.13645923137664795, 0.18019939959049225, 0.022205563262104988, -0.11941399425268173, -0.3349270224571228, 0.2811878025531769, 0.18959559500217438, 0.11863388121128082, 0.20533092319965363, 0.03804006427526474, 0.06275992095470428, 0.117606982588768, 0.24264700710773468, -0.03653636574745178, 0.030294528231024742, 0.1685941368341446, -0.3148554563522339, 0.13098740577697754, -0.1892051100730896, 0.007654459215700626, 0.06915675848722458, 0.3566359281539917, 0.21704545617103577, 0.08245676755905151, -0.2794863283634186, -0.03175649791955948, -0.22697952389717102, -0.26998528838157654, -0.002330829156562686, 0.3191482126712799, 0.19102025032043457, 0.10222608596086502, -0.015162689611315727, 0.011346161365509033, -0.19301016628742218, 0.22689279913902283, -0.20824773609638214, -0.3078661859035492, 0.3066500425338745, 0.1830957680940628, -0.20830325782299042, -0.23090937733650208, -0.018618237227201462, 0.31972536444664, -0.06261307001113892, 0.18363584578037262, -0.2037985622882843, -0.020582519471645355, -0.15343701839447021, -0.26243916153907776, 0.05857975408434868, 0.1377590298652649, -0.12196557968854904, 0.08400792628526688, 0.2842298150062561, -0.026551829650998116, -0.13562530279159546, 0.3837624490261078, 0.22470588982105255, 0.44683918356895447, -0.2881314754486084, 0.07254405319690704, -0.2710779011249542, -0.03722197934985161, -0.18282832205295563, -0.2508501708507538, -0.06375350803136826, -0.23667754232883453, -0.28605762124061584, 0.29130032658576965, 0.11359100788831711, -0.0456508994102478, -0.1543923169374466, 0.030975285917520523, -0.11914981156587601, 0.040105924010276794, 0.12871462106704712, -0.2207823395729065, -0.18328355252742767, 0.047750458121299744, -0.11058038473129272, -0.1980806589126587, -0.10841214656829834, -0.24364592134952545, -0.39497289061546326, -0.041956499218940735, -0.3827124238014221, 0.02131829410791397, 0.04853487387299538, 0.08934736996889114, -0.15867726504802704, -0.1515766680240631, 0.05042014271020889, -0.14961062371730804, 0.3462497293949127, -0.023832358419895172, 0.1577335149049759, -0.049992822110652924, 0.05294649675488472, -0.0631125196814537, -0.11125396192073822, -0.1953946352005005, 0.19291646778583527, -0.09657973796129227, -0.1747582107782364, -0.0806991383433342, -0.2527956962585449, 0.06759783625602722, 0.07478605955839157, 0.024294527247548103, -0.15724831819534302, 0.18553070724010468, 0.05044882744550705, 0.24516434967517853, 0.08600732684135437, -0.09615261852741241, 0.10215535014867783, -0.009932461194694042, -0.10804019868373871, -0.06079552695155144, 0.032441429793834686, -0.02077634632587433, -0.06561648100614548, 0.08568114042282104, -0.012822762131690979, 0.07872112095355988, 0.15168794989585876, 0.10412949323654175, 0.10475058108568192, -0.06727573275566101, 0.4342615008354187, 0.7735426425933838, -0.39381489157676697, 0.08775566518306732, -0.9451476335525513, 0.06634816527366638, -0.19662703573703766, -0.002228603232651949, -0.07163888961076736, 0.12404009699821472, 0.018132394179701805, 5.2305374993011355e-05, -0.08696983009576797, -0.13966822624206543, 0.030047103762626648, -0.005246791988611221, 0.023668356239795685, -0.10112840682268143, -0.12082389742136002, 0.15279510617256165, 0.09513544291257858, 0.018014414235949516, 0.04185539484024048, 0.1655019074678421, -0.044589366763830185, 0.02763189934194088, 0.06694914400577545, 0.042365241795778275, 0.14320522546768188, -0.13343454897403717, 0.12124297767877579, 0.14814983308315277, 0.053660597652196884, -0.6458742618560791, -0.007230956573039293, 0.018257735297083855, 0.030282117426395416, 0.07920866459608078, 0.06561752408742905, -0.06891869008541107, 0.060090139508247375, 0.09096021950244904, -0.01404756959527731, 0.06864381581544876, 0.04227480664849281, 0.07118365913629532, -0.09418582916259766, 0.02032569982111454, -0.035750772804021835, -0.23872390389442444, -0.05845339223742485, -0.11404234915971756, 0.1200663223862648, -0.08905982971191406, 0.03603632003068924, -0.14492931962013245, 0.08023112267255783, 0.2089991420507431, -0.05232089012861252, -0.043904390186071396, 0.06107279285788536, 0.0637638047337532, 0.0019204977434128523, -0.2576158344745636, -0.14416418969631195, -0.148186594247818, -0.11998628079891205, 0.10502699017524719, 0.0434098094701767, 0.03541005775332451, 0.039263490587472916, 0.06366919726133347, -0.030157776549458504, -0.07235433161258698, 0.049883101135492325, -0.10710152238607407, -0.012174543924629688, 0.47886455059051514, -0.005508751142770052, 0.07247983664274216, 0.01963433064520359, 0.07236136496067047, -0.2692728042602539, -0.12171284854412079, 0.07632910460233688, -0.05965413898229599, -0.09065767377614975, -0.06634368747472763, 0.05514458566904068, 0.1272720992565155, -0.05999893322587013, -0.12549249827861786, 0.06822696328163147, 0.08644357323646545, 0.021737921983003616, -0.026653878390789032, 0.019462957978248596, 0.12533101439476013, -0.07462522387504578, 0.06951651722192764, 0.39692673087120056, -0.08229753375053406, 0.17805594205856323, -0.033408280462026596, 0.15816433727741241, -0.07385855913162231, -0.04659910127520561, -0.08384770900011063, -0.07779587060213089, -0.0024543108884245157, -0.10644493997097015, 0.279075562953949, 0.4448157250881195, 0.2413753718137741, -0.10562669485807419, -0.29696518182754517, -0.02765430137515068, -0.20102015137672424, 0.05629417672753334, -0.03736160695552826, 0.08754253387451172, 0.19290153682231903, 0.26622331142425537, 0.04994131997227669, -0.18386821448802948, -0.31805557012557983, -0.262546569108963, 0.001380058703944087, -0.16291069984436035, 0.016601048409938812, 0.019752262160182, 0.08591116219758987, -0.0773339495062828, 0.006933915428817272, 0.2857324481010437, -0.12750732898712158, 0.015009419061243534, -0.061741773039102554, 0.10266223549842834, 0.3361557126045227, -0.3112134635448456, -0.0027652792632579803, -0.22811837494373322, 0.19337160885334015, 0.10806993395090103, 0.2543012201786041, -0.015440858900547028, 0.12285181134939194, 0.07180260121822357, 0.21537820994853973, -0.08677259087562561, -0.22454126179218292, 0.11630585044622421, 0.18325938284397125, 0.05694489926099777, 0.3069542348384857, 0.1261984407901764, -0.080887570977211, -0.16903476417064667, 0.03645382076501846, 0.2513429820537567, -0.08904905617237091, -0.13386714458465576, 0.20182612538337708, 0.13566917181015015, -0.15733197331428528, 0.14763480424880981, 0.047484174370765686, -0.2120407074689865, -0.05326700955629349, 0.18244054913520813, -0.01719091832637787, -0.23524872958660126, -0.19726350903511047, -0.07854662835597992, -0.1132197231054306, -0.1322566717863083, 0.40109333395957947, 0.1372777670621872, 0.24459347128868103, 0.3369097411632538, -0.09819131344556808, -0.08129476010799408, -0.13050784170627594, 0.18944789469242096, -0.029383018612861633, 0.11190381646156311, -0.14982813596725464, 0.17512743175029755, -0.3139003813266754, -0.2280203253030777, 0.19276879727840424, -0.12691916525363922, 0.0397656075656414, -0.2228807508945465, -0.05739264935255051, 0.17768923938274384, 0.36593371629714966, -0.04200159013271332, -0.09971760213375092, 0.14997996389865875, -0.16377906501293182, -0.21574337780475616, 0.14467135071754456, 0.3821619153022766, 0.012284288182854652, 0.11083085834980011, -0.19842290878295898, -0.2631368041038513, 0.07892809063196182, 0.23151253163814545, 0.428700715303421, -0.12626753747463226, 0.35485726594924927, -0.0997893288731575, 0.08241686224937439, -0.15813830494880676, 0.11659616231918335, 0.20220720767974854, 0.034545354545116425, -0.38543829321861267, -0.21341462433338165, -0.14771638810634613, -0.09418562799692154, 0.1607084721326828, -0.12412361800670624, -0.3612488806247711, -0.07444407045841217, -0.17012359201908112, 0.17464162409305573, 0.18483960628509521, 0.25102880597114563, 0.16744345426559448, 0.02427206188440323, -0.12142761796712875, 0.01960504613816738, 0.14808149635791779, 0.19360578060150146, -0.10617628693580627, 0.06098782271146774, 0.1822073608636856, -0.10479559004306793, -0.08951868861913681, 0.008007162250578403, 0.015263708308339119, 0.1599733829498291, -0.28162625432014465, -0.006274350453168154, 0.007852531038224697, -0.186829075217247, -0.09663613140583038, 0.031126732006669044, -0.04740919917821884, 0.19897744059562683, -0.25650542974472046, 0.19596464931964874, -0.3043116331100464, -0.056434515863657, -0.18505991995334625, -0.08334361761808395, -0.08228003978729248, -0.2491482049226761, 0.05747823044657707, -0.055077582597732544, -0.03056747280061245, -0.0024138714652508497, 0.2726303040981293, 0.05337439849972725, 0.15261462330818176, -0.14971625804901123, -0.0626210942864418, 0.054818905889987946, -0.23235201835632324, -0.36724650859832764, -0.1764657199382782, 0.22391749918460846, 0.11961523443460464, -0.13446548581123352, -0.18545378744602203, -0.10838213562965393, -0.3852286636829376, 0.013119124807417393, 0.25387606024742126, -0.12729264795780182, 0.016611836850643158, 0.15721845626831055, -0.1517227590084076, -0.04220404103398323, -0.2773367464542389, -0.12379444390535355, 0.11247453093528748, -0.8311894536018372, -0.039962150156497955, -0.10246292501688004, -0.2874632179737091, 0.26906391978263855, -0.11172578483819962, -0.046047501266002655, -0.07398863881826401, -0.07539546489715576, 0.06182736158370972, 0.3688637614250183, -0.01688823476433754, 0.1988752782344818, 0.061306942254304886, -0.1599092185497284, 0.04876358062028885, -0.07359691709280014, -0.10895631462335587, 0.08214987069368362, -0.07695230096578598, -0.23934344947338104, -0.007935798726975918, -0.08044097572565079, 0.024158835411071777, 0.30691564083099365, 0.13721375167369843, -0.1486281305551529, 0.13807350397109985, -0.17255951464176178, 0.0004080449289176613, -0.11123033612966537, 0.010953894816339016, 0.055870186537504196, -0.18550600111484528, 0.2670731544494629, -0.020533178001642227, -0.029857439920306206, 0.06971980631351471, -0.016970621421933174, -0.18980523943901062, 0.29840192198753357, -0.19637200236320496, 0.00012531722313724458, 0.08136752992868423, -0.1715819090604782, 0.03867653012275696, -0.10284838825464249, 0.1868496984243393, 0.20219199359416962, 0.030864067375659943, -0.20197340846061707, -0.090098075568676, -0.30476123094558716, -0.08973674476146698, -0.0021243125665932894, -0.1897059828042984, 0.04716194048523903, -0.019107552245259285, -0.07244060188531876, -0.11452009528875351, 0.42902329564094543, 0.14630180597305298, -0.052456822246313095, -0.1154334545135498, -0.19018222391605377, -0.13113151490688324, -0.3149092197418213, 0.00039491651114076376, 0.1881452351808548, 0.14005549252033234, -0.035774193704128265, 0.29868030548095703, 0.1868852823972702, 0.03542425110936165, -0.0852549597620964, 0.010892662219703197, -0.03201563283801079, 0.08156856894493103, -0.06380346417427063, 0.29505205154418945, 0.11773500591516495, -0.042819369584321976, 0.20682235062122345, -0.19571921229362488, -0.40725719928741455, -0.07013742625713348, 0.28424495458602905, -0.10878512263298035, 0.0897129476070404, -0.09249887615442276, -0.35150715708732605, -0.14271007478237152, -0.1526104062795639, -0.16688770055770874, -0.16489402949810028, -0.16017146408557892, 0.15345610678195953, 0.03568115830421448, 0.43972548842430115, 0.002314239274710417, 0.09757032245397568, -0.18131688237190247, -0.0198819600045681, 0.33240753412246704, -0.05901550501585007, 0.2231145054101944, 0.0579981654882431, -0.18900614976882935, -0.011971183121204376, 0.035949915647506714, 0.31354522705078125, 0.008010602556169033, -0.2205256074666977, -0.17264769971370697, -0.08419138193130493, -0.5115481615066528, -0.08109121769666672, -0.13588617742061615, -0.061317577958106995, -0.015140109695494175, -0.15499380230903625, 0.19526627659797668, -0.15058165788650513, -0.012101423926651478, 0.10820239782333374, -0.13393832743167877, 0.09781023114919662, 0.13610471785068512, -0.43731120228767395, 0.5723857879638672, 0.06799265742301941, -0.07673181593418121, 0.01605408824980259, 0.09462794661521912, 0.16890814900398254, -0.029506715014576912, 0.16912543773651123, 0.09712598472833633, -0.4531165361404419, -0.16168810427188873, 0.36877351999282837, -0.1928660273551941, -0.06181928142905235, 0.0026436063926666975, 0.10530886799097061, -0.10509982705116272, 0.07578835636377335, -0.21114684641361237, -0.1570560336112976, 0.2895797789096832, -0.082480289041996, -0.4927862882614136, 0.14939475059509277, -0.134430393576622, -0.2517823278903961, 0.16933691501617432, 0.11042328178882599, -0.1115112453699112, 0.017463956028223038, -0.2798604369163513, 0.31144702434539795, -0.16586393117904663, 0.38585832715034485, -0.0903017595410347, -0.17300106585025787, 0.04186521843075752, -0.13713428378105164, -0.06102452054619789, 0.10030674189329147, 0.43205562233924866, -0.21157191693782806, 0.25601744651794434, -0.018471844494342804, -0.11742392927408218, -0.053276848047971725, 0.012858178466558456, 0.06977840512990952, 0.13196828961372375, 0.15101033449172974, -0.09899765998125076, -0.2638051509857178, -0.22372473776340485, -0.019011905416846275, -0.05344619229435921, -0.015527835115790367, -0.3739733397960663, -0.014842408709228039, 0.026477836072444916, -0.40412768721580505, 0.10242998600006104, 0.1361793428659439, -0.07063189148902893, 0.2285037338733673, 0.03420945256948471, -0.16484574973583221, 0.25406068563461304, -0.2511177957057953, -0.05239659547805786, -0.1199202835559845, -0.580592930316925, -0.318701833486557, -0.011429236270487309, -0.03038737177848816, 0.28138861060142517, -0.06178547069430351, -0.1967250555753708, 0.016430877149105072, 0.043546706438064575, -0.25430214405059814, -0.014130542054772377, 0.18957120180130005, 0.1875925362110138, 0.02316294051706791, -0.06455899775028229, -0.13705769181251526, -0.03590264916419983, -0.06256358325481415, -0.058750785887241364, 0.1729423701763153, -0.11045709252357483, -0.05644666403532028, 0.03750096261501312, 0.06295041739940643, 0.22818408906459808, -0.10153930634260178, -0.1269550323486328, -0.2694438695907593, -0.1169053390622139, 0.09490989148616791, 0.017181547358632088, 0.1412242352962494, -0.3753270208835602, 0.3078719973564148, -0.23888903856277466, -0.03811702877283096, 0.19218014180660248, 0.046116605401039124, 0.008463721722364426, 0.03990323469042778, -0.17241986095905304, -0.11917831003665924, -0.3640238642692566, 0.010325418785214424, 0.24481169879436493, -0.09465581923723221, 0.09721636772155762, 0.17229068279266357, -0.26873478293418884, -0.03688756749033928, 0.28897976875305176, -0.012168694287538528, 0.22800737619400024, -0.022777270525693893, 0.07042618095874786, 0.08806393295526505, -0.10114459693431854, 0.5267221927642822, -0.2712607979774475, -0.0897357389330864, -0.19296088814735413, 0.0596991628408432, 0.19487544894218445, -0.1692446917295456, -0.11656642705202103, -0.047995664179325104, -0.0420386977493763, -0.13165612518787384, -0.3888052999973297, 0.1265476495027542, 0.2827492356300354, 0.010054810903966427, -0.11478397995233536, -0.10666860640048981, 0.02332492545247078, -0.2125849574804306, -0.1978548914194107, 0.16889603435993195, 0.30255061388015747, -0.2790716886520386, 0.2678481340408325, 0.02858845889568329, -0.45105043053627014, 0.13664071261882782, -0.20961672067642212, 0.038287028670310974, 0.07856346666812897, 0.19974058866500854, 0.30419135093688965, -0.062348734587430954, 0.00554651441052556, -0.38801106810569763, 0.02145172841846943, -0.07163283228874207, -0.1904291957616806, -0.0994405597448349, 0.2521134316921234, -0.46972599625587463, -0.006917344871908426, 0.09466017782688141, 0.028635237365961075, 0.013672184199094772, 0.04652797803282738, -0.07884662598371506, -0.1632864624261856, 0.08835018426179886, 0.31391340494155884, 0.12689945101737976, -0.16984789073467255, 0.14798429608345032, 0.07013016939163208, 0.2004229873418808, -0.00101453997194767, -0.17717276513576508, 0.019077306613326073, -0.1718275099992752, -0.030513469129800797, -0.20118050277233124, 0.2682161033153534, 0.019000202417373657, -0.09260229766368866, -0.07389305531978607, 0.013326308690011501, -0.09414426982402802, -0.0674925446510315, -0.07050864398479462, 0.12900729477405548, 0.1891736388206482, 0.00998504739254713, -0.11681020259857178, -0.03531588241457939, -0.003113067476078868, -0.22909313440322876, -0.1636814922094345, -0.3164866864681244, 0.06552907079458237, 0.0487685464322567, -0.014010007493197918, -0.05857878178358078, -0.019300328567624092, 0.02734234370291233, -0.12706945836544037, 0.39443203806877136, -0.43796777725219727, 0.1835547536611557, -0.012304590083658695, 0.009207700379192829, -0.04637632146477699, -0.3109455108642578, -0.10990463197231293, -0.08645982295274734, 0.057603515684604645, 0.005693850573152304, 0.29589056968688965, -0.14865365624427795, 0.10998573154211044, 0.23007909953594208, -0.013219724409282207, 0.2301327884197235, -0.23953548073768616, 0.006501391064375639, 0.06464912742376328, -0.10645218193531036, -0.2748395502567291, 0.3597820997238159, 0.01134955883026123, -0.1645367443561554, -0.08713335543870926, 0.02871294878423214, -0.48021528124809265, 0.1642439067363739, 0.07534864544868469, -0.3801881968975067, -0.16399025917053223, -0.04374486953020096, -0.26773974299430847, 0.19894586503505707, 0.14993876218795776, 0.023983964696526527, 0.6805423498153687, -0.024181818589568138, -0.12863978743553162, -0.0589805543422699, -0.11021541804075241, 0.19464288651943207, 0.04305654391646385, 0.116993747651577, -0.023188522085547447, 0.10233761370182037, -0.02984669804573059, -0.2639322876930237, -0.1023363247513771, -0.025408653542399406, -0.15654310584068298, 0.06567888706922531, 0.07743097096681595, -0.2077551782131195, 0.08627129346132278, -0.10161212831735611, 0.19118355214595795, 0.1610935628414154, -0.09248006343841553, -0.09409182518720627, 0.22220705449581146, -0.19637435674667358, -0.2877681255340576, 0.2730310559272766, -0.1289658397436142, -0.1997348964214325, 0.1149405688047409, -0.34943392872810364, -0.1671905219554901, -0.15670357644557953, -0.066112220287323, 0.12290075421333313, -0.16435804963111877, 0.2456645667552948, -0.004191082436591387, 0.1483454555273056, -0.3723756670951843, -0.10248425602912903, 0.08095566928386688, -0.22714561223983765, 0.04663509130477905, -0.45387887954711914, 0.19465219974517822, -0.07841943204402924, 0.09367713332176208, -0.185503289103508, 0.2655564844608307, -0.018421228975057602, -0.19727948307991028, 0.0021983892656862736, -0.10401152074337006, 0.0677228793501854, -0.0406501330435276, 0.01103896088898182, 0.18407005071640015, -0.12274818122386932, -0.07724440097808838, 0.08374562859535217, -0.02279788628220558, 0.1112479418516159, 0.19049586355686188, -0.24984540045261383, 0.09908685088157654, -0.21496158838272095, -0.008048975840210915, -0.25237539410591125, -0.1760372519493103, -0.1432993859052658, -0.1672080010175705, -0.23220843076705933, 0.0030702475924044847, -0.04393092542886734, 0.08480721712112427, -0.18599732220172882, 0.46618497371673584, 0.062437504529953, -0.11304490268230438, 0.3375498652458191, -0.19693998992443085, 0.2303464561700821, -0.2298237681388855, -0.13795208930969238, 0.028950652107596397, -0.11018728464841843, -0.0328214056789875, 0.03831532597541809, -0.05470601096749306, -0.11606819182634354, -0.13534727692604065, -0.09255634993314743, -0.18595823645591736, 0.1029338464140892, -0.08567073196172714, 0.46817874908447266, -0.08668437600135803, -0.11725710332393646, 0.2651744484901428, 0.16879886388778687, 0.007863680832087994, 0.10917533934116364, 0.012055978178977966, -0.03916106000542641, 0.07016179710626602, -0.0069833798334002495, -0.2067844420671463, 0.13853326439857483, -0.007020671386271715, 0.06945941597223282, 0.0482441708445549, -0.17552407085895538, -0.19832223653793335, -0.034531258046627045, -0.04085797071456909, 0.005369128193706274, 0.0901038646697998, 0.12587429583072662, -0.073664590716362, 0.1530449539422989, -0.06424708664417267, 0.03438982367515564, -0.040009647607803345, -0.15507559478282928, 0.2587103843688965, -0.02582545392215252, -0.2624809443950653, -0.12741316854953766, -0.0006781683769077063, -0.116877980530262, -0.026795409619808197, -0.24008138477802277, -0.09626566618680954, 0.037121307104825974, 0.05478271469473839, 0.07420828938484192, 0.039638131856918335, -0.07549526542425156, -0.03870365768671036, 0.04304920881986618, 0.011279555968940258, 0.0890432670712471, -0.018028821796178818, -0.09278052300214767, -0.07872068881988525, -0.06836438924074173, 0.01383956428617239, 0.05571113899350166, -0.1328103244304657, -0.11447608470916748, -0.08237555623054504, 0.10641023516654968, 0.13092048466205597, 0.1279899924993515, -0.07453776895999908, -0.042522650212049484, -0.13827046751976013, -0.03290516138076782, 0.3765958547592163, 0.14213870465755463, -0.3853725790977478, -0.03987288475036621, -0.0976286306977272, -0.1782781034708023, 0.04569762945175171, -0.07450710237026215, 0.18677785992622375, 0.15148451924324036, -0.09455560147762299, 0.053553588688373566, 0.22873565554618835, -0.042445629835128784, -0.08466867357492447, 0.04988677427172661, 0.08455943316221237, 0.1801571100950241, -0.03156827762722969, 0.01829081028699875, -0.08702857047319412, 0.10306736081838608, 0.014824919402599335, -0.13371174037456512, -0.018379295244812965, 0.07224550098180771, 0.23215772211551666, -0.005045028869062662, -0.008863558061420918, -0.023453155532479286, 0.004254084546118975, 0.09079892188310623, 0.2073008120059967, -0.1494314968585968, 0.09622673690319061, -0.20143726468086243, 0.3356725871562958, -0.0720396488904953, -0.04349087178707123, 0.25566866993904114, 0.025016676634550095, 0.20432445406913757, 0.12718315422534943, -0.23048515617847443, -0.00389542686752975, 0.05702256038784981, -0.013891140930354595, 0.0673154965043068, 0.2921907305717468, 0.20794112980365753, -0.3503924608230591, 0.04818433150649071, 0.1267121583223343, 0.22430716454982758, -0.0035875425674021244, -0.18173445761203766, 0.174574613571167, 0.21338669955730438, 0.14782139658927917, -0.1030302345752716, 0.2492695152759552, 0.15967071056365967, 0.13232849538326263, -0.04872450605034828, -0.18075421452522278, -0.22215795516967773, -0.13772854208946228, 0.17284616827964783, -0.04987393319606781, -0.30702030658721924, -0.18021172285079956, 0.14082714915275574, -0.12535619735717773, 0.029431378468871117, -0.037553224712610245, 0.017863519489765167, 0.06660248339176178, -0.173358753323555, -0.057501986622810364, -0.07095082104206085, 0.023902662098407745, -0.09357795119285583, 0.2092302143573761, 0.007877248339354992, 0.08554385602474213, -0.05298260971903801, 0.14869947731494904, 0.09315219521522522, -0.04518268629908562, 0.13888195157051086, -0.038915686309337616, -0.172699972987175, 0.06155737489461899, 0.12109259516000748, -0.10626456141471863, 0.06203487887978554, -0.033270228654146194, 0.19841858744621277, -0.061500344425439835, 0.04593699425458908, 0.4449232220649719, -0.07297930866479874, 0.26346662640571594, -0.07851625978946686, 0.1899092197418213, 0.12728248536586761, 0.17430183291435242, -0.24246786534786224, -0.17644003033638, -0.10976040363311768, -0.1300463229417801, -0.36922138929367065, 0.07617493718862534, 0.046635646373033524, -0.12938004732131958, 0.11108427494764328, 0.020376138389110565, 0.08303873240947723, 0.038893431425094604, 0.1914326697587967, -0.11995140463113785, 0.07208713889122009, -0.06376779824495316, 0.05582679435610771, -0.055908650159835815, 0.07799273729324341, -0.20542892813682556, -0.017867200076580048, -0.06342975050210953, -0.23395588994026184, -0.1250699907541275, 0.029512397944927216, 0.07891674339771271, 0.1134457066655159, -0.19438709318637848, -0.006176903378218412, 0.03146001324057579, -0.16117766499519348, 0.03510135039687157, -0.06894485652446747, 0.07247419655323029, 0.046164385974407196, -0.2776109278202057, 0.12677909433841705, -0.14770132303237915, 0.028143400326371193, 0.1497574746608734, 0.12427996844053268, 0.23689308762550354, 0.034735266119241714, -0.13964904844760895, -0.01375651452690363, -0.22795772552490234, -0.03899145871400833, 0.3080044984817505, -0.03234737366437912, 0.16326141357421875, -0.32661446928977966, 0.12468865513801575, -0.11511672288179398, 0.13919514417648315, 0.15235638618469238, 0.01679358258843422, -0.16240085661411285, 0.10823491215705872, -0.18212905526161194, 0.10556820780038834, 0.22606199979782104, -0.08120987564325333, -0.05947774276137352, -0.19091062247753143, 0.5223425030708313, -0.06532607227563858, -0.13713401556015015, -0.14660434424877167, 0.011636702343821526, 0.05148821696639061, 0.09912282228469849, 0.19059155881404877, 0.15652334690093994, -0.1557455062866211, 0.08435703814029694, 0.05202265456318855, -0.004034074489027262, 0.007807064335793257, -0.17695914208889008, -0.05715329200029373, -0.009755883365869522, -0.06679981201887131, 0.05716356635093689, -0.10467865318059921, -0.03978719189763069, -0.027956781908869743, -0.1633898764848709, 0.2277502566576004, -0.07381900399923325, 0.36850157380104065, 0.15256915986537933, 0.07819673418998718, -0.17619647085666656, -0.07514400780200958, 0.05509762838482857, 0.0690084844827652, 0.19051378965377808, -0.009344948455691338, -0.04761891067028046, -0.05453896149992943, -0.0017849178984761238, 0.02724762260913849, 0.07943234592676163, -0.006756042595952749, -0.25194355845451355, 0.07235924899578094, 0.16280671954154968, -0.1107306256890297, -0.024569807574152946, 0.17366304993629456, -0.24779260158538818, -0.021832073107361794, -0.1164914220571518, -0.09114864468574524, 0.004517992027103901, -0.01840442232787609, -0.017995711416006088, 0.0634525790810585, -0.05460893735289574, -0.09589750319719315, -0.17375263571739197, -0.37326252460479736, -0.016801899299025536, 0.13061924278736115, -0.11710353195667267, -0.017683634534478188, -0.07357506453990936, -0.09622872620820999, -0.030466876924037933, 0.025152258574962616, 0.3198617100715637, -0.06604788452386856, -0.1023232713341713, -0.10827522724866867, 0.044950053095817566, -0.04258393868803978, 0.10248704999685287, -0.012297699227929115, -0.004221143666654825, -0.15736892819404602, 0.2353397160768509, -0.009373215027153492, 0.055244628340005875, 0.08675975352525711, 0.07385250180959702, 0.14892765879631042, 0.0603555366396904, -0.0462120957672596, -0.2749444246292114, -0.06035160273313522, 0.21883489191532135, -0.07911831140518188, 0.14869444072246552, -0.26111525297164917, -0.004851203411817551, 0.025131789967417717, -0.0878133624792099, -0.24253690242767334, 0.06545461714267731, -0.22630715370178223, 0.07470621168613434, 0.08206895738840103, -0.11469408869743347, -0.010909486562013626, -0.00741082988679409, 0.022810198366642, -0.0961390808224678, -0.0722467303276062, -0.17866834998130798, 0.022877832874655724, 0.27488595247268677, 0.04595503956079483, -0.08044932037591934, -0.14910157024860382, 0.020676057785749435, -0.1548715978860855, -0.03541262075304985, -0.12456769496202469, -0.02769470028579235, -0.12540438771247864, 0.02617405541241169, 0.017658228054642677, -0.06656729429960251, -0.024416308850049973, -0.06056573614478111, -0.08259303122758865, -0.08872901648283005, 0.07386492937803268, 0.051788076758384705, -0.11008402705192566, -0.1093486100435257, -0.005202902015298605, -0.04743845760822296, -0.028481628745794296, -0.03255084902048111, -0.002037478145211935, 0.07476931065320969, 0.11802779883146286, 0.05290960147976875, -0.030743643641471863, 0.025619812309741974, 0.10390745848417282, 0.1123250275850296, -0.1710122674703598, -0.038917917758226395, -0.03489626944065094, 0.15714962780475616, -0.09866037219762802, 0.09111248701810837, -0.027182532474398613, 0.11621928215026855, -0.04267500713467598, -0.29580703377723694, 0.1840820014476776, -0.25630703568458557, 0.27768629789352417, 0.10924983024597168, -0.007005535066127777, 0.12487740069627762, -0.03387054055929184, 0.010842131450772285, -0.06440116465091705, 0.11431757360696793, -0.019304124638438225, 0.030904749408364296, 0.20225071907043457, -0.1239597424864769, 0.2867198884487152, -0.011044137179851532, 0.1004306897521019, 0.06884611397981644, 0.002312994794920087, -0.05004395171999931, 0.04518980532884598, -0.1395770013332367, -0.02936609834432602, -0.05718256160616875, 0.008828628808259964, -0.044514644891023636, 0.23903299868106842, -0.013225779868662357, -0.14672841131687164, 0.1478138417005539, 0.004529817495495081, -0.05695329234004021, 0.04354355111718178, 0.12150304019451141, 0.07585253566503525, -6.601437053177506e-05, 0.008182692341506481, -0.0975748673081398, 0.09458091109991074, 0.04961083084344864, 0.11524691432714462, 0.0853348821401596, 0.002021088730543852, 0.008528429083526134, -0.2812804877758026, 0.19110660254955292, -0.023533612489700317, 0.12488697469234467, 0.1137029230594635, -0.035337768495082855, -0.005072585307061672, -0.05533948913216591, 0.03281429037451744, -0.16481183469295502, 0.0035310471430420876, -0.0004063126689288765, 0.08878110349178314, -0.16802804172039032, 0.13257552683353424, 0.08077851682901382, -0.14642375707626343, -0.026837201789021492, -0.13565395772457123, -0.03137169033288956, 0.13477231562137604, 0.08449259400367737, 0.06627939641475677, 0.17970573902130127, 0.1074388399720192, -0.004007113166153431, 0.06118793785572052, -0.028617829084396362, -0.02387252263724804, 0.06858206540346146, 0.06100478023290634, 0.12095562368631363, 0.15655691921710968, 0.008759180083870888, 0.03617474436759949, -0.006725894287228584, 0.036669567227363586, 0.04620537906885147, -0.01822717860341072, -0.0641334280371666, 0.20763829350471497, 0.22828537225723267, -0.10446968674659729, 0.15771031379699707, 0.2719562351703644, -0.18392403423786163, -0.004970922134816647, -0.2546135187149048, -0.09384453296661377, -0.07925450056791306, 0.12351512908935547, 0.17330054938793182, 0.04527122899889946, 0.15865953266620636, 0.05322273075580597, 0.15632250905036926, 0.1450827568769455, 0.0011076569790020585, -0.06681232154369354, -0.0056634144857525826, 0.05304038152098656, 0.08587755262851715, 0.19067712128162384, -0.024532075971364975, -0.07698863744735718, 0.053094953298568726, 0.1818838268518448, 0.09078700095415115, 0.07250393182039261, -0.2826020121574402, 0.01511483546346426, -0.014088493771851063, 0.43343180418014526, -0.117021344602108, 0.05446530133485794, 0.21783863008022308, 0.27927902340888977, -0.027414461597800255, 0.13494613766670227, -0.013917318545281887, -0.36917930841445923, 0.1718427836894989, 0.21841168403625488, 0.041462771594524384, 0.08615709096193314, -0.1263473480939865, -0.10593470931053162, 0.13274028897285461, 0.06869111210107803, -0.02703789435327053, -0.2445794641971588, -0.20084013044834137, 0.21695257723331451, -0.25770890712738037, -0.313811719417572, -0.05963730067014694, -0.22258619964122772, -0.1707935631275177, -0.0022964731324464083, 0.08119282126426697, 0.12269159406423569, -0.25998544692993164, -0.003769592149183154, -0.027570180594921112, 0.10656167566776276, 0.08047202974557877, -0.05324524641036987, 0.12060144543647766, -0.24916470050811768, 0.383475661277771, 0.2786642014980316, 0.2525811195373535, -0.14549916982650757, -0.1889418363571167, -0.02123638615012169, -0.17367617785930634, 0.32398054003715515, 0.17221993207931519, -0.09150101989507675, -0.08354994654655457, -0.17496320605278015, -0.2703644335269928, -0.1291118860244751, 0.3400689363479614, 0.24400575459003448, 0.29089346528053284, 0.34723973274230957, -0.313209593296051, 0.17798371613025665, -0.1196272075176239, 0.12166796624660492, 0.018187018111348152, 0.09992767870426178, 0.2525581121444702, 0.2594887316226959, 0.0021986139472573996, 0.05689588189125061, 0.0354289673268795, -0.07126406580209732, -0.28578561544418335, 0.057867493480443954, -0.29409483075141907, 0.13359537720680237, -0.2092636078596115, -0.18588809669017792, -0.11843272298574448, -0.3192119896411896, 0.100601926445961, 0.1574125587940216, -0.24981950223445892, -0.0622856430709362, -0.04330885037779808, -0.049326371401548386, -0.14303778111934662, 0.02502232976257801, -0.2162841558456421, -0.16490909457206726, -0.21059618890285492, 0.2942025363445282, -0.09133032709360123, -0.13832519948482513, -0.14293594658374786, 0.13109855353832245, -0.05318068340420723, -0.4043401777744293, 0.2437882423400879, -0.05042484402656555, 0.19027329981327057, 0.029078733175992966, -0.002928357571363449, 0.4158670902252197, -0.24976582825183868, -0.23264707624912262, -0.12532402575016022, 0.2692311704158783, -0.20013025403022766, -0.11601462960243225, -0.29164761304855347, 0.048908431082963943, 0.26747289299964905, -0.10516973584890366, 0.1545548439025879, -0.2183847278356552, -0.35407158732414246, -0.23153458535671234, -0.33781862258911133, -0.18100816011428833, -0.19053471088409424, 0.09000984579324722, -0.16639910638332367, -0.16835254430770874, -0.03941246122121811, 0.10110334306955338, -0.14833928644657135, 0.17689131200313568, 0.11040809005498886, 0.11297569423913956, 0.031398922204971313, -0.03280012309551239, -0.1269971877336502, 0.030820012092590332, -0.2714950144290924, 0.3612651824951172, 0.10753048211336136, -0.06686794012784958, -0.29847633838653564, -0.0029507542494684458, 0.2680470943450928, 0.051059648394584656, 0.06926729530096054, -0.18001830577850342, -0.1889990121126175, -0.08022002130746841, 0.2428753525018692, 0.11907035857439041, -0.07554513216018677, -0.05907599627971649, -0.04414231330156326, 0.3250010013580322, 0.029772130772471428, -0.32171744108200073, 0.1524723917245865, -0.17124785482883453, -0.20098672807216644, 0.0514727421104908, -0.14789637923240662, -0.3233632743358612, -0.0793161392211914, 0.07337084412574768, 0.1847303956747055, 0.15383648872375488, -0.020228490233421326, 0.21897758543491364, -0.004162172321230173, -0.0014475862262770534, -0.18777503073215485, 0.1480291187763214, -0.13909414410591125, 0.08481836318969727, 0.4574589133262634, 0.12518128752708435, -0.44699040055274963, -0.11079727858304977, -0.33694595098495483, 0.17497707903385162, -0.10524280369281769, -0.1797495037317276, -0.025500094518065453, 0.028000567108392715, 0.26030853390693665, 0.4137469530105591, -0.28234127163887024, 0.18869297206401825, 0.04911324009299278, 0.06585682928562164, -0.12515567243099213, -0.5015698671340942, -0.08792296051979065, 0.12379244714975357, 0.10427319258451462, -0.2832667827606201, -0.058421242982149124, -0.01669224537909031, 0.21772989630699158, -0.15317276120185852, 0.1421818733215332, 0.07079209387302399, 0.1547044962644577, -0.08306032419204712, 0.243233323097229, -0.03193202242255211, 0.07692308723926544, 0.004567475523799658, 0.10187722742557526, 0.26100796461105347, -0.15032736957073212, 0.22630806267261505, -0.054978564381599426, -0.4086240828037262, -0.003194053191691637, -0.01347518153488636, -0.06621890515089035, 0.00042590676457621157, -0.052187882363796234, 0.08952412009239197, 0.11475339531898499, 0.2700295150279999, -0.04352909326553345, -0.004712243564426899, 0.5413596630096436, -0.21354317665100098, -0.04396688565611839, -0.13755132257938385, -0.08687866479158401, -0.47197824716567993, -0.20664049685001373, -0.14886391162872314, 0.2008320838212967, 0.16599275171756744, -0.17910417914390564, -0.16476671397686005, -0.02582044154405594, -0.18836212158203125, -0.003565986640751362, 0.5276370644569397, 0.2004884034395218, -0.01370899099856615, 0.15248775482177734, -0.24978865683078766, 0.056269291788339615, -0.16129237413406372, -0.019353022798895836, -0.13636431097984314, 0.1544792354106903, -0.10382235795259476, 0.11685768514871597, -0.0042183357290923595, 0.04731873795390129, 0.3536287844181061, -0.015330927446484566, 0.19885696470737457, -0.10473764687776566, 0.298184335231781, 0.18321049213409424, -0.1827847808599472, 0.27222034335136414, -0.021422557532787323, -0.04986879602074623, 0.611374020576477, -0.0443325936794281, -0.43885618448257446, 0.18089184165000916, -0.11800859868526459, -0.2626766860485077, -0.20975445210933685, -0.146207794547081, 0.03587111458182335, -0.36922168731689453, -0.0007852681446820498, 0.15896177291870117, -0.06246486306190491, 0.012642746791243553, 0.022526366636157036, 0.020352492108941078, -0.07198775559663773, 0.22212009131908417, 0.048229802399873734, 0.1762155294418335, 0.022005029022693634, -0.4114515483379364, 0.22264690697193146, -0.3172805607318878, -0.44271236658096313, -0.06917569786310196, 0.3848152458667755, -0.12133529782295227, -0.09677466750144958, -0.3267131745815277, -0.6351008415222168, -0.10997108370065689, -0.25914835929870605, -0.35880735516548157, 0.13826517760753632, -0.02764020673930645, -0.1579355150461197, -0.1267230361700058, 0.5322139859199524, -0.015613499097526073, 0.4396815299987793, -0.27410781383514404, 0.18930228054523468, 0.26148590445518494, 0.1586175113916397, -0.07570958137512207, 0.15811589360237122, 0.025793619453907013, -0.24285224080085754, -0.0810522586107254, -0.05947287380695343, -0.422621488571167, 0.020027965307235718, -0.18878783285617828, -0.06493876129388809, 0.23478901386260986, -0.014891061000525951, 0.0870894268155098, 0.20718686282634735, 0.3358985483646393, 0.18004919588565826, 0.09568575024604797, 0.10345032066106796, -0.036876920610666275, 0.16928015649318695, 0.052408937364816666, -0.19521406292915344, 0.11784385144710541, 0.1506468802690506, -0.0715358704328537, 0.37825900316238403, -0.13733378052711487, 0.1690475195646286, 0.006675320211797953, 0.1146814301609993, -0.05275261029601097, 0.15023307502269745, -0.12248539924621582, 0.15594837069511414, -0.15241560339927673, -0.10240064561367035, -0.10421247035264969, 0.1430019736289978, 0.09956861287355423, 0.2090366929769516, -0.3261042535305023, -0.27144479751586914, -0.09959673136472702, -0.3195396661758423, 0.07061731815338135, -0.15997172892093658, 0.0783386379480362, 0.3418820798397064, 0.07041698694229126, 0.12518058717250824, 0.08832881599664688, -0.03446989506483078, -0.08208031952381134, -0.015592094510793686, 0.0889662653207779, 0.2257537543773651, 0.13785454630851746, -0.18758141994476318, 0.41227346658706665, -0.06342494487762451, -0.024884596467018127, 0.09280204772949219, -0.1177094578742981, 0.14502066373825073, 0.3547247350215912, 0.041038356721401215, 0.06577083468437195, 0.19167475402355194, -0.3074219226837158, -0.14703963696956635, -0.3340473473072052, 0.05912119150161743, -0.388243168592453, -0.019640028476715088, -0.14778675138950348, 0.12056883424520493, 0.19203072786331177, -0.12043756246566772, 0.13331250846385956, -0.08159951120615005, 0.27032116055488586, 0.21341170370578766, -0.0552363246679306, 0.18191629648208618, 0.10712938755750656, -0.048472777009010315, -0.1367524117231369, -0.16418182849884033, 0.10568719357252121, -0.007376351859420538, -0.013415411114692688, 0.17285491526126862, 0.3584305942058563, -0.0766119584441185, -0.35182592272758484, 0.08189058303833008, 0.339897483587265, -0.027079081162810326, 0.8563097715377808, 0.12336724996566772, 0.3885590136051178, -0.12694641947746277, -0.017422111704945564, 0.05593889579176903, 0.18411387503147125, 0.024853868409991264, -0.1773994117975235, 0.21956129372119904, 0.15690010786056519, 0.006248695310205221, 0.26494717597961426, -0.1107172816991806, 0.0014779036864638329, -0.11864716559648514, -0.14887069165706635, -0.19000661373138428, -0.14175622165203094, 0.14440706372261047, 0.21573062241077423, -0.1282888799905777, -0.07858052849769592, -0.15547847747802734, -0.08103970438241959, -0.15926691889762878, 0.19604234397411346, 0.01766270399093628, -0.15917128324508667, -0.2601466774940491, -0.07866779714822769, 0.006241487339138985, 0.10826443135738373, -0.07796460390090942, -0.2659032642841339, -0.005276007577776909, 0.06730026006698608, -0.04418511316180229, 0.03543492779135704, -0.02655072882771492, -0.14065039157867432, -0.07067606598138809, -0.05332226678729057, 0.12401464581489563, 0.0529867559671402, -0.011669078841805458, -0.23584450781345367, -0.006186653859913349, -0.07717538625001907, -0.050320301204919815, 0.09652527421712875, 0.5894883871078491, -0.09390639513731003, -0.1692958027124405, -0.11937572062015533, 0.08772969245910645, 0.04666229709982872, -0.04302852973341942, -0.12994389235973358, -0.1400427669286728, -0.12416099011898041, -0.18976910412311554, -0.006815585773438215, 0.2958618998527527, -0.14985163509845734, -0.17104949057102203, -0.4533230662345886, -0.07690892368555069, 0.28291234374046326, 0.06484267115592957, -0.1407930701971054, -0.20672817528247833, 0.043272506445646286, 0.13114607334136963, 0.28272438049316406, -0.08383898437023163, 0.022316500544548035, -0.052722666412591934, 0.02974950149655342, -0.02144469879567623, 0.08625233173370361, -0.15012305974960327, -0.09304876625537872, 0.005511240102350712, -0.0651618018746376, 0.16657975316047668, -0.21100473403930664, 0.6515275835990906, -0.12007922679185867, -0.11420617997646332, -0.08777767419815063, 0.1930837780237198, -0.0202640388160944, -0.01500941812992096, -0.02099115215241909, 0.052905499935150146, 0.08042396605014801, -0.03309795632958412, 0.3886629343032837, -0.2854911684989929, -0.01036670058965683, 0.12601490318775177, -0.07150084525346756, -0.0852770209312439, 0.23788727819919586, -0.05101115256547928, -0.03884994611144066, -0.24977529048919678, -0.26068127155303955, 0.0032717238646000624, -0.2538166046142578, 0.09211698919534683, -0.12821327149868011, -0.02981259860098362, -0.12240616232156754, 0.11733526736497879, -0.09948710352182388, 0.10161726921796799, 0.040739160031080246, -0.3636193871498108, 0.19829383492469788, -0.22677110135555267, -0.4515935778617859, -0.16832830011844635, 0.03503720834851265, 0.010697690770030022, -0.14109644293785095, 0.2307884842157364, -0.43617531657218933, -0.08182599395513535, -0.09055907279253006, 0.06202878803014755, 0.11618349701166153, -0.3489673137664795, -0.10129605233669281, -0.11221886426210403, -0.04554642364382744, 0.19020403921604156, 0.2145882397890091, -0.15888603031635284, 0.11373137682676315, -0.02094038762152195, -0.03444847837090492, 0.1963557004928589, 0.05817773938179016, -0.08555538952350616, -0.057468004524707794, -0.09477376937866211, -0.06621553748846054, -0.16912515461444855, 0.43344762921333313, 0.025805961340665817, 0.025906497612595558, 0.15792521834373474, -0.05808527395129204, -0.17462463676929474, 0.30726730823516846, 0.08318518102169037, -0.14138589799404144, -0.4223954975605011, -0.01621137373149395, 0.033731859177351, -0.39756885170936584, -0.06947020441293716, 0.2070552557706833, 0.32473433017730713, 0.2021333873271942, 0.3167891502380371, -0.29033634066581726, 0.036545343697071075, -0.018665967509150505, -0.2599996030330658, 0.16697578132152557, 0.2626288831233978, 0.046869322657585144, -0.015452565625309944, 0.04561642184853554, -0.1285485029220581, -0.14182502031326294, 0.13638797402381897, -0.384490042924881, -0.07836739718914032, 0.35203027725219727, 0.2872980237007141, -0.1346883326768875, 0.13470132648944855, 0.3485013544559479, -0.2701190412044525, -0.20643174648284912, 0.16472162306308746, 0.35832998156547546, 0.18943101167678833, 0.008889008313417435, -0.343278169631958, 0.12007046490907669, 0.586975634098053, -0.5246462225914001, -0.02540457993745804, 0.04655808210372925, 0.00115435931365937, 0.025835571810603142, -0.023786934092640877, 0.02671908773481846, 0.278698205947876, 0.17755788564682007, 0.005098520312458277, 0.00388847547583282, 0.2850298285484314, 0.2696749269962311, -0.3328305184841156, -0.43698519468307495, 0.11221041530370712, 0.2700546383857727, -0.5602266788482666, -0.09424805641174316, -0.012352335266768932, 0.11476881057024002, -0.1976410150527954, 0.2159276008605957, -0.2020374983549118, -0.1637997180223465, 0.26612067222595215, -0.24584980309009552, 0.028661446645855904, -0.37983953952789307, -0.19260816276073456, 0.1150573194026947, -0.116303451359272, -0.03564366698265076, -0.025838449597358704, -0.5331891179084778, -0.06824584305286407, 0.04833357781171799, -0.07208120077848434, -0.3217582106590271, 0.032678332179784775, -0.028415191918611526, 0.18250806629657745, 0.015077631920576096, 0.043938394635915756, 0.1435517817735672, -0.1431121528148651, -0.015222808346152306, 0.15508174896240234, -0.14274108409881592, 0.18793179094791412, 0.1260121911764145, 0.26886269450187683, -0.17099641263484955, 0.0008060755208134651, -0.1417563408613205, -0.2683781087398529, 0.41266071796417236, -0.13252124190330505, -0.190414696931839, 0.14826293289661407, 0.0711263120174408, -0.05906887352466583, 0.038591258227825165, -0.24435314536094666, -0.11738676577806473, 0.12600496411323547, -0.14172206819057465, 0.08288358896970749, 0.06608042120933533, 0.08477041870355606, -0.23730851709842682, -0.1853201538324356, 0.045895956456661224, -0.1912011057138443, -0.05565781518816948, -0.20676551759243011, 0.039117563515901566, 0.12017069011926651, 0.3495047688484192, -0.1264570653438568, 0.012520188465714455, 0.6159347891807556, -0.09840017557144165, 0.08356347680091858, -0.18736658990383148, -0.4379688501358032, 0.08493292331695557, -0.09262154251337051, -0.1029924526810646, -0.38040629029273987, 0.3502119481563568, -0.1261393278837204, -0.16122643649578094, -0.2175915539264679, 0.18859145045280457, -0.15025582909584045, -0.3437997102737427, -0.11024704575538635, 0.09967022389173508, -0.20441922545433044, 0.2512427866458893, 0.017599089071154594, -0.0006864414317533374, 0.40816450119018555, -0.09414201974868774, 0.12897101044654846, 0.09047620743513107, 0.009938787668943405, 0.02164199948310852, 0.2993537485599518, -0.3488123416900635, -0.18487781286239624, -0.005778370890766382, 0.08116195350885391, 0.10342774540185928, -0.10966614633798599, -0.04986818879842758, 0.31115490198135376, -0.001972152618691325, 0.06892088055610657, 0.03191622346639633, 0.1312706023454666, -0.09982290118932724, -0.3308154344558716, -0.08980865031480789, 0.2687707841396332, 0.16810466349124908, -0.25914543867111206, -0.0869966372847557, -0.23823775351047516, 0.160483255982399, -0.2931312620639801, -0.03074301779270172, -0.06405728310346603, 0.01835966482758522, -0.21994023025035858, 0.10446777939796448, -0.07435460388660431, 0.34647268056869507, -0.2956797778606415, -0.5708411335945129, 0.17089955508708954, 0.07433515787124634, -0.14434003829956055, -0.30742132663726807, -0.2864662706851959, 0.44793230295181274, -0.05375628173351288, 0.0014037733199074864, -0.16380192339420319, 0.2222178429365158, -0.20235535502433777, 0.11878322809934616, 0.11528552323579788, -0.0040357052348554134, -0.31942203640937805, 0.019359348341822624, 0.15304163098335266, 0.03651663661003113, 0.14743737876415253, -0.22004447877407074, -0.05588369071483612, -0.02738047018647194, -0.2172354757785797, -0.18146538734436035, 0.0521334670484066, 0.2577657401561737, 0.1623324304819107, 0.44189953804016113, -0.1331787407398224, -0.22573520243167877, -0.25871533155441284, -0.07360488176345825, -0.24763686954975128, 0.4181251525878906, 0.021698536351323128, -0.24070985615253448, -0.06370556354522705, 0.054137494415044785, 0.01403417531400919, 0.11525196582078934, -0.0016895267181098461, 0.3579654097557068, -0.06923721730709076, -0.2795218229293823, -0.07555652409791946, 0.02957000955939293, -0.018069593235850334, 0.039630476385354996, 0.03413323312997818, 0.18377797305583954, -0.04314521327614784, -0.04445749893784523, -0.09903529286384583, -0.04318293556571007, 0.14383944869041443, -0.23312775790691376, -0.1835489273071289, -0.12705445289611816, 0.040560994297266006, -0.4102913439273834, 0.3784621059894562, -0.13342171907424927, 0.12242750078439713, 0.4967733919620514, -0.04725772887468338, 0.27888360619544983, -0.041625868529081345, -0.2129732072353363, -0.14300556480884552, -0.2221420854330063, 0.1799653321504593, -0.0032550112809985876, 0.01026905421167612, -0.0697261318564415, -0.29195502400398254, -0.1293298900127411, -0.13953737914562225, 0.06297214329242706, -0.09032248705625534, 0.06167345121502876, 0.01027001067996025, 0.09612926095724106, 0.19602791965007782, -0.09853912144899368, 0.04388326033949852, -0.022030269727110863, 0.0927044227719307, -0.034856341779232025, 0.038125086575746536, 0.08140058070421219, -0.08443152159452438, -0.02426244132220745, -0.1330346167087555, 0.197379931807518, 0.24180187284946442, 0.4061494469642639, 0.12976907193660736, -0.12153846770524979, -0.17356136441230774, -0.2736702263355255, 0.04219783842563629, -0.31062018871307373, 0.169934943318367, -0.13457386195659637, 0.06460624933242798, 0.013453966937959194, -0.08916988968849182, 0.6848456263542175, -0.10913548618555069, -0.2711806893348694, 0.19194786250591278, -0.13378645479679108, 0.06308826059103012, 0.32025137543678284, 0.17568036913871765, 0.07963830977678299, -0.01772867701947689, 0.18096108734607697, -0.28002166748046875, 0.18057362735271454, -0.19083446264266968, 0.1411825716495514, -0.23657208681106567, 0.03746161237359047, -0.10224798321723938, -0.510613203048706, -0.12454092502593994, 0.09203441441059113, 0.31082963943481445, 0.12485761195421219, -0.03833708167076111, -0.01218700222671032, 0.07417968660593033, -0.06547465175390244, -0.02299550175666809, 0.09675043821334839, -0.08750477433204651, 0.03017348051071167, -0.22560958564281464, 0.030684588477015495, 0.12421716004610062, -0.19966959953308105, 0.09385712444782257, -0.38158971071243286, -0.13307397067546844, 0.2278088480234146, 0.16058364510536194, 0.12333259731531143, 0.36120298504829407, -0.28511402010917664, 0.061221566051244736, -0.4145127534866333, -0.20731088519096375, -0.09634844958782196, -0.36445486545562744, 0.026663655415177345, -0.06798801571130753, -0.02843424119055271, -0.22901281714439392, 0.1780579835176468, 0.29414424300193787, 0.27858150005340576, -0.11968562006950378, -0.15206336975097656, 0.15078286826610565, -0.14079439640045166, 0.2105780988931656, -0.32516971230506897, 0.11576727777719498, 0.22167469561100006, -0.023202890530228615, -0.09831396490335464, -0.009609241038560867, -0.13141760230064392, 0.09586143493652344, 0.21503901481628418, -0.055850800126791, 0.013330980204045773, -0.0692705437541008, -0.045680657029151917, -0.033935338258743286, -0.14710156619548798, 0.39087793231010437, -0.263102650642395, -0.30607256293296814, 0.12936919927597046, 0.0944766029715538, 0.28965306282043457, -0.24063225090503693, 0.5207657814025879, 0.014131259173154831, -0.4482070803642273, -0.13683024048805237, -0.10678844153881073, 0.06848707050085068, -0.08954411000013351, 0.37413302063941956, 0.2469041347503662, -0.060421187430620193, 0.19183620810508728, 0.10014864057302475, -0.09383387118577957, -0.20214134454727173, -0.30952003598213196, -0.11729960888624191, -0.14721234142780304, -0.07301045209169388, -0.3143279254436493, -0.01715974323451519, -0.17913977801799774, 0.17843787372112274, -0.4127010703086853, -0.2303917557001114, 0.28647708892822266, -0.06349427998065948, 0.040675316005945206, 0.03261294215917587, 0.1690438687801361, 0.20309822261333466, 0.11940290778875351, 0.11109763383865356, 0.11516431719064713, -0.2313864678144455, -0.2489839494228363, -0.0954744964838028, -0.053620532155036926, -0.23880569636821747, -0.018859615549445152, 0.49597442150115967, -0.47622746229171753, 0.006733268033713102, -0.1368120163679123, -0.1959581822156906, 0.13369959592819214, -0.20543751120567322, -0.5050187706947327, -0.14012117683887482, -0.17697618901729584, -0.1256110519170761, 0.006631783675402403, 0.04763323813676834, 0.050284117460250854, 0.04084666445851326, 0.21129928529262543, 0.2052631378173828, -0.06067653000354767, 0.036022137850522995, 0.12742531299591064, 0.3249466121196747, -0.059312306344509125, 0.27674436569213867, -0.13197468221187592, -0.16321291029453278, -0.39553236961364746, -0.042332857847213745, -0.32713019847869873, 0.18052759766578674, -0.15163369476795197, -0.19612084329128265, 0.12783002853393555, -0.16092319786548615, -0.08664439618587494, 0.3939891755580902, -0.17327150702476501, 0.389723539352417, -0.0032549339812248945, -0.30481645464897156, -0.01427646353840828, 0.2150764763355255, -0.3071742653846741, -0.021919144317507744, -0.2972713112831116, -0.25615745782852173, 0.0892670750617981, 0.21237856149673462, -0.13970261812210083, 0.15086638927459717, 0.07824556529521942, -0.09482258558273315, -0.26522397994995117, -0.2540740370750427, 0.06540857255458832, -0.27922922372817993, 0.028283897787332535, 0.11069373041391373, 0.013767966069281101, 0.06888338923454285, -0.3561941981315613, 0.03120356611907482, -0.10415900498628616, 0.3968033492565155, -0.285341739654541, -0.29631683230400085, 0.2062201350927353, 0.24094247817993164, -0.22198374569416046, 0.09091474115848541, -0.1967732161283493, -0.0907280296087265, 0.03581666946411133, 0.2735712230205536, 0.24477289617061615, -0.012228461913764477, -0.03659448400139809, -0.09301907569169998, 0.05452913045883179, -0.1486312299966812, 0.0565958246588707, 0.17781759798526764, -0.010867221280932426, -0.03289185091853142, 0.06316400319337845, -0.3274303674697876, -0.36835968494415283, -0.46981966495513916, 0.26306140422821045, 0.39144736528396606, -0.13963231444358826, -0.033791542053222656, -0.07127218693494797, -0.07199224829673767, -0.056909605860710144, -0.13036476075649261, 0.34440377354621887, -0.5582854151725769, -0.042269837111234665, -0.05228043347597122, 0.34985604882240295, 0.02411181479692459, -0.12702137231826782, 0.06955357640981674, 0.33542266488075256, 0.027753138914704323, 0.26503297686576843, 0.3151493966579437, 0.3827400207519531, -0.027536379173398018, -0.005109972786158323, -0.38007909059524536, -0.011003738269209862, -0.1467798948287964, 0.03982172906398773, 0.14985422790050507, 0.1399308741092682, -0.04100614786148071, -0.006588758900761604, 0.1410306692123413, -0.14959992468357086, -0.061577558517456055, -0.30746275186538696, -0.09679991006851196, -0.17255465686321259, -0.0883534625172615, -0.07309877127408981, -0.05720973387360573, -0.06906664371490479, 0.1492079645395279, -0.018962662667036057, -0.1639373004436493, 0.33066892623901367, -0.10051332414150238, 0.0099646570160985, -0.15978579223155975, 0.002132085617631674, 0.04892678186297417, 0.11519217491149902, -0.016240006312727928, 0.05736915394663811, -0.05334843322634697, 0.2247796505689621, -0.39883771538734436, -0.21072562038898468, 0.1804276406764984, 0.07431810349225998, -0.02661256492137909, -0.4744316041469574, 0.20604650676250458, 0.15509116649627686, 0.28770899772644043, -0.711170494556427, -0.12612076103687286, 0.15840964019298553, -0.07981326431035995, 0.6063130497932434, 0.21325109899044037, 0.37793341279029846, -0.06685582548379898, -0.2958473563194275, 0.09372108429670334, 0.03629238158464432, -0.08251781016588211, 0.2333262413740158, 0.18397416174411774, -0.08012893050909042, -0.014784012921154499, 0.17074492573738098, 0.06156061217188835, -0.11908335983753204, 0.03452804684638977, -0.05845857039093971, -0.17124423384666443, 0.11201129853725433, -0.454143226146698, -0.01601596362888813, 0.12155311554670334, 0.17195431888103485, 0.5039787888526917, -0.029142936691641808, 0.5461664795875549, 0.10757967084646225, -0.0159502774477005, -0.019418248906731606, -0.13796351850032806, -0.17743068933486938, 0.10957439988851547, 0.1955547332763672, 0.17927736043930054, -0.118046335875988, 0.16234146058559418, -0.20263312757015228, -0.057366449385881424, 0.23759570717811584, 0.08467834442853928, 0.08307020366191864, -0.002694368129596114, -0.03039700724184513, 0.11516904085874557, -0.07074180245399475, -0.11378098279237747, 0.00052021280862391, -0.15047143399715424, -0.058243975043296814, 0.08618707209825516, -0.18073421716690063, -0.12003901600837708, -0.10936981439590454, 0.06732606887817383, 0.04682781547307968, -0.1280103325843811, 0.21724678575992584, 0.02741776779294014, 0.2511090040206909, -0.14733467996120453, -0.2468840777873993, -0.21108393371105194, -0.012430056929588318, 0.1749899536371231, -0.00494422297924757, 0.16977360844612122, 0.06159155070781708, -0.029534373432397842, 0.18465015292167664, 0.3569014370441437, 0.24192120134830475, 0.03830821439623833, -0.20222145318984985, -0.09830134361982346, 0.03237712383270264, -0.21374239027500153, 0.0854668915271759, 0.03916957601904869, 0.01626642979681492, -0.017753543332219124, 0.25306788086891174, 0.07548090815544128, -0.03690439090132713, 0.09275811910629272, 0.3536774218082428, -0.26635032892227173, 0.07945334911346436, -0.019663769751787186, 0.27089929580688477, -0.028312966227531433, -0.2470628321170807, -0.13356825709342957, -0.2838171720504761, 0.24004507064819336, 0.06258896738290787, -0.029756855219602585, -0.1946357786655426, 0.38207507133483887, 0.16503465175628662, -0.14754922688007355, -0.23351642489433289, -0.09658804535865784, 0.2229955643415451, 0.12363424897193909, -0.2505846917629242, 0.026954418048262596, 0.12545660138130188, -0.006548390258103609, 0.0029263885226100683, -0.26181352138519287, -0.3188221752643585, -0.09057804942131042, -0.14337146282196045, 0.0657639279961586, 0.024495119228959084, -0.1876462697982788, 0.11635982990264893, -0.08947359025478363, -0.0377810075879097, 0.07448925822973251, -0.04938757047057152, -0.10349617898464203, -0.06402567028999329, 0.20295636355876923, 0.029007019475102425, -0.04143589735031128, -0.15905988216400146, 0.24851427972316742, -0.3324313461780548, 0.22172199189662933, 0.175127774477005, 0.051572658121585846, -0.16873593628406525, 0.13883234560489655, 0.09306888282299042, 0.02601933665573597, 0.2710925042629242, -0.04647386446595192, 0.11530230194330215, -0.013806692324578762, -0.06758404523134232, -0.1378190964460373, 0.05601366609334946, 0.16478991508483887, -0.2795073688030243, 0.20670844614505768, -0.27345705032348633, 0.3596440851688385, -0.29555466771125793, -0.0507606603205204, -0.0019047688692808151, -0.14838697016239166, -0.12994974851608276, -0.08591783046722412, -0.011794624850153923, -0.09535753726959229, 0.18922585248947144, -0.24094977974891663, -0.24539417028427124, -0.07534245401620865, -0.2349349856376648, -0.0005352930747903883, -0.22302986681461334, 0.2918258011341095, 0.2452337145805359, -0.003087942488491535, -0.15177106857299805, 0.07547775655984879, 0.07124092429876328, 0.2177671641111374, -0.38307082653045654, -0.04971428960561752, 0.10651727020740509, -0.29476845264434814, -0.2770644724369049, 0.013407111167907715, -0.06822777539491653, -0.1290050595998764, -0.011550637893378735, 0.12566426396369934, -0.09314339607954025, 0.09176905453205109, 0.16864484548568726, -0.02890995889902115, -0.18524372577667236, -0.17691829800605774, -0.38149482011795044, -0.2305794507265091, 0.031219180673360825, 0.044430170208215714, 0.4031013250350952, 0.20793218910694122, -0.2661348879337311, -0.0941198542714119, 0.3412841856479645, -0.22452829778194427, 0.016217024996876717, -0.15891166031360626, -0.5597448945045471, -0.135077565908432, 0.1771048903465271, 0.2317732721567154, 0.04678238928318024, -0.06685008108615875, -0.3263649642467499, 0.058898404240608215, -0.053149718791246414, -0.252469927072525, -0.10405374318361282, 0.2239631861448288, 0.19209186732769012, -0.03358486667275429, -0.20387886464595795, 0.15828262269496918, -0.197018563747406, 0.06869829446077347, -0.16054941713809967, 0.23009105026721954, 0.011007591150701046, -0.26582249999046326, -0.11178608238697052, -0.34160369634628296, 0.6065596342086792, -0.2841578722000122, -0.01512270700186491, 0.06024006009101868, 0.24634197354316711, -0.06475146859884262, -0.02152146026492119, -0.16161397099494934, 0.10751432925462723, 0.12976591289043427, -0.717917263507843, -0.1499692052602768, -0.13623356819152832, 0.060315292328596115, -0.05178645998239517, 0.054055649787187576, 0.34930935502052307, -0.1394900530576706, -0.1455458551645279, 0.3839104175567627, 0.19812916219234467, -0.08364270627498627, -0.3086870014667511, 0.016608137637376785, -0.1533060073852539, 0.13033226132392883, -0.01939968578517437, 0.27364033460617065, -0.13819673657417297, 0.1365949660539627, 0.00050253642257303, -0.026301944628357887, -0.0898793637752533, 0.1945015788078308, -0.018093593418598175, -0.030691973865032196, -0.09387023001909256, 0.13345715403556824, 0.22325509786605835, -0.35712873935699463, -0.12926606833934784, -0.17427824437618256, 0.0807068720459938, 0.07605286687612534, 0.11830499023199081, -0.08113740384578705, -0.1558152139186859, -0.27363234758377075, 0.10095745325088501, 0.1182674989104271, -0.3375316262245178, -0.11149106174707413, -0.13304172456264496, 0.2773902118206024, -0.1790984869003296, 0.014558425173163414, 0.20013907551765442, 0.03149662911891937, 0.30809298157691956, -0.06374411284923553, 0.0757690966129303, 0.2340090274810791, -0.01860624924302101, -0.26703307032585144, -0.155223086476326, -0.23804597556591034, 0.12916502356529236, -0.5030641555786133, -0.26393428444862366, 0.14712749421596527, -0.24668484926223755, 0.03140149265527725, -0.26454398036003113, 0.035750482231378555, -0.3563375473022461, 0.051789119839668274, -0.1315673589706421, 0.2417675405740738, -0.16316530108451843, 0.18952859938144684, -0.02065778337419033, 0.04814041405916214, 0.7580673694610596, 0.3574613630771637, -0.1724860817193985, -0.18723787367343903, 0.0597001351416111, -0.25903305411338806, -0.10523488372564316, -0.2967912554740906, -0.012639259919524193, 0.2759365439414978, 0.18500570952892303, -0.24948525428771973, 0.1577150970697403, 0.11768512427806854, -0.2219931185245514, -0.041906341910362244, -0.06428549438714981, -0.020314717665314674, 0.05851069092750549, -0.005641971714794636, -0.10548988729715347, -0.11127246916294098, 0.2151549905538559, 0.06391115486621857, 0.005049892235547304, 0.06141117215156555, 0.09010094404220581, 0.02305438742041588, -0.11930225789546967, -0.2582840919494629, -0.12356909364461899, 0.012677228078246117, 0.022356238216161728, -0.10578914731740952, 0.22437253594398499, 0.01985316351056099, 0.07666987180709839, 0.19395563006401062, 0.1306190937757492, -0.29541096091270447, 0.2574712634086609, 0.2012401670217514, -0.25875288248062134, 0.11256945133209229, -0.056824471801519394, 0.08670109510421753, -0.19877153635025024, -0.2496434450149536, -0.004518815781921148, 0.22433963418006897, 0.5697280168533325, 0.1757410764694214, -0.05883052572607994, -0.07468900084495544, 0.22808226943016052, -0.017731022089719772, 0.12578590214252472, -0.10636240243911743, 0.2639748454093933, -0.2282133251428604, -0.09242991358041763, -0.20500750839710236, -0.010628638789057732, -0.0972989872097969, -0.10588926821947098, -0.05758003517985344, -0.13974682986736298, 0.05580767244100571, 0.09635293483734131, -0.13704149425029755, 0.25495871901512146, 0.0089872470125556, 0.20991523563861847, 0.35801592469215393, -0.055414654314517975, -0.1769309937953949, 0.017683163285255432, 0.05532889813184738, 0.17193347215652466, 0.0034680524840950966, 0.08784303069114685, -0.04685467481613159, -0.07454852014780045, 0.1913757622241974, 0.13308405876159668, -0.1224985122680664, -0.2537843585014343, -0.016723904758691788, -0.060087040066719055, -0.3162809908390045, -0.14369559288024902, 0.2610807418823242, 0.05325594171881676, -0.018616272136569023, 0.7245340347290039, -0.12988422811031342, -0.01686776988208294, -0.12865401804447174, -0.3996800482273102, -0.28106456995010376, 0.37064021825790405, -0.028653113171458244, 0.29161337018013, -0.0887250155210495, -0.024691911414265633, -0.16592466831207275, 0.12062185257673264, -0.026051653549075127, -0.3387566804885864, -0.3951006829738617, 0.03775424882769585, 0.17788562178611755, -0.04399631544947624, 0.32490986585617065, 0.2461005300283432, 0.052425190806388855, 0.2624979019165039, 0.27527761459350586, 0.22683709859848022, 0.37912675738334656, 0.2240327149629593, 0.1833108365535736, -0.10136250406503677, 0.5749281644821167, 0.09757225960493088, 0.22161607444286346, -0.17570844292640686, 0.09787112474441528, -0.3234855532646179, 0.03372764214873314, 0.13916796445846558, -0.3327591121196747, 0.0181510541588068, -0.15366826951503754, 0.40349650382995605, -0.1830008178949356, 0.4024547338485718, 0.29034876823425293, -0.13010278344154358, -0.009870987385511398, 0.06161060184240341, 0.09732411056756973, -0.026406047865748405, -0.01010147761553526, -0.10025248676538467, 0.2414037585258484, -0.0557652972638607, 0.08154866099357605, -0.057243332266807556, -0.06555141508579254, -0.3223540782928467, -0.355699747800827, -0.10415969789028168, -0.3846166431903839, 0.06833817809820175, 0.0937502384185791, -0.09671847522258759, -0.14254872500896454, 0.0966615378856659, 0.21222428977489471, -0.3599652051925659, 0.02958856150507927, -0.11349890381097794, -0.1455574631690979, -0.10139864683151245, -0.10551036149263382, -0.24486444890499115, -0.2629375159740448, -0.017276449128985405, 0.20371797680854797, 0.16233642399311066, 0.06392227113246918, -0.7405573129653931, 0.09655351936817169, 0.18246035277843475, 0.11314336210489273, 0.2019186019897461, -0.11911235749721527, -0.38147521018981934, -0.16512183845043182, 0.24243789911270142, -0.14797872304916382, 0.08645645529031754, 0.23055706918239594, -0.12811844050884247, -0.14375963807106018, 0.2783657908439636, 0.1368405967950821, -0.10724533349275589, -0.0238217543810606, -0.038421645760536194, 0.32643014192581177, -0.07964322715997696, -0.11004389077425003, -0.30447208881378174, -0.3505323529243469, 0.012334267608821392, -0.18396274745464325, -0.25852882862091064, 0.03816128149628639, 0.10015194863080978, -0.2508798837661743, -0.1025155633687973, -0.14002937078475952, -0.16850462555885315, -0.3297249972820282, 0.22409136593341827, -0.015490127727389336, 0.05209767818450928, 0.10222464799880981, 0.06922460347414017, -0.20033402740955353, -0.2600749433040619, 0.31219708919525146, -0.24676653742790222, -0.2575192153453827, -0.07407595962285995, -0.32715436816215515, -0.6631128191947937, 0.27336087822914124, 0.14328999817371368, -0.1907113641500473, -0.017215542495250702, -0.046556610614061356, 0.18868939578533173, -0.06457710266113281, 0.21535193920135498, -0.2197876274585724, -0.5138556361198425, -0.27401402592658997, -0.15727543830871582, -0.048848479986190796, 0.05724579095840454, -0.2050696760416031, 0.26627039909362793, -0.0001637412642594427, -0.4044809341430664, 0.7183682322502136, -0.23964235186576843, -0.17387263476848602, -0.6154111623764038, -0.26327595114707947, -0.25857895612716675, 0.004567111376672983, 0.19643807411193848, -0.3734615445137024, -0.04805932939052582, 0.23371851444244385, -0.21836012601852417, -0.12188655138015747, 0.08796575665473938, -0.11740072816610336, 0.330890029668808, 0.0010062485234811902, -0.17892925441265106, -0.08996684849262238, -0.44672340154647827, -0.09178745001554489, -0.31848013401031494, -0.15029597282409668, 0.0716383084654808, -0.0558096207678318, 0.01819714345037937, -0.06010640412569046, 0.2217070460319519, 0.14501886069774628, 0.08399724215269089, -0.13230569660663605, -0.2930697798728943, -0.13456803560256958, -0.335528701543808, -0.31382206082344055, 0.013893123716115952, -0.05944962799549103, 0.13150781393051147, -0.16976162791252136, 0.06529030203819275, -0.17915038764476776, -0.1979045867919922, -0.07746704667806625, -0.13084304332733154, 0.11063574999570847, -0.18788738548755646, -0.09202858805656433, -0.2380906343460083, -0.1307569146156311, 0.06362678110599518, 0.3221529424190521, 0.32350391149520874, 0.09051666408777237, 0.05217096954584122, 0.32767221331596375, -0.11711153388023376, -0.12389101833105087, 0.02364669181406498, -0.005481333006173372, -0.3762073516845703, 0.22512641549110413, 0.2105799913406372, 0.1369607299566269, -0.22318142652511597, -0.10761453956365585, -0.08419506251811981, -0.23655560612678528, 0.047632984817028046, 0.0832488089799881, -0.1238357424736023, 0.059428054839372635, -0.06560701876878738, 0.16182005405426025, -0.1434745192527771, 0.09083811193704605, 0.029629362747073174, 0.3435255289077759, -0.07575786858797073, 0.03439941629767418, 0.18499033153057098, 0.14363045990467072, -0.04539848119020462, -0.06279613077640533, 0.1040419414639473, 0.04681580513715744, -0.2549336850643158, -0.10543490201234818, 0.1428094506263733, -0.12468796223402023, -0.12790539860725403, 0.25880876183509827, -0.21893176436424255, 0.18240931630134583, -0.3176051378250122, -0.10883054882287979, -0.31935015320777893, 0.10603983700275421, 0.07983573526144028, 0.22890423238277435, 0.30726948380470276, -0.020913509652018547, -0.2838931679725647, 0.3238564431667328, -0.02001783810555935, -0.7542944550514221, -0.14096634089946747, -0.34267958998680115, -0.3055534064769745, 0.0706942081451416, 0.14820128679275513, -0.08685794472694397, 0.20934689044952393, -0.1624850183725357, -0.032747503370046616, 0.31037530303001404, 0.3260461688041687, 0.015527432784438133, -0.14673137664794922, -0.4335249066352844, 0.0379258468747139, 0.09102264791727066, 0.07114116102457047, 0.2671864926815033, 0.08903758227825165, 0.2890612781047821, 0.40296584367752075, -0.32868829369544983, -0.18445885181427002, 0.0005063767312094569};

const float bias_raw[128]={0.11956212669610977, 0.03947041556239128, -0.5035410523414612, 0.17182587087154388, -0.17500227689743042, -0.03569569066166878, -0.02173437364399433, -0.06685373187065125, 0.4574068784713745, -0.19436824321746826, 0.020944910123944283, -0.2541062533855438, 0.016643838956952095, -0.5574899315834045, 0.11657547950744629, -0.00041075615445151925, 0.3079672157764435, 0.337700754404068, 0.23448452353477478, -0.42846938967704773, -0.012947778217494488, 0.1332719773054123, -0.160247340798378, -0.058737773448228836, -0.0815822035074234, 0.14851327240467072, -0.055575985461473465, -0.07361678034067154, -0.0048879170790314674, -0.22530968487262726, -0.145093634724617, 0.13289174437522888, 0.2277260273694992, 0.3582574725151062, -0.479871928691864, -0.030351977795362473, -0.008344351314008236, -0.3095696270465851, -0.39466601610183716, -0.48867106437683105, 0.17806382477283478, 0.31379175186157227, 0.2289416640996933, 0.5280775427818298, 0.3269398808479309, -0.2660970985889435, -0.1694391518831253, 0.005405533593147993, 0.32825905084609985, -0.3313179612159729, -0.03949505463242531, -0.10444840788841248, 0.037915345281362534, -0.30858534574508667, -0.2786045968532562, -0.045866284519433975, 0.20303742587566376, 0.22513040900230408, -0.05489456653594971, -0.5061821937561035, -0.04936595633625984, -0.9462032914161682, 0.2809610664844513, 0.3576424717903137, 0.4787324368953705, 0.03938009962439537, -0.62764972448349, -0.175851508975029, 0.4452430307865143, -0.09078838676214218, -0.30337968468666077, -0.2334817498922348, 0.23682546615600586, 0.17961546778678894, -0.33731386065483093, -0.17657893896102905, 0.20394639670848846, -0.14323732256889343, -0.1372603476047516, -0.11659780144691467, 0.16675665974617004, 0.4798603951931, 0.009805778041481972, -0.11970856785774231, 0.23668228089809418, -0.6353662014007568, 0.06276188045740128, -0.22000744938850403, 0.32472410798072815, -0.23762044310569763, -0.093333400785923, 0.21365569531917572, -0.0018592043779790401, 0.42271921038627625, 0.29007259011268616, -0.17512492835521698, -0.22917330265045166, 0.3176017105579376, 0.46882423758506775, 0.06582843512296677, -0.33461737632751465, -0.7461258769035339, 0.38155850768089294, -0.3711094856262207, -0.006194542162120342, -0.1073976382613182, 0.1648096889257431, -0.01071041077375412, 0.2026458978652954, 0.0160521250218153, -0.7043570280075073, 0.47026610374450684, 0.0006574454600922763, 0.07807675004005432, -0.12674176692962646, -0.2036915272474289, 0.2637909948825836, 0.23809824883937836, 0.04897673800587654, 0.11770770698785782, -0.3775218725204468, -0.09054301679134369, 0.4118553400039673, 0.14724287390708923, -0.21023373305797577, -0.2805860936641693, -0.08749693632125854, 0.30219611525535583};

const int stride_width=1;
const int stride_height=1;
const TfLiteFusedActivation activation=kTfLiteActRelu;
const int dilation_width_factor=1;
const int dilation_height_factor=1;
const int32_t filter_input_channel=32;
const int32_t filter_output_channel=128;
const int filter_height=1;
const int filter_width=1;
const int filter_dims_size=4;
const int32_t filter_dims_raw[4]={128,1,1,32};
const int bias_dims_size=1;
const int32_t bias_dims_raw[1]={128};
const TfLitePadding paddings=kTfLitePaddingSame;
const TfLiteType filter_type=kTfLiteFloat32;
const bool data_supports_multithreaded_kernel=true;

struct OpData {
  // IDs are the arbitrary identifiers used by TF Lite to identify and access
  // memory buffers.
  int im2col_id = kTensorNotAllocated;
  int hwcn_weights_id = kTensorNotAllocated;
  int input_quantized_id = kTensorNotAllocated;
  int scaling_factors_id = kTensorNotAllocated;
  int input_offset_id = kTensorNotAllocated;
  int accum_scratch_id = kTensorNotAllocated;
  // Row sums are used to cache filter sums for hybrid zero-point calculations.
  int row_sums_id = kTensorNotAllocated;

  TfLitePaddingValues padding;
  // The scaling factor from input to output (aka the 'real multiplier') can
  // be represented as a fixed point multiplier plus a left shift.
  int32_t output_multiplier;
  int output_shift;

  // Per channel output multiplier and shift.
  std::vector<int32_t> per_channel_output_multiplier;
  std::vector<int> per_channel_output_shift;

  // The range of the fused activation layer. For example for kNone and
  // uint8_t these would be 0 and 255.
  int32_t output_activation_min;
  int32_t output_activation_max;
  // Indexes are the offset to the memory buffer in the array used to keep track
  // of the allocated temporaries.
  int32_t im2col_index;
  int32_t hwcn_weights_index;
  int32_t input_quantized_index;
  int32_t scaling_factors_index;
  int32_t accum_scratch_index;
  int32_t input_offset_index;
  int32_t row_sums_index;

  bool need_hwcn_weights = false;
  bool have_weights_been_transposed = false;
  bool need_im2col = false;
  // If it's true, it means im2col is needed but gets disabled because the
  // temporary im2col tensor requires too much memory (i.e.
  // >= kMaxIm2colBufferSize);
  bool im2col_oversized = false;

  bool supports_multithreaded_kernel = false;
  bool is_hybrid_per_channel = false;
  bool compute_hybrid_row_sums = true;

  // Number of convolution groups.
  int32_t groups = 1;
};

inline PaddingType RuntimePaddingType(TfLitePadding padding) {
  switch (padding) {
    case TfLitePadding::kTfLitePaddingSame:
      return PaddingType::kSame;
    case TfLitePadding::kTfLitePaddingValid:
      return PaddingType::kValid;
    case TfLitePadding::kTfLitePaddingUnknown:
    default:
      return PaddingType::kNone;
  }
}

void* Init(TfLiteContext* context, const char* buffer, size_t length) {
  // This is a builtin op, so we don't use the contents in 'buffer', if any.
  // Instead, we allocate a new object to use as scratch space for im2col, and
  // to carry information from Prepare() to Eval().
  auto* data = new OpData;
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
  eigen_support::IncrementUsageCounter(context);
#endif
  return data;
}

void Free(TfLiteContext* context, void* buffer) {
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
  eigen_support::DecrementUsageCounter(context);
#endif
  delete reinterpret_cast<OpData*>(buffer);
}

// Naive implementation of transpose for floats. Could be optimized to be more
// cache friendly, but for now it's a one-time cost on first run, and we would
// prefer to remove the need to do this at all eventually.
void TransposeFloatTensor(const TfLiteTensor* input, TfLiteTensor* output) {
  const int rows = output->dims->data[1];
  const int cols = output->dims->data[0];
  // const float* input_data = GetTensorData<float>(input);
  const float* input_data = filter_raw;
  float* output_data = GetTensorData<float>(output);
  for (int i = 0; i < rows; ++i) {
    for (int j = 0; j < cols; ++j) {
      const float in_value = input_data[i * cols + j];
      output_data[j * rows + i] = in_value;
    }
  }
}

// Check if im2col needs to be allocated, as some version of optimized Conv dont
// use it. If any change is supporting im2col in any of the Conv versions, then
// it should be updated here as well
bool IsIm2ColRequired(const TfLiteTensor* input, TfLiteConvParams* params,
                      const TfLiteTensor* filter, OpData* data, bool is_hybrid,
                      KernelType kernel_type) {
  // If HWCN weights are required, Im2Col not required
  if (data->need_hwcn_weights) return false;

  // segregate based on dilated conv & non-dialated conv
  const bool need_dilated_im2col =
      params->dilation_width_factor != 1 || params->dilation_height_factor != 1;
  // const bool need_non_dilated_im2col =
  //     params->stride_width != 1 || params->stride_height != 1 ||
  //     filter->dims->data[2] != 1 || filter->dims->data[1] != 1;
  const bool need_non_dilated_im2col =
      stride_width != 1 || stride_height != 1 ||
      filter_width != 1 || filter_height != 1;

  const bool need_im2col = need_dilated_im2col || need_non_dilated_im2col;

  // Return early as basic requirement is not met
  if (!need_im2col) return false;

  // Special case for Hybrid, as it supports only non-dilated im2col currently
  const bool is_hybrid_non_dilated = is_hybrid && need_non_dilated_im2col;
  const bool is_quantized = input->type == kTfLiteUInt8 ||
                            input->type == kTfLiteInt8 ||
                            input->type == kTfLiteInt16;

  switch (kernel_type) {
    case kReference:
      if (is_hybrid) {
        return true;
      } else {
        return false;
      }
    case kGenericOptimized:
    case kCblasOptimized:
      if (is_hybrid && !need_non_dilated_im2col) {
        return false;
      } else {
        return true;
      }
    case kMultithreadOptimized:
      if (is_hybrid_non_dilated || is_quantized ||
          !data->supports_multithreaded_kernel) {
        return true;
      } else {
        return false;
      }
    default:
      return false;
  }
}

// Allocate temporary tensors (`im2col`, `hwcn_weights` if necessary).
// Note: `context->AddTensors` might invalidate pointers to existing tensors.
// Therefore the logic to add tensors are isolated into this function.
static TfLiteStatus AllocateTemporaryTensorsIfRequired(
    TfLiteContext* context, TfLiteNode* node, bool is_hybrid,
    bool is_per_channel, KernelType kernel_type, size_t im2col_bytes) {
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams* params;
  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  // TF_LITE_ENSURE(context, node->inputs->size >= 2);
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  const TfLiteTensor* filter;
  // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));

  // If we're using the optimized multithreaded EigenTensor implementation of
  // convolution, it expects the filter weights to be transposed compared to
  // the normal TF Lite buffer format. Typical TF Lite weights are
  // [filter_count, filter_height, filter_width, input_depth], but for the float
  // implementation we need them as [filter_height, filter_width, input_depth,
  // filter_count]. We get to that format by transposing, and create a temporary
  // buffer to store the results.
  // This path is only used for float processing, so only create the buffer if
  // we're running with that data type.
  data->need_hwcn_weights =
      input->type == kTfLiteFloat32 && data->supports_multithreaded_kernel;

  // We don't always need to allocate im2col. It is only used in some versions
  // of the optimized Conv. This test just mimics something that happens inside
  // optimized_ops.h, in order to avoid a DCHECK(!im2col_data).
  data->need_im2col =
      IsIm2ColRequired(input, params, filter, data, is_hybrid, kernel_type);

  // If im2col_oversized is found to be true, we have to fallback to an
  // execution path (like kReference in float/quantized cases) that doesn't
  // require im2col operation. Therefore, we have to skip checking the hybrid
  // case (but not the hybrid-per-channel one) where there's no such a fallback
  // execution path.
  // TODO(b/178743262): Consider making this check conditioned on the available
  // memory of the system, rather than coupling to the mobile platform check.
  if (IsMobilePlatform() && !(is_hybrid && !is_per_channel) &&
      data->need_im2col && im2col_bytes >= kMaxIm2colBufferSizeMobile) {
    data->need_im2col = false;
    data->im2col_oversized = true;
  }
  int temporaries_count = 0;
  if (data->need_im2col) {
    data->im2col_index = temporaries_count;
    if (data->im2col_id == kTensorNotAllocated) {
      context->AddTensors(context, 1, &data->im2col_id);
    }
    ++temporaries_count;
  }
  if (data->need_hwcn_weights) {
    data->hwcn_weights_index = temporaries_count;
    if (data->hwcn_weights_id == kTensorNotAllocated) {
      context->AddTensors(context, 1, &data->hwcn_weights_id);
    }
    ++temporaries_count;
  }

  if (is_hybrid) {
    // Allocate tensor to store the on-the-fly quantized inputs.
    data->input_quantized_index = temporaries_count;
    if (data->input_quantized_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->input_quantized_id));
    }
    ++temporaries_count;

    // Allocate tensor to store the quantization params computed during
    // on-the-fly input quantization.
    data->scaling_factors_index = temporaries_count;
    if (data->scaling_factors_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->scaling_factors_id));
    }
    ++temporaries_count;

    // Allocate tensor to store the accumulators for the matrix multiply.
    data->accum_scratch_index = temporaries_count;
    if (data->accum_scratch_id == kTensorNotAllocated) {
      TF_LITE_ENSURE_OK(
          context, context->AddTensors(context, 1, &data->accum_scratch_id));
    }
    ++temporaries_count;
    if (is_per_channel) {
      data->input_offset_index = temporaries_count;
      if (data->input_offset_id == kTensorNotAllocated) {
        TF_LITE_ENSURE_OK(
            context, context->AddTensors(context, 1, &data->input_offset_id));
      }
      ++temporaries_count;

      data->row_sums_index = temporaries_count;
      if (data->row_sums_id == kTensorNotAllocated) {
        TF_LITE_ENSURE_OK(context,
                          context->AddTensors(context, 1, &data->row_sums_id));
      }
      ++temporaries_count;
    }
  }

  TfLiteIntArrayFree(node->temporaries);
  node->temporaries = TfLiteIntArrayCreate(temporaries_count);

  return kTfLiteOk;
}

TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,
                     TfLiteNode* node) {
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  // TfLiteConvParams* params;
  // TfLitePadding paddings=kTfLitePaddingSame;
  // --------------------------------------------------------------------------
  // int stride_width=2;
  // int stride_height=2;
  // TfLiteFusedActivation activation=kTfLiteActRelu6;
  // int dilation_width_factor = 1;
  // int dilation_height_factor = 1;
  // const int32_t filter_input_channel = 3;
  // const int32_t filter_output_channel = 24;
  // int filter_height = 3;
  // int filter_width = 3;
  // TfLiteType filter_type = kTfLiteFloat32;
  // const bool data_supports_multithreaded_kernel=true;
  // --------------------------------------------------------------------------
  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  bool has_bias = false;
  // bool has_bias = 0;
  // Check number of inputs/outputs
  // TF_LITE_ENSURE(context, has_bias || node->inputs->size == 2);
  TF_LITE_ENSURE_EQ(context, node->outputs->size, 1);
  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  const TfLiteTensor* filter;
  // TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 1, &filter));

  // Check dimensionality of input, filter
  TF_LITE_ENSURE_EQ(context, input->dims->size, 4);
  // TF_LITE_ENSURE_EQ(context, filter->dims->size, 4);
  TF_LITE_ENSURE_EQ(context, filter_dims_size, 4);
  
  // Check input channels matching filter
  // Filter input channel can be a factor of channels of input (grouped conv)
  // or equals (normal conv).
  auto input_channel = input->dims->data[3];
  // auto filter_input_channel = filter->dims->data[3];

  TF_LITE_ENSURE_EQ(context, input_channel % filter_input_channel, 0);
  data->groups = input_channel / filter_input_channel;

  // Check types. (We assume that UINT8 refers to quantized tensors)
  TfLiteType input_type = input->type;
  TF_LITE_ENSURE(context,
                 input_type == kTfLiteFloat32 || input_type == kTfLiteUInt8 ||
                     input_type == kTfLiteInt8 || input_type == kTfLiteInt16);
  TF_LITE_ENSURE_TYPES_EQ(context, output->type, input_type);

  if (input_type == kTfLiteInt16) {
    TF_LITE_ENSURE_EQ(context, input->params.zero_point, 0);
    TF_LITE_ENSURE_EQ(context, output->params.zero_point, 0);
  }
  // Filter must have zero zero-points in per-channel quantization.
  if (input_type == kTfLiteInt16 || input_type == kTfLiteInt8) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    for (int i = 0; i < affine_quantization->zero_point->size; ++i) {
      TF_LITE_ENSURE_EQ(context, affine_quantization->zero_point->data[i], 0);
    }
  }

  const TfLiteTensor* bias = nullptr;
  // std::cout << "codes runs here #-2" << std::endl;
  // TODO(ahentz): At this point the optimized versions require 'bias'. We can
  // either change that or document that convolution requires it.

  // TF_LITE_ENSURE(context, has_bias);

  if (has_bias) {
    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 2, &bias));
    if (input_type == kTfLiteUInt8 || input_type == kTfLiteInt8) {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, kTfLiteInt32);
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else if (input_type == kTfLiteInt16) {
      TF_LITE_ENSURE(context, (bias->type == kTfLiteInt32) ||
                                  (bias->type == kTfLiteInt64));
      TF_LITE_ENSURE_EQ(context, bias->params.zero_point, 0);
    } else {
      TF_LITE_ENSURE_TYPES_EQ(context, bias->type, input_type);
    }
    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 0));
  }

  const bool is_hybrid =
      (input->type == kTfLiteFloat32 &&
       (filter_type == kTfLiteUInt8 || filter_type == kTfLiteInt8));

  if (is_hybrid && filter_type == kTfLiteInt8 &&
      filter->quantization.type == kTfLiteAffineQuantization &&
      filter->quantization.params &&
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)
          ->scale &&
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params)
              ->scale->size > 1) {
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    const float scale = affine_quantization->scale->data[0];
    for (int i = 1; i < affine_quantization->scale->size; i++) {
      if (affine_quantization->scale->data[i] != scale) {
        data->is_hybrid_per_channel = true;
        break;
      }
    }
  }
  //----------------------------------------------------------------------------
  // std::cout << "codes runs here #-1" << std::endl;
  // The multi-threaded kernel supports neither dilation nor hybrid kernels, and
  // is incompatible with mutable input filters that might change between evals.
  // data->supports_multithreaded_kernel =
  //     (kernel_type == kMultithreadOptimized) &&
  //     (context->recommended_num_threads != 1) && !is_hybrid &&
  //     (dilation_width_factor == 1) &&
  //     (dilation_height_factor == 1) &&
  //     (filter->allocation_type != kTfLiteArenaRw) && !IsDynamicTensor(filter);
  data->supports_multithreaded_kernel = data_supports_multithreaded_kernel;
  // const char * bool_value = data->supports_multithreaded_kernel ? "true" : "false";
  // std::cout << bool_value << std::endl;    
  // std::cout << "codes runs here #-1.1" << std::endl;

  
  // int channels_in = filter->dims->data[3];
  int channels_in = filter_input_channel;
  // int channels_out = filter->dims->data[0];
  int channels_out = filter_output_channel;
  int width = input->dims->data[2];
  int height = input->dims->data[1];
  // int filter_width = filter->dims->data[2];
  // int filter_height = filter->dims->data[1];
  int batches = input->dims->data[0];
  //----------------------------------------------------------------------------
  // std::cout << "codes runs here #-1.2" << std::endl;
  // Matching GetWindowedOutputSize in TensorFlow.
  auto padding = paddings;
  int out_width, out_height;
  data->padding = ComputePaddingHeightWidth(
      stride_height, stride_width,
      dilation_height_factor, dilation_width_factor, height,
      width, filter_height, filter_width, padding, &out_height, &out_width);
  // std::cout << "codes runs here #-1.2" << std::endl;
  size_t im2col_type_size;
  TF_LITE_ENSURE_STATUS(GetSizeOfType(context, input->type, &im2col_type_size));
  // Note that we intentionally promote the first multiplicand (i.e. 'batches')
  // to 'size_t' to avoid integer overflow here.
  const size_t im2col_bytes = static_cast<size_t>(batches) * out_height *
                              out_width * channels_in * filter_height *
                              filter_width * im2col_type_size;
  TF_LITE_ENSURE_STATUS(AllocateTemporaryTensorsIfRequired(
      context, node, is_hybrid, data->is_hybrid_per_channel, kernel_type,
      im2col_bytes));
  // std::cout << "codes runs here #0" << std::endl;
  // TF_LITE_ENSURE(context, has_bias);

  // Note that full fixed-point inference requires that all tensors have their
  // parameters set. This is usually done during quantized training or
  // calibration.
  if (input_type != kTfLiteFloat32) {
    TF_LITE_ENSURE_EQ(context, filter->quantization.type,
                      kTfLiteAffineQuantization);
    const auto* affine_quantization =
        reinterpret_cast<TfLiteAffineQuantization*>(
            filter->quantization.params);
    TF_LITE_ENSURE(context, affine_quantization);
    TF_LITE_ENSURE(context, affine_quantization->scale);
    TF_LITE_ENSURE(context, (affine_quantization->scale->size == 1 ||
                             affine_quantization->scale->size == channels_out));

    data->per_channel_output_multiplier.resize(channels_out);
    data->per_channel_output_shift.resize(channels_out);
    // TF_LITE_ENSURE_STATUS(tflite::PopulateConvolutionQuantizationParams(
    //     context, input, filter, bias, output, activation,
    //     &data->output_multiplier, &data->output_shift,
    //     &data->output_activation_min, &data->output_activation_max,
    //     data->per_channel_output_multiplier.data(),
    //     data->per_channel_output_shift.data(), channels_out));
  }
  // std::cout << "codes runs here #0.1" << std::endl;
  TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);
  output_size->data[0] = batches;
  output_size->data[1] = out_height;
  output_size->data[2] = out_width;
  output_size->data[3] = channels_out;
  auto output_status = context->ResizeTensor(context, output, output_size);

  if (output_status != kTfLiteOk) return output_status;

  if (data->need_im2col) {
    node->temporaries->data[data->im2col_index] = data->im2col_id;

    TfLiteIntArray* im2col_size = TfLiteIntArrayCreate(4);

    // auto filter_input_channel = filter->dims->data[3];
    im2col_size->data[0] = output_size->data[0];
    im2col_size->data[1] = output_size->data[1];
    im2col_size->data[2] = output_size->data[2];
    im2col_size->data[3] = filter_input_channel * filter_height * filter_width;

    TfLiteTensor* im2col =
        &context->tensors[node->temporaries->data[data->im2col_index]];
    im2col->type = input->type;
    if (is_hybrid) {
      im2col->type = filter_type;
    }
    im2col->allocation_type = kTfLiteArenaRw;
    auto im2col_status = context->ResizeTensor(context, im2col, im2col_size);
    if (im2col_status != kTfLiteOk) return im2col_status;
  }
  // std::cout << "codes runs here #0.2" << std::endl;
  if (data->need_hwcn_weights) {
    node->temporaries->data[data->hwcn_weights_index] = data->hwcn_weights_id;
    TfLiteIntArray* hwcn_weights_size = TfLiteIntArrayCreate(2);

    // Because we're treating the filter weights as a matrix when we do the
    // transpose, we allocate the buffer with a two-dimensional shape, where one
    // dimension is the number of elements in each filter, and the second is the
    // total number of filters.
    // auto filter_input_channel = filter->dims->data[3];
    hwcn_weights_size->data[0] =
        (filter_height * filter_width * filter_input_channel);
    hwcn_weights_size->data[1] = channels_out;

    TfLiteTensor* hwcn_weights =
        &context->tensors[node->temporaries->data[data->hwcn_weights_index]];
    hwcn_weights->type = input_type;
    hwcn_weights->allocation_type = kTfLiteArenaRwPersistent;

    auto hwcn_weights_status =
        context->ResizeTensor(context, hwcn_weights, hwcn_weights_size);
    if (hwcn_weights_status != kTfLiteOk) return hwcn_weights_status;

    // TODO(petewarden): If Resize() is called when the size hasn't actually
    // changed, this will do extra redundant work.
    data->have_weights_been_transposed = false;
  }
  // std::cout << "codes runs here #0.3" << std::endl;
  if (is_hybrid) {
    node->temporaries->data[data->input_quantized_index] =
        data->input_quantized_id;
    TfLiteTensor* input_quantized;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->input_quantized_index,
                                  &input_quantized));
    input_quantized->type = kTfLiteInt8;
    input_quantized->allocation_type = kTfLiteArenaRw;
    if (!TfLiteIntArrayEqual(input_quantized->dims, input->dims)) {
      TfLiteIntArray* input_quantized_size = TfLiteIntArrayCopy(input->dims);
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_quantized,
                                                       input_quantized_size));
    }
    // std::cout << "codes runs here #0.4" << std::endl;
    node->temporaries->data[data->scaling_factors_index] =
        data->scaling_factors_id;
    TfLiteTensor* scaling_factors;
    TF_LITE_ENSURE_OK(
        context, GetTemporarySafe(context, node, data->scaling_factors_index,
                                  &scaling_factors));
    scaling_factors->type = kTfLiteFloat32;
    scaling_factors->allocation_type = kTfLiteArenaRw;
    // Only one scale factor per batch is typically necessary. See optimized
    // implementation for why we need to allocate for the height of the inputs
    // flattened to 2D.
    TF_LITE_ENSURE(context, channels_in != 0);
    const int height = NumElements(input) / channels_in;
    int scaling_dims[1] = {height};
    if (!TfLiteIntArrayEqualsArray(scaling_factors->dims, 1, scaling_dims)) {
      TfLiteIntArray* scaling_factors_size = TfLiteIntArrayCreate(1);
      scaling_factors_size->data[0] = height;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, scaling_factors,
                                                       scaling_factors_size));
    }
    // std::cout << "codes runs here #0.5" << std::endl;
    node->temporaries->data[data->accum_scratch_index] = data->accum_scratch_id;
    TfLiteTensor* accum_scratch;
    TF_LITE_ENSURE_OK(context,
                      GetTemporarySafe(context, node, data->accum_scratch_index,
                                       &accum_scratch));
    accum_scratch->type = kTfLiteInt32;
    accum_scratch->allocation_type = kTfLiteArenaRw;
    const int scratch_width = batches * out_height * out_width;
    int accum_scratch_dims[2] = {channels_out, scratch_width};
    if (!TfLiteIntArrayEqualsArray(accum_scratch->dims, 2,
                                   accum_scratch_dims)) {
      TfLiteIntArray* accum_scratch_size = TfLiteIntArrayCreate(2);
      accum_scratch_size->data[0] = channels_out;
      accum_scratch_size->data[1] = scratch_width;
      TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, accum_scratch,
                                                       accum_scratch_size));
    }
    // std::cout << "codes runs here #0.6" << std::endl;
    if (data->is_hybrid_per_channel) {
      const auto* affine_quantization =
          reinterpret_cast<TfLiteAffineQuantization*>(
              filter->quantization.params);
      TF_LITE_ENSURE_EQ(
          context, affine_quantization->scale->size,
          filter->dims->data[affine_quantization->quantized_dimension]);
      node->temporaries->data[data->input_offset_index] = data->input_offset_id;
      TfLiteTensor* input_offsets;
      TF_LITE_ENSURE_OK(
          context, GetTemporarySafe(context, node, data->input_offset_index,
                                    &input_offsets));
      input_offsets->type = kTfLiteInt32;
      input_offsets->allocation_type = kTfLiteArenaRw;
      // See above comment for the need to allocate for height of inputs.
      TF_LITE_ENSURE(context, channels_in != 0);
      const int height = NumElements(input) / channels_in;
      const int input_offset_dims[1] = {height};
      if (!TfLiteIntArrayEqualsArray(input_offsets->dims, 1,
                                     input_offset_dims)) {
        TfLiteIntArray* input_offsets_size = TfLiteIntArrayCreate(1);
        input_offsets_size->data[0] = input_offset_dims[0];
        TF_LITE_ENSURE_OK(context, context->ResizeTensor(context, input_offsets,
                                                         input_offsets_size));
      }
      node->temporaries->data[data->row_sums_index] = data->row_sums_id;
      TfLiteTensor* row_sums;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));
      row_sums->type = kTfLiteInt32;
      row_sums->allocation_type = kTfLiteArenaRwPersistent;
      // See above comment for the need to allocate for height of inputs.
      const int row_sums_dims[1] = {channels_out};
      if (!TfLiteIntArrayEqualsArray(row_sums->dims, 1, row_sums_dims)) {
        TfLiteIntArray* row_sums_size = TfLiteIntArrayCreate(1);
        row_sums_size->data[0] = row_sums_dims[0];
        TF_LITE_ENSURE_OK(
            context, context->ResizeTensor(context, row_sums, row_sums_size));
      }
    }
  }
  // std::cout << "codes runs here #1" << std::endl;
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {
  return Prepare(kernel_type, context, node);
}

template <KernelType kernel_type>
void EvalQuantized(TfLiteContext* context, TfLiteNode* node,
                   TfLiteConvParams* params, OpData* data,
                   const TfLiteTensor* input, const TfLiteTensor* filter,
                   const TfLiteTensor* bias, TfLiteTensor* im2col,
                   TfLiteTensor* output) {
  auto input_offset = -input->params.zero_point;
  auto filter_offset = -filter->params.zero_point;
  auto output_offset = output->params.zero_point;

  KernelType effective_kernel_type;
  if ((kernel_type == kMultithreadOptimized ||
       kernel_type == kCblasOptimized) &&
      (params->dilation_width_factor != 1 ||
       params->dilation_height_factor != 1)) {
    // kMultithreadOptimized and kCblasOptimized do not support dilation.
    // Therefore, fallback to optimized.
    effective_kernel_type = kGenericOptimized;
  } else {
    effective_kernel_type = kernel_type;
  }

  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.dilation_width_factor = dilation_width_factor;
  op_params.dilation_height_factor = dilation_height_factor;
  op_params.stride_width = stride_width;
  op_params.stride_height = stride_height;
  op_params.input_offset = input_offset;
  op_params.weights_offset = filter_offset;
  op_params.output_offset = output_offset;
  op_params.output_multiplier = data->output_multiplier;
  op_params.output_shift = -data->output_shift;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;
  switch (effective_kernel_type) {
    case kReference: {
      reference_ops::Conv(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(filter), GetTensorData<uint8_t>(filter),
          GetTensorShape(bias), GetTensorData<int32_t>(bias),
          GetTensorShape(output), GetTensorData<uint8_t>(output),
          GetTensorShape(im2col), GetTensorData<uint8_t>(im2col),
          /* cpu_backend_context = */ nullptr);
      break;
    }
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      // There is only one optimized implementation for Quantized Conv.
      optimized_ops::Conv(
          op_params, GetTensorShape(input), GetTensorData<uint8_t>(input),
          GetTensorShape(filter), GetTensorData<uint8_t>(filter),
          GetTensorShape(bias), GetTensorData<int32_t>(bias),
          GetTensorShape(output), GetTensorData<uint8_t>(output),
          GetTensorShape(im2col), GetTensorData<uint8_t>(im2col),
          CpuBackendContext::GetFromContext(context));
      break;
    }
  }
}

template <KernelType kernel_type>
void EvalQuantizedPerChannel(TfLiteContext* context, TfLiteNode* node,
                             TfLiteConvParams* params, OpData* data,
                             const TfLiteTensor* input,
                             const TfLiteTensor* filter,
                             const TfLiteTensor* bias, TfLiteTensor* output,
                             TfLiteTensor* im2col) {
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = stride_height;
  op_params.stride_width = stride_width;
  op_params.dilation_height_factor = dilation_height_factor;
  op_params.dilation_width_factor = dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  switch (effective_kernel_type) {
    case kReference: {
      reference_integer_ops::ConvPerChannel(
          op_params, data->per_channel_output_multiplier.data(),
          data->per_channel_output_shift.data(), GetTensorShape(input),
          GetTensorData<int8>(input), GetTensorShape(filter),
          GetTensorData<int8>(filter), GetTensorShape(bias),
          GetTensorData<int32>(bias), GetTensorShape(output),
          GetTensorData<int8>(output));
      break;
    }
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      optimized_integer_ops::ConvPerChannel(
          op_params, data->per_channel_output_multiplier.data(),
          data->per_channel_output_shift.data(), GetTensorShape(input),
          GetTensorData<int8>(input), GetTensorShape(filter),
          GetTensorData<int8>(filter), GetTensorShape(bias),
          GetTensorData<int32>(bias), GetTensorShape(output),
          GetTensorData<int8>(output), GetTensorShape(im2col),
          GetTensorData<int8>(im2col),
          CpuBackendContext::GetFromContext(context));
      break;
    }
  }
}

template <KernelType kernel_type>
void EvalQuantizedPerChannel16x8(TfLiteContext* context, TfLiteNode* node,
                                 TfLiteConvParams* params, OpData* data,
                                 const TfLiteTensor* input,
                                 const TfLiteTensor* filter,
                                 const TfLiteTensor* bias, TfLiteTensor* output,
                                 TfLiteTensor* im2col) {
  ConvParams op_params;
  op_params.input_offset = -input->params.zero_point;
  op_params.output_offset = output->params.zero_point;
  op_params.stride_height = stride_height;
  op_params.stride_width = stride_width;
  op_params.dilation_height_factor = dilation_height_factor;
  op_params.dilation_width_factor = dilation_width_factor;
  op_params.padding_values.height = data->padding.height;
  op_params.padding_values.width = data->padding.width;
  op_params.quantized_activation_min = data->output_activation_min;
  op_params.quantized_activation_max = data->output_activation_max;

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  // To prevent 32bit accum overflow for 16x8 quantization, it enables the
  // optimized path only when zero_point is 0.
  bool has_non_zero_point = input->params.zero_point ||
                            filter->params.zero_point ||
                            output->params.zero_point;

  // Fallback to reference kernel when bias_type is int64 as
  // there is no optimized kernel for int64 bias yet.
  if (bias && bias->type == kTfLiteInt64) {
    reference_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<std::int64_t>(bias), GetTensorShape(output),
        GetTensorData<int16>(output));
  } else if (effective_kernel_type == kReference || has_non_zero_point) {
    reference_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16>(input), GetTensorShape(filter),
        GetTensorData<int8>(filter), GetTensorShape(bias),
        GetTensorData<std::int32_t>(bias), GetTensorShape(output),
        GetTensorData<int16>(output));
  } else {
    optimized_integer_ops::ConvPerChannel(
        op_params, data->per_channel_output_multiplier.data(),
        data->per_channel_output_shift.data(), GetTensorShape(input),
        GetTensorData<int16_t>(input), GetTensorShape(filter),
        GetTensorData<int8_t>(filter), GetTensorShape(bias),
        GetTensorData<std::int32_t>(bias), GetTensorShape(output),
        GetTensorData<int16_t>(output), GetTensorShape(im2col),
        GetTensorData<int16_t>(im2col),
        CpuBackendContext::GetFromContext(context));
  }
}

template <KernelType kernel_type>
void EvalFloat(TfLiteContext* context, TfLiteNode* node,
               TfLiteConvParams* params, OpData* data,
               const TfLiteTensor* input, const TfLiteTensor* filter,
               const TfLiteTensor* bias, TfLiteTensor* im2col,
               TfLiteTensor* hwcn_weights, TfLiteTensor* output) {

  float output_activation_min, output_activation_max;

//----------------------------------------------------------------
  CalculateActivationRange(activation, &output_activation_min,
                           &output_activation_max);
  KernelType effective_kernel_type = kernel_type;
  // Fall back to the optimized path if multi-threaded conv is unsupported.
  if ((kernel_type == kMultithreadOptimized) &&
      !data->supports_multithreaded_kernel) {
    effective_kernel_type = kGenericOptimized;
  }

  // When im2col is needed (which is implied when 'im2col_oversized' is true),
  // the GEMMM-based optimized path requires im2col data be allocated to ensure
  // the correctness. Therefore, when im2col is disabled because of the
  // oversized temporary im2col tensor, fallback to a non-optimized path is
  // needed.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
    // As detailed by tflite::multithreaded_ops::Conv implementation in
    // multithreaded_conv.h, the Eigen-based execution doesn't need im2col data.
    // Therefore, we could rely on it as a better-optimized fallback than the
    // reference one.
    if (data->supports_multithreaded_kernel) {
      effective_kernel_type = kMultithreadOptimized;
    }
#endif
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = RuntimePaddingType(paddings);
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.stride_width = stride_width;
  op_params.stride_height = stride_height;
  op_params.dilation_width_factor = dilation_width_factor;
  op_params.dilation_height_factor = dilation_height_factor;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;
  // std::cout << "codes runs here #5" << std::endl;
  // const int filter_dims_size=4;
  const int32_t* filter_dims_data;
  filter_dims_data = filter_dims_raw;
  const int32_t* bias_dims_data;
  bias_dims_data = bias_dims_raw;

  const float* filter_data;
  filter_data = filter_raw;
  const float* bias_data;
  bias_data = bias_raw;

  switch (effective_kernel_type) {
    case kReference: {
      // std::cout << "codes runs here #6" << std::endl;
      reference_ops::Conv(op_params, GetTensorShape(input),
                          GetTensorData<float>(input), RuntimeShape(filter_dims_size, filter_dims_data),
                          filter_data, RuntimeShape(bias_dims_size, bias_dims_data),
                          bias_data, GetTensorShape(output),
                          GetTensorData<float>(output), GetTensorShape(im2col),
                          GetTensorData<float>(im2col));
      break;
    }
    case kCblasOptimized:
    case kGenericOptimized: {
      // std::cout << "codes runs here #7" << std::endl;
      optimized_ops::Conv(op_params, GetTensorShape(input),
                          GetTensorData<float>(input), RuntimeShape(filter_dims_size, filter_dims_data),
                          filter_data, RuntimeShape(bias_dims_size, bias_dims_data),
                          bias_data, GetTensorShape(output),
                          GetTensorData<float>(output), GetTensorShape(im2col),
                          GetTensorData<float>(im2col),
                          CpuBackendContext::GetFromContext(context));
      break;
    }
    case kMultithreadOptimized: {
#if defined(TFLITE_WITH_MULTITHREADED_EIGEN)
      
      multithreaded_ops::Conv(
          *eigen_support::GetThreadPoolDevice(context), op_params,
          GetTensorShape(input), GetTensorData<float>(input),
          RuntimeShape(filter_dims_size, filter_dims_data), filter_data, RuntimeShape(bias_dims_size, bias_dims_data),
          bias_data, GetTensorShape(output),
          GetTensorData<float>(output), GetTensorShape(im2col),
          GetTensorData<float>(im2col));
      break;
#else   // !defined(TFLITE_WITH_MULTITHREADED_EIGEN)
      // See Register_CONV_2D: we should never be here when TFLITE_WITH_RUY
      // was enabled. We #if out this code in order to get the corresponding
      // binary size benefits.
      TFLITE_DCHECK(false);
#endif  // defined(TFLITE_WITH_MULTITHREADED_EIGEN)
    }
  }
}

template <KernelType kernel_type>
TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,
                                  TfLiteConvParams* params, OpData* data,
                                  const TfLiteTensor* input,
                                  const TfLiteTensor* filter,
                                  const TfLiteTensor* bias,
                                  TfLiteTensor* im2col, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(activation, &output_activation_min,
                           &output_activation_max);

  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;
  TfLiteTensor* quantized_input_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &quantized_input_tensor));
  int8_t* quantized_input_ptr_batch =
      GetTensorData<int8_t>(quantized_input_tensor);
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);
  TfLiteTensor* input_offset_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_offset_index,
                                     &input_offset_tensor));
  int32_t* input_offset_ptr = GetTensorData<int32_t>(input_offset_tensor);

  for (int b = 0; b < batch_size; ++b) {
    const int offset = b * input_size;
    tensor_utils::AsymmetricQuantizeFloats(
        GetTensorData<float>(input) + offset, input_size,
        quantized_input_ptr_batch + offset, &scaling_factors_ptr[b],
        &input_offset_ptr[b]);
  }

  int8_t* im2col_ptr = nullptr;
  int8_t* filter_ptr = nullptr;
  if (im2col != nullptr) {
    im2col_ptr = im2col->data.int8;
  }
  filter_ptr = filter->data.int8;
  const auto* affine_quantization =
      reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);

  KernelType effective_kernel_type = kernel_type;
  // We have to fallback to reference execution path when im2col is needed but
  // disabled because to-be-allocated temporary im2col tensor is too large.
  // See b/178743262 for the detailed motivation.
  if (data->im2col_oversized) {
    effective_kernel_type = kReference;
  }

  // Grouped convolution is right now only supported on reference kernel.
  if (data->groups != 1) {
    effective_kernel_type = kReference;
  }

  ConvParams op_params;
  op_params.padding_type = PaddingType::kSame;
  op_params.padding_values.width = data->padding.width;
  op_params.padding_values.height = data->padding.height;
  op_params.dilation_width_factor = dilation_width_factor;
  op_params.dilation_height_factor = dilation_height_factor;
  op_params.stride_width = stride_width;
  op_params.stride_height = stride_height;
  op_params.float_activation_min = output_activation_min;
  op_params.float_activation_max = output_activation_max;

  switch (effective_kernel_type) {
    case kReference:
      reference_ops::HybridConvPerChannel(
          op_params, scaling_factors_ptr, GetTensorShape(input),
          quantized_input_ptr_batch, GetTensorShape(filter), filter_ptr,
          GetTensorShape(bias), GetTensorData<float>(bias),
          GetTensorShape(output), GetTensorData<float>(output),
          GetTensorShape(im2col), im2col_ptr, affine_quantization->scale->data,
          input_offset_ptr);
      break;
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      TfLiteTensor* row_sums;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->row_sums_index, &row_sums));
      TfLiteTensor* scratch;
      TF_LITE_ENSURE_OK(
          context,
          GetTemporarySafe(context, node, data->accum_scratch_index, &scratch));
      optimized_ops::HybridConvPerChannel(
          op_params, scaling_factors_ptr, GetTensorShape(input),
          quantized_input_ptr_batch, GetTensorShape(filter), filter_ptr,
          GetTensorShape(bias), GetTensorData<float>(bias),
          GetTensorShape(output), GetTensorData<float>(output),
          GetTensorShape(im2col), im2col_ptr, affine_quantization->scale->data,
          input_offset_ptr, GetTensorShape(scratch),
          GetTensorData<int32>(scratch), GetTensorData<int32_t>(row_sums),
          &data->compute_hybrid_row_sums,
          CpuBackendContext::GetFromContext(context));
      data->compute_hybrid_row_sums = false;
      break;
    }
  }

  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus EvalHybrid(TfLiteContext* context, TfLiteNode* node,
                        TfLiteConvParams* params, OpData* data,
                        const TfLiteTensor* input, const TfLiteTensor* filter,
                        const TfLiteTensor* bias, TfLiteTensor* im2col,
                        TfLiteTensor* accum_scratch, TfLiteTensor* output) {
  float output_activation_min, output_activation_max;
  CalculateActivationRange(activation, &output_activation_min,
                           &output_activation_max);

  const int batch_size = SizeOfDimension(input, 0);
  TF_LITE_ENSURE(context, batch_size != 0);
  const int input_size = NumElements(input) / batch_size;

  const float* input_ptr = GetTensorData<float>(input);
  TfLiteTensor* quantized_input_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->input_quantized_index,
                                     &quantized_input_tensor));
  int8_t* quantized_input_ptr_batch =
      GetTensorData<int8_t>(quantized_input_tensor);
  TfLiteTensor* scaling_factors_tensor;
  TF_LITE_ENSURE_OK(context,
                    GetTemporarySafe(context, node, data->scaling_factors_index,
                                     &scaling_factors_tensor));
  float* scaling_factors_ptr = GetTensorData<float>(scaling_factors_tensor);

  // Per-batch input quantization for higher accuracy.
  {
    ruy::profiler::ScopeLabel label("ConvHybridQuantizeInputs");
    for (int b = 0; b < batch_size; ++b) {
      float unused_min, unused_max;
      const int offset = b * input_size;
      tensor_utils::SymmetricQuantizeFloats(
          input_ptr + offset, input_size, quantized_input_ptr_batch + offset,
          &unused_min, &unused_max, &scaling_factors_ptr[b]);
      scaling_factors_ptr[b] *= filter->params.scale;
    }
  }

  switch (kernel_type) {
    case kReference:
    case kGenericOptimized:
    case kMultithreadOptimized:
    case kCblasOptimized: {
      // There is only one implementation for hybrid kernel.
      ConvParams op_params;
      op_params.padding_type = PaddingType::kSame;
      op_params.padding_values.width = data->padding.width;
      op_params.padding_values.height = data->padding.height;
      op_params.stride_width = stride_width;
      op_params.stride_height = stride_height;
      op_params.dilation_width_factor = dilation_width_factor;
      op_params.dilation_height_factor = dilation_height_factor;
      op_params.float_activation_min = output_activation_min;
      op_params.float_activation_max = output_activation_max;
      if (data->groups == 1) {
        optimized_ops::HybridConv(
            op_params, scaling_factors_ptr, GetTensorShape(input),
            quantized_input_ptr_batch, GetTensorShape(filter),
            GetTensorData<int8_t>(filter), GetTensorShape(bias),
            GetTensorData<float>(bias), GetTensorShape(accum_scratch),
            GetTensorData<int32_t>(accum_scratch), GetTensorShape(output),
            GetTensorData<float>(output), GetTensorShape(im2col),
            GetTensorData<int8_t>(im2col),
            CpuBackendContext::GetFromContext(context));
      } else {
        // This case is handled by (fallbacked to) per channel hybrid group conv
        // and shouldn't hit this branch.
        TF_LITE_KERNEL_LOG(
            context,
            "Group convolution currently not supported for hybrid kernel.");
        return kTfLiteError;
      }
      break;
    }
  }

  return kTfLiteOk;
}

template <KernelType kernel_type, TfLiteType input_type>
TfLiteStatus EvalImpl(TfLiteContext* context, TfLiteNode* node) {
  // std::cout << "codes runs here #2" << std::endl;
  // auto* params = reinterpret_cast<TfLiteConvParams*>(node->builtin_data);
  TfLiteConvParams* params;

  OpData* data = reinterpret_cast<OpData*>(node->user_data);

  TfLiteTensor* output;
  TF_LITE_ENSURE_OK(context, GetOutputSafe(context, node, 0, &output));
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));
  const TfLiteTensor* filter;
  const int dims_size = filter_dims_size;

  // std::cout << "filter dim size:" << dims_size << std::endl;
  // const int32_t* dims_data = reinterpret_cast<const int32_t*>(dims->data);
  const int32_t* dims_data = filter_dims_raw;
  // bool has_bias = node->inputs->size == 3;
  // const TfLiteTensor* bias = has_bias ? GetInput(context, node, 2) : nullptr;
  const TfLiteTensor* bias = nullptr;
  TfLiteTensor* im2col =
      data->need_im2col
          ? &context->tensors[node->temporaries->data[data->im2col_index]]
          : nullptr;
  TfLiteTensor* hwcn_weights =
      data->need_hwcn_weights
          ? &context->tensors[node->temporaries->data[data->hwcn_weights_index]]
          : nullptr;

  if (data->need_hwcn_weights && !data->have_weights_been_transposed) {
    TransposeFloatTensor(filter, hwcn_weights);
    data->have_weights_been_transposed = true;
  }
  // std::cout << "codes runs here #3" << std::endl;

  TFLITE_DCHECK_EQ(input_type, input->type);
  switch (input_type) {  // Already know in/outtypes are same.
    case kTfLiteFloat32:
      if (filter_type == kTfLiteUInt8 || filter_type == kTfLiteInt8) {
        if (data->is_hybrid_per_channel ||
            // TODO(b/162870360): Fallback to PerChannel implementation
            // before we have grouped hybrid convolution.
            data->groups != 1) {
          TF_LITE_ENSURE_OK(context, EvalHybridPerChannel<kernel_type>(
                                         context, node, params, data, input,
                                         filter, bias, im2col, output));
        } else {
          TfLiteTensor* accum_scratch =
              &context->tensors[node->temporaries
                                    ->data[data->accum_scratch_index]];
          TF_LITE_ENSURE_OK(context,
                            EvalHybrid<kernel_type>(context, node, params, data,
                                                    input, filter, bias, im2col,
                                                    accum_scratch, output));
        }
      } else {
        EvalFloat<kernel_type>(context, node, params, data, input, filter, bias,
                               im2col, hwcn_weights, output);
      }
      break;
    case kTfLiteUInt8:
      EvalQuantized<kernel_type>(context, node, params, data, input, filter,
                                 bias, im2col, output);
      break;
    case kTfLiteInt8:
      EvalQuantizedPerChannel<kernel_type>(context, node, params, data, input,
                                           filter, bias, output, im2col);
      break;
    case kTfLiteInt16:
      EvalQuantizedPerChannel16x8<kernel_type>(
          context, node, params, data, input, filter, bias, output, im2col);
      break;
    default:
      TF_LITE_KERNEL_LOG(context, "Type %s currently not supported.",
                         TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
  return kTfLiteOk;
}

template <KernelType kernel_type>
TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {
  const TfLiteTensor* input;
  TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, 0, &input));

  switch (input->type) {
    case kTfLiteFloat32:
      return EvalImpl<kernel_type, kTfLiteFloat32>(context, node);
    case kTfLiteUInt8:
      return EvalImpl<kernel_type, kTfLiteUInt8>(context, node);
    case kTfLiteInt8:
      return EvalImpl<kernel_type, kTfLiteInt8>(context, node);
    case kTfLiteInt16:
      return EvalImpl<kernel_type, kTfLiteInt16>(context, node);
    default:
      TF_LITE_KERNEL_LOG(context, "Type %s not currently supported.",
                         TfLiteTypeGetName(input->type));
      return kTfLiteError;
  }
}

}  // namespace conv

TfLiteRegistration* Register_muonzn_REF() {
  static TfLiteRegistration r = {muonzn::Init, muonzn::Free,
                                 muonzn::Prepare<muonzn::kReference>,
                                 muonzn::Eval<muonzn::kReference>};
  return &r;
}

TfLiteRegistration* Register_muonzn_GENERIC_OPT() {
  static TfLiteRegistration r = {muonzn::Init, muonzn::Free,
                                 muonzn::Prepare<muonzn::kGenericOptimized>,
                                 muonzn::Eval<muonzn::kGenericOptimized>};
  return &r;
}

TfLiteRegistration* Register_muonzn_MULTITHREADED_OPT() {
  static TfLiteRegistration r = {muonzn::Init, muonzn::Free,
                                 muonzn::Prepare<muonzn::kMultithreadOptimized>,
                                 muonzn::Eval<muonzn::kMultithreadOptimized>};
  return &r;
}

// TfLiteRegistration* Register_muonzn_CBLAS_OPT() {
//   static TfLiteRegistration r = {muonzn::Init, muonzn::Free,
//                                  muonzn::Prepare<muonzn::kCblasOptimized>,
//                                  muonzn::Eval<muonzn::kCblasOptimized>};
//   return &r;
// }

TfLiteRegistration* Register_muonzn() {
#if defined TFLITE_WITH_MULTITHREADED_EIGEN
  return Register_muonzn_MULTITHREADED_OPT();
#else
  return Register_muonzn_GENERIC_OPT();
#endif
}


}  // namespace builtin
}  // namespace ops
}  // namespace tflite
